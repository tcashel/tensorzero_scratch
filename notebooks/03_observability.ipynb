{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Observability & Tracing\n",
    "\n",
    "This notebook explores TensorZero's observability features:\n",
    "- Understanding inference data stored in ClickHouse\n",
    "- Using the TensorZero UI for monitoring\n",
    "- Implementing feedback loops\n",
    "- Analyzing performance metrics\n",
    "- Testing structured outputs with advanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to TensorZero gateway\n",
      "üåê TensorZero UI: http://localhost:4000\n",
      "üìä ClickHouse: http://localhost:8123\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize gateway client with new method\n",
    "client = TensorZeroGateway.build_http(gateway_url=\"http://localhost:3000\")\n",
    "print(\"‚úÖ Connected to TensorZero gateway\")\n",
    "print(\"üåê TensorZero UI: http://localhost:4000\")\n",
    "print(\"üìä ClickHouse: http://localhost:8123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Test Data\n",
    "\n",
    "Let's generate some inference data across different providers to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Generating test data...\n",
      "‚úÖ OpenAI GPT-4o Mini: What is machine learning?... - ID: 0198f224-4400-72d2-a279-a6638ad1670f\n",
      "‚úÖ Anthropic Claude 3 Haiku: What is machine learning?... - ID: 0198f224-6099-7e91-8ee1-4cb1002ae8ba\n",
      "‚úÖ xAI Grok-3 Mini: What is machine learning?... - ID: 0198f224-6acb-7c43-a7c7-d92b78b370a8\n",
      "‚úÖ OpenAI GPT-4o Mini: Explain quantum computing brie... - ID: 0198f224-82f4-7b13-a3a6-95dd31fb2a4e\n",
      "‚úÖ Anthropic Claude 3 Haiku: Explain quantum computing brie... - ID: 0198f224-8fd7-76b3-a735-93954d1cde4e\n",
      "‚úÖ xAI Grok-3 Mini: Explain quantum computing brie... - ID: 0198f224-9cc4-7402-b4bf-20c44a0ceb2a\n",
      "‚úÖ OpenAI GPT-4o Mini: How does blockchain work?... - ID: 0198f224-b204-7623-87e7-2021c9df4110\n",
      "‚úÖ Anthropic Claude 3 Haiku: How does blockchain work?... - ID: 0198f224-dde0-7380-90e8-79abedf9df55\n",
      "‚úÖ xAI Grok-3 Mini: How does blockchain work?... - ID: 0198f224-e994-7063-8c84-00a947fa134d\n",
      "‚úÖ OpenAI GPT-4o Mini: What is cloud computing?... - ID: 0198f225-10dc-7083-a4e8-5959cf7a3701\n",
      "‚úÖ Anthropic Claude 3 Haiku: What is cloud computing?... - ID: 0198f225-2e7b-7d01-8329-0a9be6abb9a5\n",
      "‚úÖ xAI Grok-3 Mini: What is cloud computing?... - ID: 0198f225-384f-72d3-9279-370f2bae55db\n",
      "‚úÖ OpenAI GPT-4o Mini: Describe artificial intelligen... - ID: 0198f225-5619-7732-9082-e8666af38833\n",
      "‚úÖ Anthropic Claude 3 Haiku: Describe artificial intelligen... - ID: 0198f225-7077-7dd1-b186-88c67a9dad9b\n",
      "‚úÖ xAI Grok-3 Mini: Describe artificial intelligen... - ID: 0198f225-7e0c-7373-8e44-5f84a643ffe7\n",
      "\n",
      "üìä Generated 15 successful inferences\n"
     ]
    }
   ],
   "source": [
    "# Test scenarios for observability\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain quantum computing briefly.\",\n",
    "    \"How does blockchain work?\",\n",
    "    \"What is cloud computing?\",\n",
    "    \"Describe artificial intelligence.\"\n",
    "]\n",
    "\n",
    "# Providers to test\n",
    "providers_to_test = [\n",
    "    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\"),\n",
    "    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\"),\n",
    "    (\"grok3_mini\", \"xAI Grok-3 Mini\"),\n",
    "]\n",
    "\n",
    "# Generate test data\n",
    "inference_ids = []\n",
    "\n",
    "print(\"üß™ Generating test data...\")\n",
    "for prompt in test_prompts:\n",
    "    for variant, provider_name in providers_to_test:\n",
    "        try:\n",
    "            response = client.inference(\n",
    "                function_name=\"chat\",\n",
    "                variant_name=variant,\n",
    "                input={\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            inference_ids.append({\n",
    "                \"inference_id\": response.inference_id,\n",
    "                \"variant\": variant,\n",
    "                \"provider\": provider_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ {provider_name}: {prompt[:30]}... - ID: {response.inference_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {provider_name}: {prompt[:30]}... - Error: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\nüìä Generated {len(inference_ids)} successful inferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collecting Feedback\n",
    "\n",
    "TensorZero allows collecting feedback on inferences for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Collecting feedback on inferences...\n",
      "‚úÖ Feedback for xAI Grok-3 Mini: Score=0.57, Helpful=False\n",
      "   Rating ID: {'feedback_id': '0198f225-a6d4-79b3-9f41-4d554f966d1d'}\n",
      "   Comment: Generated by xAI Grok-3 Mini - Could be better...\n",
      "‚úÖ Feedback for Anthropic Claude 3 Haiku: Score=0.71, Helpful=True\n",
      "   Rating ID: {'feedback_id': '0198f225-a6e2-72f2-8f42-b1c57a6e6f57'}\n",
      "   Comment: Generated by Anthropic Claude 3 Haiku - Great resp...\n",
      "‚úÖ Feedback for xAI Grok-3 Mini: Score=0.66, Helpful=False\n",
      "   Rating ID: {'feedback_id': '0198f225-a6e9-77c1-8ed1-db8571ebdc61'}\n",
      "   Comment: Generated by xAI Grok-3 Mini - Could be better...\n",
      "‚úÖ Feedback for OpenAI GPT-4o Mini: Score=0.67, Helpful=False\n",
      "   Rating ID: {'feedback_id': '0198f225-a6f0-73f0-8cf7-aafe14fb33d4'}\n",
      "   Comment: Generated by OpenAI GPT-4o Mini - Could be better...\n",
      "‚úÖ Feedback for xAI Grok-3 Mini: Score=0.90, Helpful=True\n",
      "   Rating ID: {'feedback_id': '0198f225-a6f6-7190-ae39-dbcad2eaeefc'}\n",
      "   Comment: Generated by xAI Grok-3 Mini - Great response!...\n",
      "\n",
      "üí° View feedback in TensorZero UI: http://localhost:4000\n"
     ]
    }
   ],
   "source": [
    "# Collect feedback on some inferences\n",
    "import random\n",
    "\n",
    "print(\"üìù Collecting feedback on inferences...\")\n",
    "\n",
    "# Sample some inference IDs for feedback\n",
    "feedback_samples = random.sample(inference_ids, min(5, len(inference_ids)))\n",
    "\n",
    "for sample in feedback_samples:\n",
    "    # Simulate different types of feedback\n",
    "    feedback_score = random.uniform(0.5, 1.0)\n",
    "    helpful = feedback_score > 0.7\n",
    "    \n",
    "    try:\n",
    "        # Submit rating feedback (float metric: 0.0-1.0)\n",
    "        rating_response = client.feedback(\n",
    "            metric_name=\"user_rating\",\n",
    "            inference_id=sample[\"inference_id\"],\n",
    "            value=feedback_score\n",
    "        )\n",
    "        \n",
    "        # Submit helpful feedback (boolean metric)\n",
    "        helpful_response = client.feedback(\n",
    "            metric_name=\"helpful\", \n",
    "            inference_id=sample[\"inference_id\"],\n",
    "            value=helpful\n",
    "        )\n",
    "        \n",
    "        # Submit comment feedback (built-in metric - no explicit config needed)\n",
    "        comment = f\"Generated by {sample['provider']} - {'Great response!' if helpful else 'Could be better'}\"\n",
    "        comment_response = client.feedback(\n",
    "            metric_name=\"comment\",\n",
    "            inference_id=sample[\"inference_id\"],\n",
    "            value=comment\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Feedback for {sample['provider']}: Score={feedback_score:.2f}, Helpful={helpful}\")\n",
    "        print(f\"   Rating ID: {rating_response.feedback_id}\")\n",
    "        print(f\"   Comment: {comment[:50]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to submit feedback: {e}\")\n",
    "\n",
    "print(f\"\\nüí° View feedback in TensorZero UI: http://localhost:4000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Direct ClickHouse Queries\n",
    "\n",
    "Let's query ClickHouse directly to analyze our inference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ClickHouse Tables:\n",
      "   ‚Ä¢ BatchIdByInferenceId\n",
      "   ‚Ä¢ BatchIdByInferenceIdView\n",
      "   ‚Ä¢ BatchModelInference\n",
      "   ‚Ä¢ BatchRequest\n",
      "   ‚Ä¢ BooleanMetricFeedback\n",
      "   ‚Ä¢ BooleanMetricFeedbackByTargetId\n",
      "   ‚Ä¢ BooleanMetricFeedbackByTargetIdView\n",
      "   ‚Ä¢ BooleanMetricFeedbackTagView\n",
      "   ‚Ä¢ ChatInference\n",
      "   ‚Ä¢ ChatInferenceByEpisodeIdView\n",
      "   ‚Ä¢ ChatInferenceByIdView\n",
      "   ‚Ä¢ ChatInferenceDatapoint\n",
      "   ‚Ä¢ ChatInferenceTagView\n",
      "   ‚Ä¢ CommentFeedback\n",
      "   ‚Ä¢ CommentFeedbackByTargetId\n",
      "   ‚Ä¢ CommentFeedbackByTargetIdView\n",
      "   ‚Ä¢ CommentFeedbackTagView\n",
      "   ‚Ä¢ CumulativeUsage\n",
      "   ‚Ä¢ CumulativeUsageView\n",
      "   ‚Ä¢ DemonstrationFeedback\n",
      "   ‚Ä¢ DemonstrationFeedbackByInferenceId\n",
      "   ‚Ä¢ DemonstrationFeedbackByInferenceIdView\n",
      "   ‚Ä¢ DemonstrationFeedbackTagView\n",
      "   ‚Ä¢ DeploymentID\n",
      "   ‚Ä¢ DynamicEvaluationRun\n",
      "   ‚Ä¢ DynamicEvaluationRunByProjectName\n",
      "   ‚Ä¢ DynamicEvaluationRunByProjectNameView\n",
      "   ‚Ä¢ DynamicEvaluationRunEpisode\n",
      "   ‚Ä¢ DynamicEvaluationRunEpisodeByRunId\n",
      "   ‚Ä¢ DynamicEvaluationRunEpisodeByRunIdView\n",
      "   ‚Ä¢ DynamicInContextLearningExample\n",
      "   ‚Ä¢ FeedbackTag\n",
      "   ‚Ä¢ FloatMetricFeedback\n",
      "   ‚Ä¢ FloatMetricFeedbackByTargetId\n",
      "   ‚Ä¢ FloatMetricFeedbackByTargetIdView\n",
      "   ‚Ä¢ FloatMetricFeedbackTagView\n",
      "   ‚Ä¢ InferenceByEpisodeId\n",
      "   ‚Ä¢ InferenceById\n",
      "   ‚Ä¢ InferenceTag\n",
      "   ‚Ä¢ JsonInference\n",
      "   ‚Ä¢ JsonInferenceByEpisodeIdView\n",
      "   ‚Ä¢ JsonInferenceByIdView\n",
      "   ‚Ä¢ JsonInferenceDatapoint\n",
      "   ‚Ä¢ JsonInferenceTagView\n",
      "   ‚Ä¢ ModelInference\n",
      "   ‚Ä¢ ModelInferenceCache\n",
      "   ‚Ä¢ StaticEvaluationBooleanHumanFeedbackView\n",
      "   ‚Ä¢ StaticEvaluationFloatHumanFeedbackView\n",
      "   ‚Ä¢ StaticEvaluationHumanFeedback\n",
      "   ‚Ä¢ TagChatInferenceView\n",
      "   ‚Ä¢ TagInference\n",
      "   ‚Ä¢ TagJsonInferenceView\n",
      "   ‚Ä¢ TensorZeroMigration\n"
     ]
    }
   ],
   "source": [
    "# ClickHouse connection details\n",
    "clickhouse_url = \"http://localhost:8123\"\n",
    "clickhouse_user = \"chuser\"\n",
    "clickhouse_password = \"chpassword\"\n",
    "database = \"tensorzero\"\n",
    "\n",
    "def query_clickhouse(query):\n",
    "    \"\"\"Execute a query against ClickHouse.\"\"\"\n",
    "    response = httpx.post(\n",
    "        f\"{clickhouse_url}/\",\n",
    "        params={\n",
    "            \"database\": database,\n",
    "            \"user\": clickhouse_user,\n",
    "            \"password\": clickhouse_password,\n",
    "            \"default_format\": \"JSONEachRow\"\n",
    "        },\n",
    "        data=query\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.strip().split('\\n')\n",
    "        return [json.loads(line) for line in lines if line]\n",
    "    else:\n",
    "        raise Exception(f\"ClickHouse query failed: {response.text}\")\n",
    "\n",
    "# Test ClickHouse connection\n",
    "try:\n",
    "    tables = query_clickhouse(\"SHOW TABLES\")\n",
    "    print(\"üìä ClickHouse Tables:\")\n",
    "    for table in tables:\n",
    "        print(f\"   ‚Ä¢ {table['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ClickHouse connection error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query error: ClickHouse query failed: {\"exception\": \"Code: 60. DB::Exception: Unknown table expression identifier 'Chat_inferences' in scope SELECT variant_name, COUNT(*) AS count, AVG(inference_duration_ms) AS avg_duration_ms FROM Chat_inferences WHERE timestamp > (now() - toIntervalHour(1)) GROUP BY variant_name ORDER BY count DESC. (UNKNOWN_TABLE) (version 24.12.6.70 (official build))\"}\n",
      "\n",
      "\n",
      "Available tables:\n",
      "  ‚Ä¢ BatchIdByInferenceId\n",
      "  ‚Ä¢ BatchIdByInferenceIdView\n",
      "  ‚Ä¢ BatchModelInference\n",
      "  ‚Ä¢ BatchRequest\n",
      "  ‚Ä¢ BooleanMetricFeedback\n",
      "  ‚Ä¢ BooleanMetricFeedbackByTargetId\n",
      "  ‚Ä¢ BooleanMetricFeedbackByTargetIdView\n",
      "  ‚Ä¢ BooleanMetricFeedbackTagView\n",
      "  ‚Ä¢ ChatInference\n",
      "  ‚Ä¢ ChatInferenceByEpisodeIdView\n",
      "  ‚Ä¢ ChatInferenceByIdView\n",
      "  ‚Ä¢ ChatInferenceDatapoint\n",
      "  ‚Ä¢ ChatInferenceTagView\n",
      "  ‚Ä¢ CommentFeedback\n",
      "  ‚Ä¢ CommentFeedbackByTargetId\n",
      "  ‚Ä¢ CommentFeedbackByTargetIdView\n",
      "  ‚Ä¢ CommentFeedbackTagView\n",
      "  ‚Ä¢ CumulativeUsage\n",
      "  ‚Ä¢ CumulativeUsageView\n",
      "  ‚Ä¢ DemonstrationFeedback\n",
      "  ‚Ä¢ DemonstrationFeedbackByInferenceId\n",
      "  ‚Ä¢ DemonstrationFeedbackByInferenceIdView\n",
      "  ‚Ä¢ DemonstrationFeedbackTagView\n",
      "  ‚Ä¢ DeploymentID\n",
      "  ‚Ä¢ DynamicEvaluationRun\n",
      "  ‚Ä¢ DynamicEvaluationRunByProjectName\n",
      "  ‚Ä¢ DynamicEvaluationRunByProjectNameView\n",
      "  ‚Ä¢ DynamicEvaluationRunEpisode\n",
      "  ‚Ä¢ DynamicEvaluationRunEpisodeByRunId\n",
      "  ‚Ä¢ DynamicEvaluationRunEpisodeByRunIdView\n",
      "  ‚Ä¢ DynamicInContextLearningExample\n",
      "  ‚Ä¢ FeedbackTag\n",
      "  ‚Ä¢ FloatMetricFeedback\n",
      "  ‚Ä¢ FloatMetricFeedbackByTargetId\n",
      "  ‚Ä¢ FloatMetricFeedbackByTargetIdView\n",
      "  ‚Ä¢ FloatMetricFeedbackTagView\n",
      "  ‚Ä¢ InferenceByEpisodeId\n",
      "  ‚Ä¢ InferenceById\n",
      "  ‚Ä¢ InferenceTag\n",
      "  ‚Ä¢ JsonInference\n",
      "  ‚Ä¢ JsonInferenceByEpisodeIdView\n",
      "  ‚Ä¢ JsonInferenceByIdView\n",
      "  ‚Ä¢ JsonInferenceDatapoint\n",
      "  ‚Ä¢ JsonInferenceTagView\n",
      "  ‚Ä¢ ModelInference\n",
      "  ‚Ä¢ ModelInferenceCache\n",
      "  ‚Ä¢ StaticEvaluationBooleanHumanFeedbackView\n",
      "  ‚Ä¢ StaticEvaluationFloatHumanFeedbackView\n",
      "  ‚Ä¢ StaticEvaluationHumanFeedback\n",
      "  ‚Ä¢ TagChatInferenceView\n",
      "  ‚Ä¢ TagInference\n",
      "  ‚Ä¢ TagJsonInferenceView\n",
      "  ‚Ä¢ TensorZeroMigration\n"
     ]
    }
   ],
   "source": [
    "# Query recent inferences\n",
    "try:\n",
    "    # Get inference count by variant\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        variant_name,\n",
    "        COUNT(*) as count,\n",
    "        AVG(inference_duration_ms) as avg_duration_ms\n",
    "    FROM Chat_inferences\n",
    "    WHERE timestamp > now() - INTERVAL 1 HOUR\n",
    "    GROUP BY variant_name\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    results = query_clickhouse(query)\n",
    "    \n",
    "    if results:\n",
    "        print(\"üìà Inference Statistics (Last Hour):\")\n",
    "        print(\"=\" * 50)\n",
    "        for row in results:\n",
    "            print(f\"Variant: {row['variant_name']}\")\n",
    "            print(f\"  Count: {row['count']}\")\n",
    "            print(f\"  Avg Duration: {row.get('avg_duration_ms', 'N/A')} ms\\n\")\n",
    "    else:\n",
    "        print(\"No inference data found in the last hour\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Query error: {e}\")\n",
    "    # Let's try to see what tables actually exist\n",
    "    try:\n",
    "        tables = query_clickhouse(\"SHOW TABLES\")\n",
    "        print(\"\\nAvailable tables:\")\n",
    "        for table in tables:\n",
    "            print(f\"  ‚Ä¢ {table['name']}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Output Testing\n",
    "\n",
    "Let's test structured outputs, especially with Grok models that support this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing Structured Output Capabilities\n",
      "========================================\n",
      "\u001b[2m2025-08-28T19:26:50.763243Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "‚ö†Ô∏è  Sentiment analysis not configured or failed: TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\n",
      "To enable structured output, add this to your tensorzero.toml:\n",
      "[functions.analyze_sentiment]\n",
      "type = \"json\"\n",
      "schema = '''{\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"sentiment\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\", \"neutral\"]},\n",
      "    \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
      "    \"explanation\": {\"type\": \"string\"}\n",
      "  },\n",
      "  \"required\": [\"sentiment\", \"confidence\", \"explanation\"]\n",
      "}'''\n",
      "\n",
      "[functions.analyze_sentiment.variants.grok3_mini]\n",
      "type = \"chat_completion\"\n",
      "model = \"xai::grok-3-mini\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's check if we have a structured output function configured\n",
    "# If not, we'll create one\n",
    "\n",
    "print(\"üîß Testing Structured Output Capabilities\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test with sentiment analysis (if configured)\n",
    "test_texts = [\n",
    "    \"TensorZero is amazing! It makes LLM integration so easy.\",\n",
    "    \"The setup was a bit complex but worth it.\",\n",
    "    \"Having issues with the configuration.\"\n",
    "]\n",
    "\n",
    "# Try sentiment analysis if available\n",
    "try:\n",
    "    for text in test_texts:\n",
    "        response = client.inference(\n",
    "            function_name=\"analyze_sentiment\",\n",
    "            variant_name=\"gpt4_json\",  # Try with GPT-4 first\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Parse structured output\n",
    "        result = json.loads(response.content[0].text)\n",
    "        \n",
    "        print(f\"\\nText: '{text[:50]}...'\")\n",
    "        print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "        print(f\"Explanation: {result['explanation']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Sentiment analysis not configured or failed: {str(e)[:100]}\")\n",
    "    print(\"\\nTo enable structured output, add this to your tensorzero.toml:\")\n",
    "    print(\"\"\"[functions.analyze_sentiment]\n",
    "type = \"json\"\n",
    "schema = '''{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"sentiment\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\", \"neutral\"]},\n",
    "    \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
    "    \"explanation\": {\"type\": \"string\"}\n",
    "  },\n",
    "  \"required\": [\"sentiment\", \"confidence\", \"explanation\"]\n",
    "}'''\n",
    "\n",
    "[functions.analyze_sentiment.variants.grok3_mini]\n",
    "type = \"chat_completion\"\n",
    "model = \"xai::grok-3-mini\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis Dashboard\n",
    "\n",
    "Let's create a simple performance dashboard using the data we've collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Inference Summary\n",
      "========================================\n",
      "\n",
      "Inferences by Provider:\n",
      "  OpenAI GPT-4o Mini: 5\n",
      "  Anthropic Claude 3 Haiku: 5\n",
      "  xAI Grok-3 Mini: 5\n",
      "\n",
      "Total Inferences: 15\n",
      "Unique Prompts: 5\n",
      "Time Range: 2025-08-28T15:25:27.065180 to 2025-08-28T15:26:50.574570\n",
      "\n",
      "üîç Sample Inference IDs (for UI exploration):\n",
      "  ‚Ä¢ 0198f224-4400-72d2-a279-a6638ad1670f (OpenAI GPT-4o Mini)\n",
      "  ‚Ä¢ 0198f224-6099-7e91-8ee1-4cb1002ae8ba (Anthropic Claude 3 Haiku)\n",
      "  ‚Ä¢ 0198f224-6acb-7c43-a7c7-d92b78b370a8 (xAI Grok-3 Mini)\n",
      "\n",
      "üåê View these in TensorZero UI: http://localhost:4000\n"
     ]
    }
   ],
   "source": [
    "# Create a performance summary\n",
    "if inference_ids:\n",
    "    df = pd.DataFrame(inference_ids)\n",
    "    \n",
    "    print(\"üìä Inference Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Count by provider\n",
    "    provider_counts = df['provider'].value_counts()\n",
    "    print(\"\\nInferences by Provider:\")\n",
    "    for provider, count in provider_counts.items():\n",
    "        print(f\"  {provider}: {count}\")\n",
    "    \n",
    "    # Recent activity\n",
    "    print(f\"\\nTotal Inferences: {len(df)}\")\n",
    "    print(f\"Unique Prompts: {df['prompt'].nunique()}\")\n",
    "    print(f\"Time Range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    \n",
    "    # Show sample IDs for UI exploration\n",
    "    print(\"\\nüîç Sample Inference IDs (for UI exploration):\")\n",
    "    for _, row in df.head(3).iterrows():\n",
    "        print(f\"  ‚Ä¢ {row['inference_id']} ({row['provider']})\")\n",
    "    \n",
    "    print(f\"\\nüåê View these in TensorZero UI: http://localhost:4000\")\n",
    "else:\n",
    "    print(\"‚ùå No inference data collected yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Observability Features\n",
    "\n",
    "Let's explore more advanced features like tracing multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Multi-Step Workflow Tracing\n",
      "========================================\n",
      "Step 1 - Topic Generated: Quantum Computing Security\n",
      "  Inference ID: 0198f225-a81b-7811-9686-83c3b977f3b2\n",
      "\n",
      "Step 2 - Explanation: Quantum computing has the potential to break many of the current cryptographic algorithms used to secure digital communications, making the development of quantum-resistant cryptography crucial for ensuring the future security of data and information.\n",
      "  Inference ID: 0198f225-aad3-7881-943c-52121a1e27a6\n",
      "\n",
      "Step 3 - Haiku:\n",
      "Bits dance in the light,  \n",
      "Shadows of quantum whispers,  \n",
      "Guardians of code.\n",
      "  Inference ID: 0198f225-af37-7ae1-a78e-840bf3a2d3a8\n",
      "\n",
      "‚úÖ Multi-step workflow completed!\n",
      "View the trace in TensorZero UI to see how these steps connect.\n"
     ]
    }
   ],
   "source": [
    "# Multi-step workflow example\n",
    "print(\"üîÑ Testing Multi-Step Workflow Tracing\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "try:\n",
    "    step1 = client.inference(\n",
    "        function_name=\"chat\",\n",
    "        variant_name=\"gpt4_mini\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Generate a random technical topic in 3 words or less.\"}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    topic = step1.content[0].text\n",
    "    print(f\"Step 1 - Topic Generated: {topic}\")\n",
    "    print(f\"  Inference ID: {step1.inference_id}\")\n",
    "    \n",
    "    # Step 2: Explain the topic\n",
    "    step2 = client.inference(\n",
    "        function_name=\"chat\",\n",
    "        variant_name=\"claude3_haiku\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Explain '{topic}' in one sentence.\"}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    explanation = step2.content[0].text\n",
    "    print(f\"\\nStep 2 - Explanation: {explanation}\")\n",
    "    print(f\"  Inference ID: {step2.inference_id}\")\n",
    "    \n",
    "    # Step 3: Generate a haiku about it\n",
    "    step3 = client.inference(\n",
    "        function_name=\"generate_haiku\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Write a haiku about {topic}.\"}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    haiku = step3.content[0].text\n",
    "    print(f\"\\nStep 3 - Haiku:\\n{haiku}\")\n",
    "    print(f\"  Inference ID: {step3.inference_id}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Multi-step workflow completed!\")\n",
    "    print(\"View the trace in TensorZero UI to see how these steps connect.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Workflow failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### Observability Features:\n",
    "1. **Inference Tracking**: Every API call gets a unique ID\n",
    "2. **Feedback Loop**: Can attach feedback to any inference\n",
    "3. **ClickHouse Storage**: All data queryable for analysis\n",
    "4. **UI Dashboard**: Visual exploration at http://localhost:4000\n",
    "\n",
    "### Advanced Capabilities:\n",
    "1. **Structured Output**: JSON schema validation (all Grok models support this)\n",
    "2. **Multi-Step Tracing**: Track complex workflows\n",
    "3. **Performance Metrics**: Latency, token usage, costs\n",
    "4. **A/B Testing**: Built-in experimentation framework\n",
    "\n",
    "### Next Steps:\n",
    "1. Explore the TensorZero UI for visual insights\n",
    "2. Set up custom ClickHouse queries for specific metrics\n",
    "3. Implement structured output functions\n",
    "4. Create feedback-driven optimization loops\n",
    "\n",
    "Next notebook: We'll explore prompt management and A/B testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
