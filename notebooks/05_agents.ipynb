{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 05. Agent Integration with LangGraph\n\nThis notebook demonstrates how to integrate TensorZero with LangGraph agents:\n- Using TensorZero's OpenAI-compatible endpoint with LangChain\n- Creating a ReAct agent with tools  \n- Observing agent interactions in TensorZero UI\n- Collecting feedback on agent performance\n\n**Key Learning**: TensorZero provides an OpenAI-compatible API endpoint at `/openai/v1`, making it easy to use with any OpenAI-compatible client!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Use TensorZero's OpenAI-Compatible Endpoint\nimport httpx\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\nfrom rich.console import Console\nfrom rich.panel import Panel\n\nconsole = Console()\n\n# Initialize TensorZero chat model using OpenAI-compatible endpoint\n# This is much simpler than a custom chat model!\nllm = init_chat_model(\n    \"agent_chat/gpt4_mini\",  # TensorZero function/variant format\n    model_provider=\"openai\",\n    base_url=\"http://localhost:3000/openai/v1\",  # TensorZero's OpenAI-compatible endpoint\n    api_key=\"dummy\",  # TensorZero ignores the API key\n    http_client=httpx.Client()\n)\n\nprint(\"‚úÖ TensorZero Chat Model initialized\")\nprint(\"üîß Using OpenAI-compatible endpoint: http://localhost:3000/openai/v1\")\nprint(\"üè∑Ô∏è  Function: agent_chat, Variant: gpt4_mini\")\n\n# Test basic chat\ntry:\n    test_response = llm.invoke(\"Hello! Can you introduce yourself?\")\n    console.print(Panel(test_response.content, title=\"ü§ñ TensorZero Response\", border_style=\"blue\"))\nexcept Exception as e:\n    print(f\"‚ùå Connection test failed: {e}\")\n    print(\"üí° Make sure TensorZero services are running with 'poe up'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Define Tools for the Agent\n\nLet's create some Python tools that our agent can use. These complement the TensorZero-configured tools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define Python-based tools for our agent\n@tool\ndef python_calculator(expression: str) -> str:\n    \"\"\"\n    Evaluate mathematical expressions using Python's built-in calculator.\n    \n    Args:\n        expression: A mathematical expression like '2 + 2' or 'sqrt(16)'\n    \"\"\"\n    try:\n        import math\n        # Safe evaluation with math functions\n        allowed_names = {\n            \"sqrt\": math.sqrt, \"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan,\n            \"log\": math.log, \"exp\": math.exp, \"pi\": math.pi, \"e\": math.e,\n            \"abs\": abs, \"pow\": pow, \"min\": min, \"max\": max\n        }\n        \n        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n        return f\"Python Calculator: {expression} = {result}\"\n    except Exception as e:\n        return f\"Calculator Error: {str(e)}\"\n\n@tool\ndef current_time(timezone: str = \"UTC\") -> str:\n    \"\"\"\n    Get the current time in a specified timezone.\n    \n    Args:\n        timezone: The timezone (UTC, EST, PST, etc.)\n    \"\"\"\n    try:\n        from datetime import datetime\n        import pytz\n        \n        utc_now = datetime.now(pytz.UTC)\n        timezone_map = {\n            \"UTC\": \"UTC\", \"EST\": \"US/Eastern\", \"PST\": \"US/Pacific\",\n            \"CST\": \"US/Central\", \"MST\": \"US/Mountain\"\n        }\n        \n        tz_name = timezone_map.get(timezone.upper(), timezone)\n        tz = pytz.timezone(tz_name)\n        local_time = utc_now.astimezone(tz)\n        return f\"Current time in {timezone}: {local_time.strftime('%Y-%m-%d %H:%M:%S %Z')}\"\n    except Exception as e:\n        return f\"Time Error: {str(e)}\"\n\n@tool\ndef text_analyzer(text: str) -> str:\n    \"\"\"\n    Analyze text properties including word count and basic sentiment.\n    \n    Args:\n        text: The text to analyze\n    \"\"\"\n    try:\n        words = text.split()\n        word_count = len(words)\n        char_count = len(text)\n        \n        # Simple sentiment analysis\n        positive_words = [\"good\", \"great\", \"excellent\", \"awesome\", \"fantastic\", \"wonderful\"]\n        negative_words = [\"bad\", \"terrible\", \"awful\", \"horrible\", \"worst\", \"hate\"]\n        \n        positive_count = sum(1 for word in words if word.lower() in positive_words)\n        negative_count = sum(1 for word in words if word.lower() in negative_words)\n        \n        sentiment = \"neutral\"\n        if positive_count > negative_count:\n            sentiment = \"positive\"\n        elif negative_count > positive_count:\n            sentiment = \"negative\"\n        \n        return f\"\"\"Text Analysis:\n- Words: {word_count}\n- Characters: {char_count}  \n- Sentiment: {sentiment}\n- Positive indicators: {positive_count}\n- Negative indicators: {negative_count}\"\"\"\n        \n    except Exception as e:\n        return f\"Analysis Error: {str(e)}\"\n\n# List of tools for our agent\ntools = [python_calculator, current_time, text_analyzer]\n\nprint(\"üîß Python Tools Created:\")\nfor tool_func in tools:\n    print(f\"  ‚Ä¢ {tool_func.name}: {tool_func.description.split('.')[0]}\")\n    \n# Test a tool\ntest_result = python_calculator.invoke({\"expression\": \"sqrt(144) + 5\"})\nconsole.print(Panel(test_result, title=\"üß™ Tool Test\", border_style=\"green\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Create the LangGraph Agent\n\nNow we'll create a ReAct agent using LangGraph that uses TensorZero as the backend."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a ReAct agent using TensorZero + LangGraph\ntry:\n    # Create the agent - LangGraph handles all the ReAct logic!\n    agent = create_react_agent(\n        llm,  # Our TensorZero chat model\n        tools,  # Our Python tools\n        messages_modifier=\"\"\"You are a helpful assistant powered by TensorZero. \n\nAvailable tools:\n- python_calculator: Advanced mathematical calculations with math functions\n- current_time: Get current time in different timezones  \n- text_analyzer: Analyze text properties and sentiment\n\nImportant: TensorZero may also provide additional tools like calculator, get_weather, and search_tensorzero_docs through its configuration. Use the most appropriate tool for each task.\"\"\"\n    )\n    \n    print(\"‚úÖ ReAct Agent created successfully!\")\n    print(\"ü§ñ Backend: TensorZero Gateway\")\n    print(\"üîß Tools: Python tools + TensorZero configured tools\")\n    print(\"‚ö° Ready for tool calling and reasoning!\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Agent creation failed: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Test the Agent\n\nLet's test our agent with various tasks that require tool usage."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test function to run agent and display results nicely\ndef test_agent(query: str, max_iterations: int = 10):\n    \"\"\"Run the agent with a query and display results.\"\"\"\n    console.print(f\"\\n[bold blue]üîµ User:[/bold blue] {query}\")\n    console.print(\"[dim]\" + \"=\"*60 + \"[/dim]\")\n    \n    try:\n        # Run the agent\n        messages = [{\"role\": \"user\", \"content\": query}]\n        result = agent.invoke({\"messages\": messages})\n        \n        if result and \"messages\" in result:\n            for i, message in enumerate(result[\"messages\"]):\n                if hasattr(message, 'content') and message.content:\n                    # Determine message type\n                    if hasattr(message, 'type'):\n                        msg_type = message.type\n                    else:\n                        msg_type = type(message).__name__.lower().replace('message', '')\n                    \n                    if msg_type == 'human':\n                        console.print(f\"[green]üë§ Human:[/green] {message.content}\")\n                    elif msg_type == 'ai':\n                        console.print(f\"[blue]ü§ñ Agent:[/blue] {message.content}\")\n                        \n                        # Check for tool calls\n                        if hasattr(message, 'tool_calls') and message.tool_calls:\n                            console.print(\"[cyan]üîß Tool Calls:[/cyan]\")\n                            for tool_call in message.tool_calls:\n                                console.print(f\"  ‚Ä¢ {tool_call['name']}: {tool_call['args']}\")\n                    elif msg_type == 'tool':\n                        console.print(f\"[yellow]‚öôÔ∏è  Tool Result:[/yellow] {message.content}\")\n        else:\n            console.print(\"[red]‚ùå No response received[/red]\")\n        \n        return result\n        \n    except Exception as e:\n        console.print(f\"[bold red]‚ùå Error:[/bold red] {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# Test 1: Math calculation\nprint(\"üßÆ Test 1: Mathematical Calculation\")\nresult1 = test_agent(\"What is 234 multiplied by 567? Please calculate this for me.\")\n\n# Test 2: Time query  \nprint(\"\\nüïê Test 2: Time Query\")\nresult2 = test_agent(\"What time is it in Tokyo right now?\")\n\n# Test 3: Text analysis\nprint(\"\\nüìä Test 3: Text Analysis\") \nresult3 = test_agent(\"Can you analyze this text: 'This is an amazing product, I love it!'\")\n\n# Test 4: Multi-step task\nprint(\"\\nüîÑ Test 4: Multi-Step Task\")\nresult4 = test_agent(\"Calculate the square root of 144, then tell me what time it is in EST.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Agent Observability with TensorZero\n\nAll agent interactions are automatically tracked by TensorZero. Let's explore the observability features."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate collecting feedback on agent performance\nfrom tensorzero import TensorZeroGateway\n\n# We need TensorZero client for feedback collection\ntz_client = TensorZeroGateway.build_http(gateway_url=\"http://localhost:3000\")\n\nprint(\"üìä Agent Observability Features\")\nprint(\"=\" * 40)\n\n# In a real application, you would collect these inference IDs from the agent responses\n# For this demo, we'll simulate the feedback collection process\n\ndef collect_agent_feedback(description: str, rating: float, helpful: bool, comment: str):\n    \"\"\"Simulate feedback collection.\"\"\"\n    try:\n        # In a real scenario, you'd link this to actual inference IDs\n        # For demo purposes, we'll just show the feedback collection process\n        \n        print(f\"‚úÖ Feedback simulated: {description}\")\n        print(f\"   Rating: {rating}/1.0\")\n        print(f\"   Helpful: {helpful}\")\n        print(f\"   Comment: {comment}\")\n        print(f\"   Status: Would be submitted to TensorZero\")\n        \n    except Exception as e:\n        print(f\"‚ùå Feedback collection failed: {e}\")\n\n# Simulate feedback for our test cases\nfeedback_examples = [\n    {\n        \"description\": \"Math calculation test\",\n        \"rating\": 0.95,\n        \"helpful\": True,\n        \"comment\": \"Agent correctly used tools for mathematical calculations\"\n    },\n    {\n        \"description\": \"Time query test\", \n        \"rating\": 0.9,\n        \"helpful\": True,\n        \"comment\": \"Provided accurate time information for requested timezone\"\n    },\n    {\n        \"description\": \"Text analysis test\",\n        \"rating\": 0.85,\n        \"helpful\": True,\n        \"comment\": \"Good text analysis with sentiment and basic statistics\"\n    },\n    {\n        \"description\": \"Multi-step task test\",\n        \"rating\": 0.92,\n        \"helpful\": True,\n        \"comment\": \"Successfully handled multiple tool calls in sequence\"\n    }\n]\n\nfor feedback in feedback_examples:\n    collect_agent_feedback(**feedback)\n\nprint(f\"\\nüåê TensorZero UI: http://localhost:4000\")\nprint(\"üìà In the TensorZero UI you can:\")\nprint(\"   ‚Ä¢ View all agent conversations and tool calls\")\nprint(\"   ‚Ä¢ Monitor performance metrics and costs\")\nprint(\"   ‚Ä¢ Analyze tool usage patterns\")\nprint(\"   ‚Ä¢ A/B test different agent configurations\")\nprint(\"   ‚Ä¢ Set up alerts for performance issues\")\n\nprint(f\"\\nüîç Key Benefits of TensorZero for Agents:\")\nprint(\"   ‚úÖ Automatic observability - no custom logging needed\")\nprint(\"   ‚úÖ Multi-provider support - easy to switch models\")\nprint(\"   ‚úÖ Built-in experimentation - A/B testing made simple\")\nprint(\"   ‚úÖ Cost tracking - monitor LLM usage costs\")\nprint(\"   ‚úÖ Tool call tracing - see exactly how agents use tools\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Performance Analysis\n\nLet's create a simple performance analysis of our agent."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport pandas as pd\nfrom typing import List, Dict, Any\n\ndef benchmark_agent(test_cases: List[Dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"Simple benchmark of our TensorZero agent.\"\"\"\n    results = []\n    \n    console.print(f\"üèÉ [bold]Running Agent Benchmark[/bold] ({len(test_cases)} test cases)\")\n    console.print(\"=\" * 50)\n    \n    for i, test_case in enumerate(test_cases, 1):\n        query = test_case[\"query\"]\n        category = test_case[\"category\"]\n        expected_tools = test_case.get(\"expected_tools\", [])\n        \n        console.print(f\"\\nüß™ [cyan]Test {i}/{len(test_cases)}: {category}[/cyan]\")\n        console.print(f\"Query: {query[:60]}...\")\n        \n        start_time = time.time()\n        \n        try:\n            # Run the agent\n            messages = [{\"role\": \"user\", \"content\": query}]\n            result = agent.invoke({\"messages\": messages})\n            \n            end_time = time.time()\n            duration = round(end_time - start_time, 2)\n            \n            # Count tool calls in the response\n            tool_count = 0\n            response_length = 0\n            \n            if result and \"messages\" in result:\n                for message in result[\"messages\"]:\n                    if hasattr(message, 'tool_calls') and message.tool_calls:\n                        tool_count += len(message.tool_calls)\n                    if hasattr(message, 'content') and message.content:\n                        response_length += len(str(message.content))\n            \n            results.append({\n                \"test_case\": i,\n                \"category\": category,\n                \"query\": query[:50] + \"...\" if len(query) > 50 else query,\n                \"success\": True,\n                \"duration_seconds\": duration,\n                \"tool_calls\": tool_count,\n                \"response_length\": response_length\n            })\n            \n            console.print(f\"‚úÖ [green]Success:[/green] {duration}s, {tool_count} tools, {response_length} chars\")\n            \n        except Exception as e:\n            end_time = time.time()\n            duration = round(end_time - start_time, 2)\n            \n            results.append({\n                \"test_case\": i,\n                \"category\": category,\n                \"query\": query[:50] + \"...\" if len(query) > 50 else query,\n                \"success\": False,\n                \"duration_seconds\": duration,\n                \"tool_calls\": 0,\n                \"response_length\": 0,\n                \"error\": str(e)[:50]\n            })\n            \n            console.print(f\"‚ùå [red]Failed:[/red] {duration}s, Error: {str(e)[:50]}...\")\n    \n    return pd.DataFrame(results)\n\n# Define our test cases\ntest_cases = [\n    {\n        \"category\": \"Math\",\n        \"query\": \"What is the square root of 256?\",\n        \"expected_tools\": [\"python_calculator\"]\n    },\n    {\n        \"category\": \"Time\",\n        \"query\": \"What time is it in PST?\",\n        \"expected_tools\": [\"current_time\"]\n    },\n    {\n        \"category\": \"Analysis\",\n        \"query\": \"Analyze this text: 'The weather is absolutely wonderful today!'\",\n        \"expected_tools\": [\"text_analyzer\"]\n    },\n    {\n        \"category\": \"Multi-step\",\n        \"query\": \"Calculate 15 * 23, then analyze the sentiment of 'great result'\",\n        \"expected_tools\": [\"python_calculator\", \"text_analyzer\"]\n    },\n    {\n        \"category\": \"Conversational\",\n        \"query\": \"Tell me about the benefits of using TensorZero for LLM applications\",\n        \"expected_tools\": []  # No tools expected for this\n    }\n]\n\n# Run the benchmark\nbenchmark_results = benchmark_agent(test_cases)\n\n# Display results summary\nconsole.print(\"\\nüìä [bold]Benchmark Results Summary[/bold]\")\nconsole.print(\"=\" * 40)\n\n# Calculate summary statistics\nif not benchmark_results.empty:\n    summary = benchmark_results.groupby('category').agg({\n        'success': ['count', 'sum'],\n        'duration_seconds': ['mean', 'std'],\n        'tool_calls': 'mean',\n        'response_length': 'mean'\n    }).round(2)\n    \n    print(summary)\n    \n    # Overall statistics\n    total_tests = len(benchmark_results)\n    successful_tests = benchmark_results['success'].sum()\n    avg_duration = benchmark_results['duration_seconds'].mean()\n    avg_tools = benchmark_results['tool_calls'].mean()\n    \n    console.print(f\"\\nüìà [bold]Overall Performance:[/bold]\")\n    console.print(f\"   ‚Ä¢ Success Rate: {successful_tests}/{total_tests} ({100*successful_tests/total_tests:.1f}%)\")\n    console.print(f\"   ‚Ä¢ Average Duration: {avg_duration:.2f} seconds\")\n    console.print(f\"   ‚Ä¢ Average Tool Calls: {avg_tools:.1f}\")\n    \nelse:\n    console.print(\"[red]No benchmark results to display[/red]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Key Insights and Best Practices\n\nSummary of what we learned about building agents with TensorZero."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "console.print(Panel(\"\"\"\n## üéØ TensorZero Agent Integration - Key Learnings\n\n### ‚úÖ What Works Great:\n‚Ä¢ **OpenAI-Compatible Endpoint**: Use `/openai/v1` - no custom wrappers needed!\n‚Ä¢ **LangChain Integration**: `init_chat_model()` works perfectly with TensorZero\n‚Ä¢ **LangGraph Agents**: `create_react_agent()` works out-of-the-box\n‚Ä¢ **Tool Calling**: Both Python tools and TensorZero-configured tools work together\n‚Ä¢ **Automatic Observability**: All interactions tracked without extra code\n\n### üõ†Ô∏è Architecture Patterns:\n‚Ä¢ **Model Setup**: `init_chat_model(\"function/variant\", base_url=\"http://localhost:3000/openai/v1\")`\n‚Ä¢ **Tool Definition**: Use `@tool` decorator for Python tools + TensorZero config for others\n‚Ä¢ **Agent Creation**: Standard LangGraph patterns work unchanged\n‚Ä¢ **Observability**: Leverage TensorZero's built-in tracking and UI\n\n### ‚ö° Performance Benefits:\n‚Ä¢ **Multi-Provider**: Easy switching between OpenAI, Anthropic, xAI, etc.\n‚Ä¢ **Cost Optimization**: Automatic routing and cost tracking\n‚Ä¢ **Experimentation**: A/B testing different models/prompts\n‚Ä¢ **Production Ready**: <1ms latency overhead, enterprise observability\n\n### üîß Best Practices:\n1. **Use TensorZero's OpenAI endpoint** - simplest integration path\n2. **Configure tools in tensorzero.toml** for TensorZero-managed tools\n3. **Define Python tools locally** for custom logic\n4. **Leverage built-in observability** instead of custom logging\n5. **Use variants** for easy model switching and A/B testing\n\n### üöÄ Production Considerations:\n‚Ä¢ **Error Handling**: Robust fallback mechanisms for tool failures\n‚Ä¢ **Rate Limiting**: Manage API costs through TensorZero configuration\n‚Ä¢ **Security**: Validate tool inputs and sanitize outputs\n‚Ä¢ **Monitoring**: Set up alerts using TensorZero's observability features\n‚Ä¢ **Scaling**: Use TensorZero's load balancing and caching\n\n---\n\n**üåê Next Steps**: \n‚Ä¢ Explore TensorZero UI at http://localhost:4000\n‚Ä¢ Try different model variants in tensorzero.toml\n‚Ä¢ Set up feedback collection and optimization\n‚Ä¢ Build multi-agent systems using the same patterns\n\"\"\", title=\"üìö Complete Guide to TensorZero Agents\", border_style=\"green\"))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}