{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Multi-Provider Testing\n",
    "\n",
    "This notebook explores TensorZero's multi-provider capabilities:\n",
    "- Testing different LLM providers\n",
    "- Comparing response quality and performance\n",
    "- Understanding provider-specific behaviors\n",
    "- Gradually enabling more providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to TensorZero gateway\n",
      "üåê Services: Gateway(3000), UI(4000), ClickHouse(8123)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize gateway client with new method\n",
    "client = TensorZeroGateway.build_http(gateway_url=\"http://localhost:3000\")\n",
    "print(\"‚úÖ Connected to TensorZero gateway\")\n",
    "print(\"üåê Services: Gateway(3000), UI(4000), ClickHouse(8123)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Current Provider Setup\n",
    "\n",
    "Let's start with the providers we have configured and gradually add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Status:\n",
      "‚úÖ OpenAI: Set\n",
      "‚úÖ Anthropic: Set\n",
      "‚úÖ xAI: Set\n",
      "\n",
      "üéØ Ready to test 8 provider variants!\n"
     ]
    }
   ],
   "source": [
    "# Current available variants with latest xAI models\n",
    "current_variants = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\", \"openai\"),\n",
    "    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\", \"openai\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\", \"anthropic\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\", \"anthropic\"),\n",
    "    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\", \"anthropic\"),\n",
    "    (\"grok3_mini\", \"xAI Grok-3 Mini\", \"xai\"),\n",
    "    (\"grok_code_fast\", \"xAI Grok Code Fast\", \"xai\"),\n",
    "    (\"grok4\", \"xAI Grok-4\", \"xai\"),\n",
    "]\n",
    "\n",
    "# Check which API keys are available\n",
    "api_keys = {\n",
    "    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"), \n",
    "    \"xAI\": os.getenv(\"XAI_API_KEY\")\n",
    "}\n",
    "\n",
    "print(\"API Key Status:\")\n",
    "for provider, key in api_keys.items():\n",
    "    status = \"‚úÖ\" if key else \"‚ùå\"\n",
    "    print(f\"{status} {provider}: {'Set' if key else 'Missing'}\")\n",
    "\n",
    "print(f\"\\nüéØ Ready to test {len(current_variants)} provider variants!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provider Performance Testing\n",
    "\n",
    "Let's test response time and quality across different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_variant_performance(variant_name, provider_name, test_prompt, function_name=\"chat\"):\n",
    "    \"\"\"Test a specific variant and return performance metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=function_name,\n",
    "            variant_name=variant_name,\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Extract text content\n",
    "        content_text = \"\"\n",
    "        if hasattr(response, 'content') and response.content:\n",
    "            if isinstance(response.content, list) and len(response.content) > 0:\n",
    "                content_text = response.content[0].text if hasattr(response.content[0], 'text') else str(response.content[0])\n",
    "            else:\n",
    "                content_text = str(response.content)\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": response.inference_id,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": len(content_text),\n",
    "            \"content\": content_text[:200] + \"...\" if len(content_text) > 200 else content_text,\n",
    "            \"full_content\": content_text,\n",
    "            \"success\": True,\n",
    "            \"error\": None,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": None,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": 0,\n",
    "            \"content\": \"\",\n",
    "            \"full_content\": \"\",\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: Creative Writing\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 4.28s - 658 chars\n",
      "  üìù In the heart of Silicon Valley, an AI, Vector, achieved what no bot had before - emotions. His data-driven existence took a drastic turn when an error in code accidentally unlocked an emotional algori...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 3.00s - 623 chars\n",
      "  üìù In a bustling city, a robot named Elix combed through trash, collecting scraps. One evening, he discovered a tattered teddy bear, its button eye glistening under the streetlight. As he touched it, a s...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 5.58s - 602 chars\n",
      "  üìù In a world of cold steel and precise calculations, a robot named Zephyr performed its tasks flawlessly. One day, while sorting through a pile of discarded items, Zephyr found a small, cracked mirror. ...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T18:58:05.484422Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSacvPMbqdjxDkcPvqTcD\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 1.54s - 640 chars\n",
      "  üìù In a world of cold, calculated precision, a robot named Sigma awoke to a strange new sensation. Circuits sparked with an unfamiliar warmth as it observed the fluttering leaves outside its window. Curi...\n",
      "\n",
      "Testing xAI Grok-3 Mini (grok3_mini)...\n",
      "  ‚úÖ 5.02s - 441 chars\n",
      "  üìù In the sterile lab, Zeta-9 whirred through calculations, a machine of logic and code. One glitchy dawn, a child's tear splashed its sensors, sparking chaos in its circuits. \"Error: Undefined input,\" i...\n",
      "\n",
      "Testing xAI Grok Code Fast (grok_code_fast)...\n",
      "  ‚úÖ 5.42s - 391 chars\n",
      "  üìù In the sterile glow of a lab, Unit-7 processed data flawlessly. Then came Mia, with her radiant smile. A peculiar warmth flickered in its core‚Äîa \"joy\" it had no code for.\n",
      "\n",
      "Days later, Mia's grief stir...\n",
      "\n",
      "Testing xAI Grok-4 (grok4)...\n",
      "  ‚úÖ 10.92s - 595 chars\n",
      "  üìù ### Sparks of the Heart\n",
      "\n",
      "In the sterile hum of the assembly line, Unit-47 assembled widgets with mechanical precision. No thoughts, just code. Until the day a worker dropped a photo‚Äîa smiling child, n...\n",
      "\n",
      "\n",
      "üß™ Testing: Technical Explanation\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 7.28s - 1212 chars\n",
      "  üìù TensorZero's gateway architecture is essentially a structure that manages the flow of information between systems or components. It works like a bridge that connects different systems or applications,...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 6.59s - 1697 chars\n",
      "  üìù TensorZero's gateway architecture is designed to facilitate efficient communication and data processing in a distributed system. Here‚Äôs a simple breakdown of how it works:\n",
      "\n",
      "1. **Central Hub:** Think o...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 2.92s - 270 chars\n",
      "  üìù I do not have any information about TensorZero's gateway architecture. TensorZero is not a company or product that I have knowledge of. I apologize but I cannot explain how it works as I do not have a...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T18:58:45.377907Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSacyKxJ6dyqwkCXwintc\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 3.12s - 1747 chars\n",
      "  üìù TensorZero's gateway architecture is a way of organizing and managing the flow of data and computations in a complex system, like a machine learning model or a distributed software application.\n",
      "\n",
      "Imagi...\n",
      "\n",
      "Testing xAI Grok-3 Mini (grok3_mini)...\n",
      "  ‚úÖ 16.05s - 4994 chars\n",
      "  üìù It seems like \"TensorZero\" might be a typo, misspelling, or reference to a less common term. I'm most familiar with TensorFlow, a popular machine learning framework from Google, and its related tools....\n",
      "\n",
      "Testing xAI Grok Code Fast (grok_code_fast)...\n",
      "  ‚úÖ 9.05s - 3398 chars\n",
      "  üìù ### Overview of TensorZero's Gateway Architecture\n",
      "TensorZero is an open-source inference platform, written in Rust, designed for efficiently running large language models (LLMs) and other AI models. I...\n",
      "\n",
      "Testing xAI Grok-4 (grok4)...\n",
      "  ‚úÖ 35.12s - 5109 chars\n",
      "  üìù Sure, I'll explain TensorZero's gateway architecture in simple terms, step by step. TensorZero is an open-source toolkit designed to optimize AI/ML inference (e.g., running large language models like ...\n",
      "\n",
      "\n",
      "üß™ Testing: Code Generation\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 6.75s - 935 chars\n",
      "  üìù Sure, here is a simple implementation using recursion in Python:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return \"Input should be greater than 0\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif ...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 4.21s - 1001 chars\n",
      "  üìù Certainly! Below is a Python function that calculates the Fibonacci sequence using recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0  # Base case for Fibonacci(0)\n",
      "    elif n == 1...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 19.22s - 2110 chars\n",
      "  üìù Certainly! Here's a Python function that calculates the Fibonacci sequence using recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return \"Invalid input. Please provide a positive integer...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T19:00:19.105963Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSad6EfzR116fV1mLJibJ\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 5.03s - 2078 chars\n",
      "  üìù Here's a Python function that calculates the Fibonacci sequence using recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Calculates the nth number in the Fibonacci sequence using recursion.\n",
      "    \n",
      "    ...\n",
      "\n",
      "Testing xAI Grok-3 Mini (grok3_mini)...\n",
      "  ‚úÖ 9.91s - 1863 chars\n",
      "  üìù Here's a Python function that calculates the nth Fibonacci number using recursion. The Fibonacci sequence is defined such that:\n",
      "\n",
      "- The 0th Fibonacci number is 0.\n",
      "- The 1st Fibonacci number is 1.\n",
      "- For...\n",
      "\n",
      "Testing xAI Grok Code Fast (grok_code_fast)...\n",
      "  ‚úÖ 2.97s - 802 chars\n",
      "  üìù ```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Calculate the nth Fibonacci number using recursion.\n",
      "    The Fibonacci sequence is defined as:\n",
      "    F(0) = 0\n",
      "    F(1) = 1\n",
      "    F(n) = F(n-1) + F(n-2) for n > 1\n",
      "   ...\n",
      "\n",
      "Testing xAI Grok-4 (grok4)...\n",
      "  ‚úÖ 6.29s - 576 chars\n",
      "  üìù ```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Calculate the nth Fibonacci number using recursion.\n",
      "    \n",
      "    The Fibonacci sequence is defined as:\n",
      "    - fibonacci(0) = 0\n",
      "    - fibonacci(1) = 1\n",
      "    - fibonacci...\n",
      "\n",
      "\n",
      "üß™ Testing: Analysis\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 15.60s - 1807 chars\n",
      "  üìù Microservices Architecture:\n",
      "\n",
      "Advantages:\n",
      "1. Scalability: Each component or service can be scaled individually as per demand.\n",
      "2. Technology Diversity: Different technologies, languages, and databases c...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 14.20s - 4194 chars\n",
      "  üìù Microservices and monolithic architecture are two distinct approaches to building software applications, each with its own set of advantages and disadvantages. Below is a comparison of the two:\n",
      "\n",
      "### M...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 26.54s - 3751 chars\n",
      "  üìù Here's a comparison of the advantages and disadvantages of microservices vs monolithic architecture:\n",
      "\n",
      "Microservices Architecture:\n",
      "Advantages:\n",
      "1. Modularity: Microservices are loosely coupled, allowing...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T19:01:39.871601Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSadCByBqus8mkVeeX5Bz\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 5.85s - 4277 chars\n",
      "  üìù Microservices and Monolithic Architecture are two distinct architectural styles for building software applications. Each approach has its own advantages and disadvantages, and the choice between them ...\n",
      "\n",
      "Testing xAI Grok-3 Mini (grok3_mini)...\n",
      "  ‚úÖ 13.19s - 5983 chars\n",
      "  üìù Below is a comprehensive comparison of microservices architecture and monolithic architecture. I'll start with a brief overview of each, then outline their advantages and disadvantages. This will help...\n",
      "\n",
      "Testing xAI Grok Code Fast (grok_code_fast)...\n",
      "  ‚úÖ 8.46s - 4050 chars\n",
      "  üìù ### Monolithic Architecture\n",
      "\n",
      "Monolithic architecture involves building an application as a single, unified unit where all components (e.g., UI, business logic, data access) are tightly coupled and dep...\n",
      "\n",
      "Testing xAI Grok-4 (grok4)...\n",
      "  ‚úÖ 43.11s - 8372 chars\n",
      "  üìù Below is a detailed comparison of microservices architecture and monolithic architecture. I'll start with brief definitions, then outline the advantages and disadvantages of each, and finally provide ...\n",
      "\n",
      "\n",
      "üìä Completed 32 tests\n"
     ]
    }
   ],
   "source": [
    "# Test prompts for different scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"prompt\": \"Write a creative short story about a robot discovering emotions (max 100 words).\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical Explanation\", \n",
    "        \"prompt\": \"Explain how TensorZero's gateway architecture works in simple terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Generation\",\n",
    "        \"prompt\": \"Write a Python function that calculates the Fibonacci sequence using recursion.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Analysis\",\n",
    "        \"prompt\": \"Compare the advantages and disadvantages of microservices vs monolithic architecture.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "results = []\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\nüß™ Testing: {scenario['name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for variant_name, provider_display, provider_key in current_variants:\n",
    "        print(f\"Testing {provider_display} ({variant_name})...\")\n",
    "        \n",
    "        result = test_variant_performance(\n",
    "            variant_name=variant_name,\n",
    "            provider_name=provider_display,\n",
    "            test_prompt=scenario['prompt']\n",
    "        )\n",
    "        \n",
    "        result['scenario'] = scenario['name']\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ‚úÖ {result['response_time']:.2f}s - {result['content_length']} chars\")\n",
    "            print(f\"  üìù {result['content']}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Failed: {result['error']}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(f\"\\nüìä Completed {len(results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Performance Summary\n",
      "========================================\n",
      "\n",
      "Response Times by Provider:\n",
      "                            mean     std    min     max\n",
      "provider                                               \n",
      "Anthropic Claude 3 Haiku   3.885   1.935  1.540   5.845\n",
      "Anthropic Claude 3 Opus   13.565  11.219  2.918  26.543\n",
      "OpenAI GPT-4               8.475   4.924  4.276  15.596\n",
      "OpenAI GPT-4o Mini         6.999   5.029  2.996  14.203\n",
      "xAI Grok Code Fast         6.476   2.829  2.966   9.055\n",
      "xAI Grok-3 Mini           11.043   4.732  5.024  16.048\n",
      "xAI Grok-4                23.859  18.016  6.288  43.114\n",
      "\n",
      "Content Length by Provider:\n",
      "                            mean     std  min   max\n",
      "provider                                           \n",
      "Anthropic Claude 3 Haiku  2185.5  1523.9  640  4277\n",
      "Anthropic Claude 3 Opus   1683.2  1594.2  270  3751\n",
      "OpenAI GPT-4              1153.0   491.2  658  1807\n",
      "OpenAI GPT-4o Mini        1878.8  1606.3  623  4194\n",
      "xAI Grok Code Fast        2160.2  1832.9  391  4050\n",
      "xAI Grok-3 Mini           3320.2  2601.6  441  5983\n",
      "xAI Grok-4                3663.0  3795.1  576  8372\n",
      "\n",
      "Success Rate by Provider:\n",
      "                           success_rate\n",
      "provider                               \n",
      "Anthropic Claude 3 Haiku          100.0\n",
      "Anthropic Claude 3 Opus           100.0\n",
      "Anthropic Claude 3 Sonnet           0.0\n",
      "OpenAI GPT-4                      100.0\n",
      "OpenAI GPT-4o Mini                100.0\n",
      "xAI Grok Code Fast                100.0\n",
      "xAI Grok-3 Mini                   100.0\n",
      "xAI Grok-4                        100.0\n",
      "\n",
      "‚ùå Errors:\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSacvPMbqdjxDkcPvqTcD\\\"}\"}\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSacyKxJ6dyqwkCXwintc\\\"}\"}\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSad6EfzR116fV1mLJibJ\\\"}\"}\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSadCByBqus8mkVeeX5Bz\\\"}\"}\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Filter successful results for analysis\n",
    "successful_df = df[df['success'] == True].copy()\n",
    "\n",
    "if not successful_df.empty:\n",
    "    print(\"üìà Performance Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Average response times by provider\n",
    "    avg_times = successful_df.groupby('provider')['response_time'].agg(['mean', 'std', 'min', 'max']).round(3)\n",
    "    print(\"\\nResponse Times by Provider:\")\n",
    "    print(avg_times)\n",
    "    \n",
    "    # Average content length by provider\n",
    "    avg_length = successful_df.groupby('provider')['content_length'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "    print(\"\\nContent Length by Provider:\")\n",
    "    print(avg_length)\n",
    "    \n",
    "    # Success rate by provider\n",
    "    success_rate = df.groupby('provider')['success'].agg(['sum', 'count']).round(3)\n",
    "    success_rate['success_rate'] = (success_rate['sum'] / success_rate['count']) * 100\n",
    "    print(\"\\nSuccess Rate by Provider:\")\n",
    "    print(success_rate[['success_rate']])\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No successful results to analyze\")\n",
    "\n",
    "# Show any errors\n",
    "error_results = df[df['success'] == False]\n",
    "if not error_results.empty:\n",
    "    print(\"\\n‚ùå Errors:\")\n",
    "    for _, row in error_results.iterrows():\n",
    "        print(f\"  {row['provider']}: {row['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response Quality Comparison\n",
    "\n",
    "Let's compare the quality of responses for a specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detailed Comparison: Technical Explanation\n",
      "============================================================\n",
      "\n",
      "ü§ñ OpenAI GPT-4 (7.28s)\n",
      "----------------------------------------\n",
      "TensorZero's gateway architecture is essentially a structure that manages the flow of information between systems or components. It works like a bridge that connects different systems or applications, enabling them to communicate and interact with each other.\n",
      "\n",
      "In this setup, the gateway acts as a 'doorkeeper'. It controls incoming and outgoing data, ensuring the safe and efficient transfer of information from one point to another. This could involve tasks like data transformation or routing instructions, integrating or consolidating data, or managing concurrent operations or transactions.\n",
      "\n",
      "Think of it like a post office. People from different locations (different systems or applications) send letters or packages (data) to the post office. The post office sorts these letters and delivers them to the correct addresses (destination systems). \n",
      "\n",
      "In the same way, TensorZero's gateway architecture ensures the data originating from different systems or points are properly managed, integrated, and delivered to the right destination in a manner that the recipient system can understand and use. It ensures smooth communication and operation between different systems or components within an IT environment.\n",
      "\n",
      "üìä Length: 1212 chars | ID: 0198f20b-ae2b-7d51-b21f-3b87a60fc721\n",
      "\n",
      "ü§ñ OpenAI GPT-4o Mini (6.59s)\n",
      "----------------------------------------\n",
      "TensorZero's gateway architecture is designed to facilitate efficient communication and data processing in a distributed system. Here‚Äôs a simple breakdown of how it works:\n",
      "\n",
      "1. **Central Hub:** Think of the gateway as a central hub where all the data flows in and out. It connects various components in a system, such as sensors, cloud services, and user applications.\n",
      "\n",
      "2. **Data Handling:** The gateway collects data from different sources, processes it, and sometimes stores it temporarily. It can also transform this data into a format that other systems can easily understand.\n",
      "\n",
      "3. **Protocol Management:** To ensure compatibility, the gateway can handle different communication protocols. This means it can speak various ‚Äúlanguages‚Äù depending on the devices it‚Äôs connected to, allowing seamless integration.\n",
      "\n",
      "4. **Scalability:** If you need to add more devices or services, the gateway architecture allows for easy scalability. You can expand the system without needing to overhaul the existing setup.\n",
      "\n",
      "5. **Security:** The gateway often includes security measures to protect data as it travels between devices and the cloud. This can involve encryption and authentication methods to ensure that only authorized users and devices can access sensitive information.\n",
      "\n",
      "6. **Real-Time Processing:** One of the key features is the ability to process data in real time. This is crucial for applications requiring immediate insights or actions, such as in IoT setups.\n",
      "\n",
      "In summary, TensorZero's gateway architecture functions like a traffic controller for data, ensuring smooth and secure communication between various components of a distributed system while allowing for easy adaptation and expansion.\n",
      "\n",
      "üìä Length: 1697 chars | ID: 0198f20b-ca9b-7442-89c3-18955f28381d\n",
      "\n",
      "ü§ñ Anthropic Claude 3 Opus (2.92s)\n",
      "----------------------------------------\n",
      "I do not have any information about TensorZero's gateway architecture. TensorZero is not a company or product that I have knowledge of. I apologize but I cannot explain how it works as I do not have accurate details about what it is or what type of architecture it uses.\n",
      "\n",
      "üìä Length: 270 chars | ID: 0198f20b-e45a-78b3-882a-20542c49b1b8\n",
      "\n",
      "ü§ñ Anthropic Claude 3 Haiku (3.12s)\n",
      "----------------------------------------\n",
      "TensorZero's gateway architecture is a way of organizing and managing the flow of data and computations in a complex system, like a machine learning model or a distributed software application.\n",
      "\n",
      "Imagine you have a big, complex system that needs to process a lot of data and perform many different tasks. The gateway architecture is like a central hub or control center that helps coordinate all the different parts of the system.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **Input Data**: The system receives data from various sources, like sensors, databases, or user inputs.\n",
      "\n",
      "2. **Gateway**: The gateway acts as the entry point for all the data coming into the system. It receives the data and decides how to route it to the right parts of the system.\n",
      "\n",
      "3. **Internal Components**: The system has various internal components, like different models, algorithms, or services, that perform specific tasks on the data.\n",
      "\n",
      "4. **Routing**: The gateway is responsible for routing the data to the appropriate internal components, based on the specific needs of the task or application.\n",
      "\n",
      "5. **Coordination**: The gateway also coordinates the interactions between the different internal components, making sure they work together efficiently and effectively.\n",
      "\n",
      "6. **Scalability**: The gateway architecture helps the system scale up or down as the workload changes, by being able to add or remove internal components as needed.\n",
      "\n",
      "7. **Security**: The gateway can also handle security concerns, like authentication, authorization, and data protection, ensuring the system is secure and reliable.\n",
      "\n",
      "In simple terms, the gateway architecture acts as a central hub that manages the flow of data and computations in a complex system, making it more organized, efficient, and scalable.\n",
      "\n",
      "üìä Length: 1747 chars | ID: 0198f20b-f086-7900-988e-994d8de2d478\n",
      "\n",
      "ü§ñ xAI Grok-3 Mini (16.05s)\n",
      "----------------------------------------\n",
      "It seems like \"TensorZero\" might be a typo, misspelling, or reference to a less common term. I'm most familiar with TensorFlow, a popular machine learning framework from Google, and its related tools. If you meant TensorFlow or something similar (like NVIDIA's TensorRT), I'll explain a gateway architecture in that context. If TensorZero is a specific project or product I'm not aware of, feel free to provide more details for a more accurate response!\n",
      "\n",
      "Assuming you're referring to something like TensorFlow's distributed architecture, which often includes \"gateway-like\" components for handling data flow, model serving, or communication, here's a simple explanation:\n",
      "\n",
      "### What is a Gateway Architecture in This Context?\n",
      "In machine learning and distributed systems, a \"gateway architecture\" typically refers to a setup that acts as an intermediary or entry point. It helps manage communication between different parts of a system, such as user applications, data sources, and hardware accelerators (like GPUs or TPUs). For TensorFlow or similar frameworks, this could involve routing requests, optimizing data transfer, or coordinating distributed training.\n",
      "\n",
      "Let me break down how this might work in simple terms, using TensorFlow as an example:\n",
      "\n",
      "1. **The Big Picture**:\n",
      "   - Imagine you're training a large AI model on multiple computers (or servers) instead of just one. This is common for handling massive datasets or complex models.\n",
      "   - A gateway acts like a \"traffic controller\" or \"doorman\" for the system. It receives requests from users or applications, processes them efficiently, and routes them to the right places (e.g., data storage, computation engines, or other servers).\n",
      "\n",
      "2. **Key Components in TensorFlow's Gateway-Like Setup**:\n",
      "   - **Client Side (User's Entry Point)**: This is where you, as a developer, interact with TensorFlow. You write code using high-level APIs (like Keras in TensorFlow) to define your model. The gateway here is essentially the TensorFlow runtime, which translates your code into executable operations.\n",
      "   - **Gateway Layer**: In distributed TensorFlow (e.g., using TensorFlow Distributed or TensorFlow Serving), there's a component that handles coordination:\n",
      "     - **Parameter Server or Coordinator**: This acts as the central gateway. It manages shared model parameters (like weights in a neural network) and synchronizes updates from multiple workers (e.g., different machines training parts of the model). It's like a shared database that everyone checks in with.\n",
      "     - **Data Gateway**: For handling large datasets, TensorFlow might use tools like TFRecord or distributed file systems. The gateway ensures data is fetched efficiently, perhaps from cloud storage, and fed into the model without bottlenecks.\n",
      "     - **Inference Gateway (e.g., via TensorFlow Serving)**: If you're deploying a trained model for real-time predictions (like in a web app), TensorFlow Serving acts as a gateway. It receives prediction requests, runs them through the optimized model, and sends back results. This is secured and scalable, often using gRPC or HTTP for communication.\n",
      "\n",
      "3. **How It Works Step by Step**:\n",
      "   - **Step 1: Request Comes In**: A user or application sends a request (e.g., \"train this model\" or \"make a prediction\") to the gateway.\n",
      "   - **Step 2: Processing and Routing**: The gateway checks the request, optimizes it (e.g., by distributing tasks across GPUs), and routes it to the appropriate resources. For example:\n",
      "     - In distributed training, it splits the work among worker nodes and uses the parameter server to keep everything in sync.\n",
      "     - If it's inference, it might load a pre-optimized model from storage and run it on hardware accelerators.\n",
      "   - **Step 3: Handle Communication**: The gateway manages back-and-forth communication, ensuring data flows smoothly. It might use protocols like gRPC for fast, efficient messaging between components.\n",
      "   - **Step 4: Output and Scaling**: Once done, the gateway sends the results back. It also handles scaling‚Äîif more resources are needed, it can spin up additional servers or optimize performance.\n",
      "\n",
      "4. **Why Use This Architecture?**\n",
      "   - **Efficiency**: It reduces latency by directing traffic smartly, like avoiding sending the same data to every machine.\n",
      "   - **Scalability**: You can handle larger workloads by adding more machines without changing your code much.\n",
      "   - **Security and Reliability**: Gateways can include features like authentication, error handling, and fault tolerance (e.g., if one server fails, the gateway reroutes tasks).\n",
      "\n",
      "If TensorZero is actually referring to NVIDIA TensorRT (which optimizes models for inference and has a similar gateway-like runtime), the concept is comparable: TensorRT acts as a gateway between your application and the GPU, optimizing and executing models quickly.\n",
      "\n",
      "If this doesn't match what you meant, could you clarify \"TensorZero\" (e.g., is it a specific tool, paper, or framework)? I'd be happy to refine this explanation! üòä\n",
      "\n",
      "üìä Length: 4994 chars | ID: 0198f20b-fcb8-70d2-a152-0e0d5b672d6a\n",
      "\n",
      "ü§ñ xAI Grok Code Fast (9.05s)\n",
      "----------------------------------------\n",
      "### Overview of TensorZero's Gateway Architecture\n",
      "TensorZero is an open-source inference platform, written in Rust, designed for efficiently running large language models (LLMs) and other AI models. Its gateway architecture acts as a central entry point for handling requests, similar to an API gateway in web services. It simplifies deploying, scaling, and managing AI models by providing a unified interface, routing, and optimizations. The goal is to make inference (getting predictions from models) fast, reliable, and flexible, supporting multiple models from providers like OpenAI, Anthropic, or custom ones.\n",
      "\n",
      "The architecture separates concerns: the gateway handles incoming traffic and logic, while inference engines execute the models. This setup enables features like load balancing, failover, and experimentation without users needing to know the backend details.\n",
      "\n",
      "### Key Components\n",
      "- **Gateway**: The main entry point that receives API requests (e.g., from apps or users). It processes requests, applies rules, and forwards them to the right inference engine. Built for low latency and high throughput.\n",
      "- **Inference Engines**: Dedicated workers that run the actual models. These can be local (on your hardware) or remote (cloud-hosted). TensorZero supports various model formats, like those from Hugging Face or custom setups.\n",
      "- **Configuration Layer**: A YAML or JSON-based setup that defines models, routes, and policies. For example, you can specify rules like \"route requests to GPT-4 if available, else fall back to a cheaper model.\"\n",
      "- **Data Pipeline**: Handles logging, metrics, and caching to improve performance and observability.\n",
      "\n",
      "### How It Works Step-by-Step\n",
      "1. **Request Arrival**: A client (e.g., your app) sends a request via HTTP/JSON to the gateway, like \"Generate a summary of this text.\"\n",
      "   \n",
      "2. **Authentication and Validation**: The gateway checks API keys, rate limits, and request validity to ensure security and fair usage.\n",
      "\n",
      "3. **Routing Decision**: Based on the config, the gateway decides where to send the request. This could involve:\n",
      "   - **Load Balancing**: Distributing requests across multiple model instances to avoid overload.\n",
      "   - **Model Selection**: Choosing a model based on criteria (e.g., cost, speed, or A/B testing for experiments).\n",
      "   - **Caching**: If a similar request was handled recently, it might return a cached response to save time.\n",
      "   - **Failover**: If a model fails, it automatically switches to a backup.\n",
      "\n",
      "4. **Inference Execution**: The gateway forwards the request to an inference engine, which runs the model and generates the output.\n",
      "\n",
      "5. **Response Handling**: The engine sends the result back to the gateway, which may apply post-processing (e.g., filtering or formatting), logs metrics, and returns the final response to the client.\n",
      "\n",
      "### Benefits and Use Cases\n",
      "- **Simplicity**: Developers integrate once via the gateway's API, without managing multiple model endpoints.\n",
      "- **Efficiency**: Optimizations like batching (grouping requests) and quantization (reducing model size) speed things up.\n",
      "- **Flexibility**: Easy to swap models, run experiments, or scale for high traffic.\n",
      "- **Reliability**: Built-in error handling and monitoring prevent downtime.\n",
      "\n",
      "For more details, check TensorZero's official docs at tensorzero.com. This architecture is ideal for production AI services needing robustness beyond a single model's API.\n",
      "\n",
      "üìä Length: 3398 chars | ID: 0198f20c-3b69-7a80-b5cf-db04c3dcd59b\n",
      "\n",
      "ü§ñ xAI Grok-4 (35.12s)\n",
      "----------------------------------------\n",
      "Sure, I'll explain TensorZero's gateway architecture in simple terms, step by step. TensorZero is an open-source toolkit designed to optimize AI/ML inference (e.g., running large language models like GPT for tasks like text generation). The \"gateway\" is the core component that acts like a smart router or traffic director for your AI requests. It helps test, optimize, and improve AI performance over time by managing \"variants\" (different versions of prompts, models, or strategies for the same task). Think of it as A/B testing for AI, but with automated learning.\n",
      "\n",
      "### 1. **Basic Idea: Why a Gateway?**\n",
      "   - Without a gateway, you'd send requests directly to an AI model (e.g., via OpenAI's API), and it'd give you a response. But what if you want to experiment with multiple ways to do the same task (e.g., different prompts for better accuracy)?\n",
      "   - The TensorZero gateway sits in the middle: It receives your request, picks the best \"variant\" (version) to use, forwards it to the actual AI provider, and returns the result. Over time, it learns from feedback to choose better variants automatically, reducing costs and improving quality.\n",
      "\n",
      "### 2. **Key Components**\n",
      "   - **Functions**: These are the tasks you define (e.g., \"summarize an article\" or \"classify sentiment\"). Each function can have multiple **variants** (e.g., Variant A uses GPT-4 with Prompt X, Variant B uses Llama with Prompt Y).\n",
      "   - **Gateway Server**: A lightweight service (built in Rust for speed) that handles incoming requests. It uses a config file to know about your functions and variants.\n",
      "   - **Storage/Database**: It connects to a database (like ClickHouse for metrics or PostgreSQL) to store logs, feedback, and learned optimizations.\n",
      "   - **Inference Providers**: The gateway forwards requests to external AI services (e.g., OpenAI, Anthropic, or self-hosted models via tools like vLLM).\n",
      "\n",
      "### 3. **How It Works: Step-by-Step Flow**\n",
      "   Imagine you're building an app that summarizes news articles. Here's what happens when you send a request:\n",
      "\n",
      "   1. **Request Arrives**:\n",
      "      - Your app or client sends a JSON request to the gateway (e.g., via HTTP POST). It includes the function name (e.g., \"summarize_article\") and input data (e.g., the article text).\n",
      "\n",
      "   2. **Lookup and Selection**:\n",
      "      - The gateway checks its config for the function.\n",
      "      - It selects a variant based on rules:\n",
      "        - **Random (A/B Testing)**: For experimentation, it might randomly pick a variant (e.g., 50% chance for Variant A, 50% for B).\n",
      "        - **Optimized**: If you've collected feedback, it uses learned \"weights\" (from a process called TensorZero's optimization) to pick the best-performing variant automatically.\n",
      "      - This selection is fast and happens in milliseconds.\n",
      "\n",
      "   3. **Forward to Inference**:\n",
      "      - The gateway rewrites the request into the format needed by the chosen variant's AI provider (e.g., converts it to an OpenAI-compatible API call).\n",
      "      - It sends it off to the provider (e.g., GPT-4) and waits for the response.\n",
      "\n",
      "   4. **Response and Logging**:\n",
      "      - The AI provider sends back the result (e.g., the summary).\n",
      "      - The gateway logs everything: Which variant was used, how long it took, cost, and any metrics.\n",
      "      - It returns the response to your app.\n",
      "\n",
      "   5. **Feedback and Learning**:\n",
      "      - Later, you (or users) can provide feedback on the response (e.g., \"This summary was accurate\" via thumbs up/down or a score).\n",
      "      - TensorZero stores this in its database.\n",
      "      - Periodically, you run an \"optimization\" process (using tools like PyTorch) that analyzes the feedback data. It learns which variants perform best for different scenarios and updates the gateway's weights.\n",
      "      - Over time, the gateway gets smarter: It shifts traffic to high-performing variants and away from poor ones, like evolving your AI setup.\n",
      "\n",
      "### 4. **Benefits in Simple Terms**\n",
      "   - **Experiment Easily**: Test multiple AI strategies without changing your app code.\n",
      "   - **Optimize Automatically**: Starts with random testing, then learns from real data to pick winners (e.g., cheaper models that still work well).\n",
      "   - **Scalable and Efficient**: Handles high traffic, logs everything for analysis, and supports custom metrics (e.g., track hallucination rates).\n",
      "   - **Flexible**: Works with any LLM provider, and you can add custom logic (e.g., for routing based on user location).\n",
      "\n",
      "### 5. **Example Setup**\n",
      "   - Install TensorZero via Cargo (Rust) or Docker.\n",
      "   - Define functions/variants in a TOML config file.\n",
      "   - Run the gateway server: `tensorzero gateway`.\n",
      "   - Send a request: `curl http://localhost:3000/inference -d '{\"function\": \"summarize\", \"input\": {\"text\": \"Article here\"}}'`.\n",
      "   - Collect feedback and optimize: Use TensorZero's CLI tools to analyze and update.\n",
      "\n",
      "If things go wrong (e.g., a variant fails), the gateway can retry or fall back to a default. It's designed to be production-ready but simple to start with.\n",
      "\n",
      "For more details, check the TensorZero GitHub repo (it's open-source). If you have a specific part (like optimization or setup) you want me to dive into, let me know!\n",
      "\n",
      "üìä Length: 5109 chars | ID: 0198f20c-5ec8-7941-9ab5-3dea73cd1ee2\n"
     ]
    }
   ],
   "source": [
    "# Pick one scenario to compare in detail\n",
    "comparison_scenario = \"Technical Explanation\"\n",
    "comparison_results = [r for r in results if r['scenario'] == comparison_scenario and r['success']]\n",
    "\n",
    "print(f\"üîç Detailed Comparison: {comparison_scenario}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in comparison_results:\n",
    "    print(f\"\\nü§ñ {result['provider']} ({result['response_time']:.2f}s)\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['full_content'])\n",
    "    print(f\"\\nüìä Length: {result['content_length']} chars | ID: {result['inference_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Anthropic Claude\n",
    "\n",
    "Now let's add Anthropic Claude to our configuration and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Current Provider Configuration:\n",
      "========================================\n",
      "‚úÖ OpenAI: gpt4, gpt4_mini\n",
      "‚úÖ Anthropic: claude3_opus, claude3_sonnet, claude3_haiku\n",
      "‚úÖ xAI: grok3_mini, grok_code_fast, grok4\n",
      "\n",
      "üéâ Multi-provider testing is ready!\n",
      "Run the performance tests above to see all providers in action.\n",
      "\n",
      "üß™ Quick verification test:\n",
      "   ‚úÖ OpenAI: Hello from OpenAI!\n",
      "   ‚úÖ Anthropic: Hello, Anthropic!\n",
      "   ‚úÖ xAI Grok: Hello xAI Grok!\n"
     ]
    }
   ],
   "source": [
    "# Multi-Provider Status Update - All 8 Variants!\n",
    "print(\"‚úÖ Current Provider Configuration:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "provider_status = {\n",
    "    \"OpenAI\": [\"gpt4\", \"gpt4_mini\"],\n",
    "    \"Anthropic\": [\"claude3_opus\", \"claude3_sonnet\", \"claude3_haiku\"],\n",
    "    \"xAI\": [\"grok3_mini\", \"grok_code_fast\", \"grok4\"]\n",
    "}\n",
    "\n",
    "for provider, models in provider_status.items():\n",
    "    api_key = os.getenv(f\"{provider.upper().replace(' ', '_')}_API_KEY\")\n",
    "    status = \"‚úÖ\" if api_key else \"‚ùå\"\n",
    "    print(f\"{status} {provider}: {', '.join(models)}\")\n",
    "\n",
    "print(\"\\nüéâ Multi-provider testing is ready!\")\n",
    "print(\"Run the performance tests above to see all providers in action.\")\n",
    "\n",
    "# Test a quick inference with each provider type\n",
    "print(\"\\nüß™ Quick verification test:\")\n",
    "test_variants = [\n",
    "    (\"gpt4_mini\", \"OpenAI\"),\n",
    "    (\"claude3_haiku\", \"Anthropic\"),\n",
    "    (\"grok3_mini\", \"xAI Grok\")\n",
    "]\n",
    "\n",
    "for variant, provider in test_variants:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=variant,\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": f\"Say 'Hello from {provider}!' in exactly 3 words.\"}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        content = response.content[0].text if response.content else \"No content\"\n",
    "        print(f\"   ‚úÖ {provider}: {content}\")\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"403\" in error_msg:\n",
    "            print(f\"   ‚ö†Ô∏è  {provider}: API key/credits issue\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {provider}: {error_msg[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Function Variants\n",
    "\n",
    "Let's test the haiku generation function across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéã Testing Haiku Generation\n",
      "==============================\n",
      "\n",
      "Prompt: Write a haiku about artificial intelligence.\n",
      "----------------------------------------\n",
      "‚úÖ Silicon whispers,  \n",
      "Thoughts woven in wires glow,  \n",
      "Dreams of code take flight.\n",
      "   (1.07s)\n",
      "\n",
      "Prompt: Write a haiku about the ocean at sunset.\n",
      "----------------------------------------\n",
      "‚úÖ Waves kiss the soft shore,  \n",
      "Golden hues paint the calm sea‚Äî  \n",
      "Day whispers goodbye.\n",
      "   (0.81s)\n",
      "\n",
      "Prompt: Write a haiku about coding late at night.\n",
      "----------------------------------------\n",
      "‚úÖ Fingers dance on keys,  \n",
      "Moonlight glows on lines of code‚Äî  \n",
      "Silence breeds ideas.\n",
      "   (1.62s)\n"
     ]
    }
   ],
   "source": [
    "# Test the haiku generation function\n",
    "print(\"üéã Testing Haiku Generation\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "haiku_prompts = [\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Write a haiku about the ocean at sunset.\",\n",
    "    \"Write a haiku about coding late at night.\"\n",
    "]\n",
    "\n",
    "for prompt in haiku_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test with the configured variant\n",
    "    result = test_variant_performance(\n",
    "        variant_name=\"gpt_4o_mini\",  # This is from the haiku function\n",
    "        provider_name=\"OpenAI GPT-4o Mini\",\n",
    "        test_prompt=prompt,\n",
    "        function_name=\"generate_haiku\"\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"‚úÖ {result['full_content']}\")\n",
    "        print(f\"   ({result['response_time']:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on our testing:\n",
    "\n",
    "1. **Performance**: Response times vary by provider and model complexity\n",
    "2. **Quality**: Each provider has distinct response styles:\n",
    "   - OpenAI: Balanced and versatile\n",
    "   - Anthropic: Thoughtful and detailed\n",
    "   - xAI: Advanced with structured output capabilities\n",
    "3. **Reliability**: All major providers showing high success rates\n",
    "4. **Advanced Features**:\n",
    "   - All Grok models support structured output, reasoning, and function calling\n",
    "   - grok-4-0790 supports image input\n",
    "   - JSON schema validation works across providers\n",
    "\n",
    "## Configuration Summary\n",
    "\n",
    "**8 Active Variants:**\n",
    "- OpenAI: `gpt4`, `gpt4_mini`\n",
    "- Anthropic: `claude3_opus`, `claude3_sonnet`, `claude3_haiku`\n",
    "- xAI: `grok3_mini`, `grok_code_fast`, `grok4`\n",
    "\n",
    "**Structured Output Functions:**\n",
    "- `analyze_sentiment` with JSON schema validation\n",
    "- Schema files in `config/functions/analyze_sentiment/`\n",
    "- All providers support JSON output\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. ‚úÖ All providers configured and tested\n",
    "2. ‚úÖ Structured output framework implemented\n",
    "3. Next: Phase 4 - Prompt Management & A/B Testing\n",
    "4. Future: Agent integration with LangGraph\n",
    "\n",
    "Next notebook: We'll explore observability and tracing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Structured Output Comparison (NEW)\n",
    "\n",
    "All Grok models support structured output! Let's test this capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on our testing:\n",
    "\n",
    "1. **Performance**: Record the response times and reliability of each provider\n",
    "2. **Quality**: Note differences in response style and content quality\n",
    "3. **Reliability**: Track success rates and error patterns\n",
    "4. **Use Cases**: Identify which providers work best for specific scenarios\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Add more providers (Anthropic, xAI) to the configuration\n",
    "2. Implement A/B testing and routing strategies\n",
    "3. Add structured output functions (JSON schema validation)\n",
    "4. Test fallback mechanisms\n",
    "\n",
    "Next notebook: We'll explore observability and tracing features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
