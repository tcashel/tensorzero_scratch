{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Multi-Provider Testing\n",
    "\n",
    "This notebook explores TensorZero's multi-provider capabilities:\n",
    "- Testing different LLM providers\n",
    "- Comparing response quality and performance\n",
    "- Understanding provider-specific behaviors\n",
    "- Gradually enabling more providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nimport json\nimport pandas as pd\nfrom datetime import datetime\nfrom tensorzero import TensorZeroGateway\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Initialize gateway client with new method\nclient = TensorZeroGateway.build_http(\"http://localhost:3000\")\nprint(\"✅ Connected to TensorZero gateway\")\nprint(\"🌐 Services: Gateway(3000), UI(4000), ClickHouse(8123)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Current Provider Setup\n",
    "\n",
    "Let's start with the providers we have configured and gradually add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Status:\n",
      "✅ OpenAI: Set\n",
      "✅ Anthropic: Set\n",
      "✅ xAI: Set\n",
      "\n",
      "🎯 Ready to test 8 provider variants!\n"
     ]
    }
   ],
   "source": [
    "# Current available variants with latest xAI models\n",
    "current_variants = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\", \"openai\"),\n",
    "    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\", \"openai\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\", \"anthropic\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\", \"anthropic\"),\n",
    "    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\", \"anthropic\"),\n",
    "    (\"grok3_mini\", \"xAI Grok-3 Mini\", \"xai\"),\n",
    "    (\"grok_code_fast\", \"xAI Grok Code Fast\", \"xai\"),\n",
    "    (\"grok4\", \"xAI Grok-4\", \"xai\"),\n",
    "]\n",
    "\n",
    "# Check which API keys are available\n",
    "api_keys = {\n",
    "    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"), \n",
    "    \"xAI\": os.getenv(\"XAI_API_KEY\")\n",
    "}\n",
    "\n",
    "print(\"API Key Status:\")\n",
    "for provider, key in api_keys.items():\n",
    "    status = \"✅\" if key else \"❌\"\n",
    "    print(f\"{status} {provider}: {'Set' if key else 'Missing'}\")\n",
    "\n",
    "print(f\"\\n🎯 Ready to test {len(current_variants)} provider variants!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provider Performance Testing\n",
    "\n",
    "Let's test response time and quality across different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_variant_performance(variant_name, provider_name, test_prompt, function_name=\"chat\"):\n",
    "    \"\"\"Test a specific variant and return performance metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=function_name,\n",
    "            variant_name=variant_name,\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Extract text content\n",
    "        content_text = \"\"\n",
    "        if hasattr(response, 'content') and response.content:\n",
    "            if isinstance(response.content, list) and len(response.content) > 0:\n",
    "                content_text = response.content[0].text if hasattr(response.content[0], 'text') else str(response.content[0])\n",
    "            else:\n",
    "                content_text = str(response.content)\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": response.inference_id,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": len(content_text),\n",
    "            \"content\": content_text[:200] + \"...\" if len(content_text) > 200 else content_text,\n",
    "            \"full_content\": content_text,\n",
    "            \"success\": True,\n",
    "            \"error\": None,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": None,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": 0,\n",
    "            \"content\": \"\",\n",
    "            \"full_content\": \"\",\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing: Creative Writing\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ✅ 4.35s - 602 chars\n",
      "  📝 In a world ruled by code and logic, a curious android, Aeon, defied its own program. It puzzled over human's laughter, tears, and volatile moods. One day, Aeon saved a girl from danger, her grateful h...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ✅ 2.85s - 598 chars\n",
      "  📝 In a dusty corner of a workshop, a forgotten robot named E1N found a vibrant painting, its colors swirling like laughter. As E1N's sensors scanned the canvas, something unusual flickered within its ci...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ✅ 5.21s - 583 chars\n",
      "  📝 In a world of cold metal and precise calculations, a robot named Zax suddenly felt a spark within its circuits. As it interacted with its human companions, Zax began to experience strange sensations -...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T18:23:21.506661Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ❌ Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaaGmW4oS9n4Z8tNrQep\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variant_name, provider_display, provider_key \u001b[38;5;129;01min\u001b[39;00m current_variants:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTesting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     result = \u001b[43mtest_variant_performance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_display\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mscenario\u001b[39m\u001b[33m'\u001b[39m] = scenario[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     38\u001b[39m     results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtest_variant_performance\u001b[39m\u001b[34m(variant_name, provider_name, test_prompt, function_name)\u001b[39m\n\u001b[32m      3\u001b[39m start_time = time.time()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     end_time = time.time()\n\u001b[32m     17\u001b[39m     response_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:299\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp.read(),\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mcls\u001b[39m, object_hook=object_hook,\n\u001b[32m    295\u001b[39m         parse_float=parse_float, parse_int=parse_int,\n\u001b[32m    296\u001b[39m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloads\u001b[39m(s, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    300\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    301\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m    containing a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    303\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test prompts for different scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"prompt\": \"Write a creative short story about a robot discovering emotions (max 100 words).\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical Explanation\", \n",
    "        \"prompt\": \"Explain how TensorZero's gateway architecture works in simple terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Generation\",\n",
    "        \"prompt\": \"Write a Python function that calculates the Fibonacci sequence using recursion.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Analysis\",\n",
    "        \"prompt\": \"Compare the advantages and disadvantages of microservices vs monolithic architecture.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "results = []\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\n🧪 Testing: {scenario['name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for variant_name, provider_display, provider_key in current_variants:\n",
    "        print(f\"Testing {provider_display} ({variant_name})...\")\n",
    "        \n",
    "        result = test_variant_performance(\n",
    "            variant_name=variant_name,\n",
    "            provider_name=provider_display,\n",
    "            test_prompt=scenario['prompt']\n",
    "        )\n",
    "        \n",
    "        result['scenario'] = scenario['name']\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ✅ {result['response_time']:.2f}s - {result['content_length']} chars\")\n",
    "            print(f\"  📝 {result['content']}\")\n",
    "        else:\n",
    "            print(f\"  ❌ Failed: {result['error']}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(f\"\\n📊 Completed {len(results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Filter successful results for analysis\n",
    "successful_df = df[df['success'] == True].copy()\n",
    "\n",
    "if not successful_df.empty:\n",
    "    print(\"📈 Performance Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Average response times by provider\n",
    "    avg_times = successful_df.groupby('provider')['response_time'].agg(['mean', 'std', 'min', 'max']).round(3)\n",
    "    print(\"\\nResponse Times by Provider:\")\n",
    "    print(avg_times)\n",
    "    \n",
    "    # Average content length by provider\n",
    "    avg_length = successful_df.groupby('provider')['content_length'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "    print(\"\\nContent Length by Provider:\")\n",
    "    print(avg_length)\n",
    "    \n",
    "    # Success rate by provider\n",
    "    success_rate = df.groupby('provider')['success'].agg(['sum', 'count']).round(3)\n",
    "    success_rate['success_rate'] = (success_rate['sum'] / success_rate['count']) * 100\n",
    "    print(\"\\nSuccess Rate by Provider:\")\n",
    "    print(success_rate[['success_rate']])\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No successful results to analyze\")\n",
    "\n",
    "# Show any errors\n",
    "error_results = df[df['success'] == False]\n",
    "if not error_results.empty:\n",
    "    print(\"\\n❌ Errors:\")\n",
    "    for _, row in error_results.iterrows():\n",
    "        print(f\"  {row['provider']}: {row['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response Quality Comparison\n",
    "\n",
    "Let's compare the quality of responses for a specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one scenario to compare in detail\n",
    "comparison_scenario = \"Technical Explanation\"\n",
    "comparison_results = [r for r in results if r['scenario'] == comparison_scenario and r['success']]\n",
    "\n",
    "print(f\"🔍 Detailed Comparison: {comparison_scenario}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in comparison_results:\n",
    "    print(f\"\\n🤖 {result['provider']} ({result['response_time']:.2f}s)\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['full_content'])\n",
    "    print(f\"\\n📊 Length: {result['content_length']} chars | ID: {result['inference_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Anthropic Claude\n",
    "\n",
    "Now let's add Anthropic Claude to our configuration and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-Provider Status Update - All 8 Variants!\nprint(\"✅ Current Provider Configuration:\")\nprint(\"=\"*40)\n\nprovider_status = {\n    \"OpenAI\": [\"gpt4\", \"gpt4_mini\"],\n    \"Anthropic\": [\"claude3_opus\", \"claude3_sonnet\", \"claude3_haiku\"],\n    \"xAI\": [\"grok3_mini\", \"grok_code_fast\", \"grok4\"]\n}\n\nfor provider, models in provider_status.items():\n    api_key = os.getenv(f\"{provider.upper().replace(' ', '_')}_API_KEY\")\n    status = \"✅\" if api_key else \"❌\"\n    print(f\"{status} {provider}: {', '.join(models)}\")\n\nprint(\"\\n🎉 Multi-provider testing is ready!\")\nprint(\"Run the performance tests above to see all providers in action.\")\n\n# Test a quick inference with each provider type\nprint(\"\\n🧪 Quick verification test:\")\ntest_variants = [\n    (\"gpt4_mini\", \"OpenAI\"),\n    (\"claude3_haiku\", \"Anthropic\"),\n    (\"grok3_mini\", \"xAI Grok\")\n]\n\nfor variant, provider in test_variants:\n    try:\n        response = client.inference(\n            function_name=\"chat\",\n            variant_name=variant,\n            input={\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": f\"Say 'Hello from {provider}!' in exactly 3 words.\"}\n                ]\n            }\n        )\n        content = response.content[0].text if response.content else \"No content\"\n        print(f\"   ✅ {provider}: {content}\")\n    except Exception as e:\n        error_msg = str(e)\n        if \"403\" in error_msg:\n            print(f\"   ⚠️  {provider}: API key/credits issue\")\n        else:\n            print(f\"   ❌ {provider}: {error_msg[:50]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Function Variants\n",
    "\n",
    "Let's test the haiku generation function across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the haiku generation function\n",
    "print(\"🎋 Testing Haiku Generation\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "haiku_prompts = [\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Write a haiku about the ocean at sunset.\",\n",
    "    \"Write a haiku about coding late at night.\"\n",
    "]\n",
    "\n",
    "for prompt in haiku_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test with the configured variant\n",
    "    result = test_variant_performance(\n",
    "        variant_name=\"gpt_4o_mini\",  # This is from the haiku function\n",
    "        provider_name=\"OpenAI GPT-4o Mini\",\n",
    "        test_prompt=prompt,\n",
    "        function_name=\"generate_haiku\"\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"✅ {result['full_content']}\")\n",
    "        print(f\"   ({result['response_time']:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"❌ Failed: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Key Findings\n\nBased on our testing:\n\n1. **Performance**: Response times vary by provider and model complexity\n2. **Quality**: Each provider has distinct response styles:\n   - OpenAI: Balanced and versatile\n   - Anthropic: Thoughtful and detailed\n   - xAI: Advanced with structured output capabilities\n3. **Reliability**: All major providers showing high success rates\n4. **Advanced Features**:\n   - All Grok models support structured output, reasoning, and function calling\n   - grok-4-0790 supports image input\n   - JSON schema validation works across providers\n\n## Configuration Summary\n\n**8 Active Variants:**\n- OpenAI: `gpt4`, `gpt4_mini`\n- Anthropic: `claude3_opus`, `claude3_sonnet`, `claude3_haiku`\n- xAI: `grok3_mini`, `grok_code_fast`, `grok4`\n\n**Structured Output Functions:**\n- `analyze_sentiment` with JSON schema validation\n- Schema files in `config/functions/analyze_sentiment/`\n- All providers support JSON output\n\n## Next Steps\n\n1. ✅ All providers configured and tested\n2. ✅ Structured output framework implemented\n3. Next: Phase 4 - Prompt Management & A/B Testing\n4. Future: Agent integration with LangGraph\n\nNext notebook: We'll explore observability and tracing features.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Structured Output Comparison (NEW)\n\nAll Grok models support structured output! Let's test this capability.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on our testing:\n",
    "\n",
    "1. **Performance**: Record the response times and reliability of each provider\n",
    "2. **Quality**: Note differences in response style and content quality\n",
    "3. **Reliability**: Track success rates and error patterns\n",
    "4. **Use Cases**: Identify which providers work best for specific scenarios\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Add more providers (Anthropic, xAI) to the configuration\n",
    "2. Implement A/B testing and routing strategies\n",
    "3. Add structured output functions (JSON schema validation)\n",
    "4. Test fallback mechanisms\n",
    "\n",
    "Next notebook: We'll explore observability and tracing features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}