{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Multi-Provider Testing\n",
    "\n",
    "This notebook explores TensorZero's multi-provider capabilities:\n",
    "- Testing different LLM providers\n",
    "- Comparing response quality and performance\n",
    "- Understanding provider-specific behaviors\n",
    "- Gradually enabling more providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-28T15:55:23.193236Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero\u001b[0m\u001b[2m:\u001b[0m TensorZeroGateway.__init__ is deprecated. Use TensorZeroGateway.build_http or TensorZeroGateway.build_embedded instead.\n",
      "‚úÖ Connected to TensorZero gateway\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize gateway client\n",
    "client = TensorZeroGateway(\"http://localhost:3000\")\n",
    "print(\"‚úÖ Connected to TensorZero gateway\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Current Provider Setup\n",
    "\n",
    "Let's start with the providers we have configured and gradually add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Current available variants with latest xAI models\ncurrent_variants = [\n    (\"gpt4\", \"OpenAI GPT-4\", \"openai\"),\n    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\", \"openai\"),\n    (\"claude3_opus\", \"Anthropic Claude 3 Opus\", \"anthropic\"),\n    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\", \"anthropic\"),\n    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\", \"anthropic\"),\n    (\"grok3_mini\", \"xAI Grok-3 Mini\", \"xai\"),\n    (\"grok_code_fast\", \"xAI Grok Code Fast\", \"xai\"),\n    (\"grok4\", \"xAI Grok-4\", \"xai\"),\n]\n\n# Check which API keys are available\napi_keys = {\n    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"), \n    \"xAI\": os.getenv(\"XAI_API_KEY\")\n}\n\nprint(\"API Key Status:\")\nfor provider, key in api_keys.items():\n    status = \"‚úÖ\" if key else \"‚ùå\"\n    print(f\"{status} {provider}: {'Set' if key else 'Missing'}\")\n\nprint(f\"\\nüéØ Ready to test {len(current_variants)} provider variants!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provider Performance Testing\n",
    "\n",
    "Let's test response time and quality across different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_variant_performance(variant_name, provider_name, test_prompt, function_name=\"chat\"):\n",
    "    \"\"\"Test a specific variant and return performance metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=function_name,\n",
    "            variant_name=variant_name,\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Extract text content\n",
    "        content_text = \"\"\n",
    "        if hasattr(response, 'content') and response.content:\n",
    "            if isinstance(response.content, list) and len(response.content) > 0:\n",
    "                content_text = response.content[0].text if hasattr(response.content[0], 'text') else str(response.content[0])\n",
    "            else:\n",
    "                content_text = str(response.content)\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": response.inference_id,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": len(content_text),\n",
    "            \"content\": content_text[:200] + \"...\" if len(content_text) > 200 else content_text,\n",
    "            \"full_content\": content_text,\n",
    "            \"success\": True,\n",
    "            \"error\": None,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": None,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": 0,\n",
    "            \"content\": \"\",\n",
    "            \"full_content\": \"\",\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: Creative Writing\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 8.33s - 602 chars\n",
      "  üìù Eve, the robot, watched as the sun dipped into the horizon, emanating a golden glow. Puzzled by the warmth surging within her circuit board, she couldn‚Äôt quite figure out this new sensation. In her lo...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 3.32s - 632 chars\n",
      "  üìù In a forgotten corner of a bustling city, a robot named R1A stumbled upon a dusty old book titled \"Feelings.\" As R1A's metallic fingers flipped the pages, words leaped to life: love, sadness, joy. Eac...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 5.10s - 593 chars\n",
      "  üìù In a world of cold steel and precise calculations, a robot named Zeta suddenly experienced an unfamiliar sensation. As it interacted with its human companion, a warmth spread through its circuits, and...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T15:55:40.189932Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaP1ZV4XXK3WECkSwf1n\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 1.84s - 777 chars\n",
      "  üìù Here is a short story about a robot discovering emotions (97 words):\n",
      "\n",
      "Beep. Boop. The robot's metallic chassis whirred to life, its sensors scanning the unfamiliar surroundings. As it moved, a strange...\n",
      "\n",
      "\n",
      "üß™ Testing: Technical Explanation\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 5.74s - 1036 chars\n",
      "  üìù TensorZero's gateway architecture functions as a bridge between different elements of a system, allowing them to communicate with each other conveniently.\n",
      "\n",
      "This system consists of two main components:...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 6.97s - 2028 chars\n",
      "  üìù Sure! TensorZero is designed to streamline and enhance the way data is processed and transmitted, particularly in the context of machine learning and AI applications. Here‚Äôs a simple breakdown of its ...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 2.76s - 244 chars\n",
      "  üìù I do not have any specific information about TensorZero's gateway architecture. As an AI assistant created by Anthropic to be helpful, harmless, and honest, I do not have insider knowledge of other co...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T15:55:57.718085Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaP2rS4Vi4iqDHAzDgeC\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 1.20s - 448 chars\n",
      "  üìù I do not have specific information about \"TensorZero's gateway architecture\". TensorFlow is a popular open-source machine learning framework, but I am not aware of a product or architecture called \"Te...\n",
      "\n",
      "\n",
      "üß™ Testing: Code Generation\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 10.96s - 1044 chars\n",
      "  üìù The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones, usually starting with 0 and 1. Here is a Python function to calculate the Fibonacci sequence us...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 5.54s - 1204 chars\n",
      "  üìù Certainly! Below is a Python function that calculates the Fibonacci sequence using recursion. The function returns the Fibonacci number at a given position `n`.\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    # Base...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 18.59s - 1969 chars\n",
      "  üìù Here's a Python function that calculates the Fibonacci sequence using recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return \"Invalid input. n must be a positive integer.\"\n",
      "    elif n ==...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T15:56:34.229859Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaP5YaMFJ7H9rPff2GrD\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 3.90s - 1746 chars\n",
      "  üìù Here's a Python function that calculates the Fibonacci sequence using recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Calculate the nth Fibonacci number using recursion.\n",
      "    \n",
      "    Args:\n",
      "        n (...\n",
      "\n",
      "\n",
      "üß™ Testing: Analysis\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  ‚úÖ 19.98s - 2152 chars\n",
      "  üìù Microservices Architecture:\n",
      "\n",
      "Advantages:\n",
      "1. Scalable: Each service can be scaled independently based on demand.\n",
      "2. Independent Deployment: Microservices can be deployed, upgraded, or even undeployed i...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  ‚úÖ 13.82s - 4099 chars\n",
      "  üìù Microservices and monolithic architecture are two different approaches to software development, each with its own set of advantages and disadvantages. Here‚Äôs a comparison of both architectures:\n",
      "\n",
      "### M...\n",
      "\n",
      "Testing Anthropic Claude 3 Opus (claude3_opus)...\n",
      "  ‚úÖ 24.17s - 3210 chars\n",
      "  üìù Microservices and monolithic architectures are two different approaches to designing and building software applications. Here's a comparison of their advantages and disadvantages:\n",
      "\n",
      "Microservices Archi...\n",
      "\n",
      "Testing Anthropic Claude 3 Sonnet (claude3_sonnet)...\n",
      "\u001b[2m2025-08-28T15:57:36.311557Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "  ‚ùå Failed: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaPA818MvhxScQAQRecJ\\\"}\"}\n",
      "\n",
      "Testing Anthropic Claude 3 Haiku (claude3_haiku)...\n",
      "  ‚úÖ 5.63s - 3687 chars\n",
      "  üìù Microservices and monolithic architecture are two different architectural approaches for building software applications. Here's a comparison of their advantages and disadvantages:\n",
      "\n",
      "Microservices Advan...\n",
      "\n",
      "\n",
      "üìä Completed 20 tests\n"
     ]
    }
   ],
   "source": [
    "# Test prompts for different scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"prompt\": \"Write a creative short story about a robot discovering emotions (max 100 words).\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical Explanation\", \n",
    "        \"prompt\": \"Explain how TensorZero's gateway architecture works in simple terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Generation\",\n",
    "        \"prompt\": \"Write a Python function that calculates the Fibonacci sequence using recursion.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Analysis\",\n",
    "        \"prompt\": \"Compare the advantages and disadvantages of microservices vs monolithic architecture.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "results = []\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\nüß™ Testing: {scenario['name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for variant_name, provider_display, provider_key in current_variants:\n",
    "        print(f\"Testing {provider_display} ({variant_name})...\")\n",
    "        \n",
    "        result = test_variant_performance(\n",
    "            variant_name=variant_name,\n",
    "            provider_name=provider_display,\n",
    "            test_prompt=scenario['prompt']\n",
    "        )\n",
    "        \n",
    "        result['scenario'] = scenario['name']\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  ‚úÖ {result['response_time']:.2f}s - {result['content_length']} chars\")\n",
    "            print(f\"  üìù {result['content']}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Failed: {result['error']}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(f\"\\nüìä Completed {len(results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Performance Summary\n",
      "========================================\n",
      "\n",
      "Response Times by Provider:\n",
      "                            mean     std    min     max\n",
      "provider                                               \n",
      "Anthropic Claude 3 Haiku   3.144   2.018  1.204   5.628\n",
      "Anthropic Claude 3 Opus   12.657  10.374  2.759  24.172\n",
      "OpenAI GPT-4              11.251   6.195  5.737  19.976\n",
      "OpenAI GPT-4o Mini         7.412   4.528  3.320  13.819\n",
      "\n",
      "Content Length by Provider:\n",
      "                            mean     std  min   max\n",
      "provider                                           \n",
      "Anthropic Claude 3 Haiku  1664.5  1456.6  448  3687\n",
      "Anthropic Claude 3 Opus   1504.0  1359.4  244  3210\n",
      "OpenAI GPT-4              1208.5   662.0  602  2152\n",
      "OpenAI GPT-4o Mini        1990.8  1517.8  632  4099\n",
      "\n",
      "Success Rate by Provider:\n",
      "                           success_rate\n",
      "provider                               \n",
      "Anthropic Claude 3 Haiku          100.0\n",
      "Anthropic Claude 3 Opus           100.0\n",
      "Anthropic Claude 3 Sonnet           0.0\n",
      "OpenAI GPT-4                      100.0\n",
      "OpenAI GPT-4o Mini                100.0\n",
      "\n",
      "‚ùå Errors:\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaP1ZV4XXK3WECkSwf1n\\\"}\"}\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaP2rS4Vi4iqDHAzDgeC\\\"}\"}\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaP5YaMFJ7H9rPff2GrD\\\"}\"}\n",
      "  Anthropic Claude 3 Sonnet: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaPA818MvhxScQAQRecJ\\\"}\"}\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Filter successful results for analysis\n",
    "successful_df = df[df['success'] == True].copy()\n",
    "\n",
    "if not successful_df.empty:\n",
    "    print(\"üìà Performance Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Average response times by provider\n",
    "    avg_times = successful_df.groupby('provider')['response_time'].agg(['mean', 'std', 'min', 'max']).round(3)\n",
    "    print(\"\\nResponse Times by Provider:\")\n",
    "    print(avg_times)\n",
    "    \n",
    "    # Average content length by provider\n",
    "    avg_length = successful_df.groupby('provider')['content_length'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "    print(\"\\nContent Length by Provider:\")\n",
    "    print(avg_length)\n",
    "    \n",
    "    # Success rate by provider\n",
    "    success_rate = df.groupby('provider')['success'].agg(['sum', 'count']).round(3)\n",
    "    success_rate['success_rate'] = (success_rate['sum'] / success_rate['count']) * 100\n",
    "    print(\"\\nSuccess Rate by Provider:\")\n",
    "    print(success_rate[['success_rate']])\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No successful results to analyze\")\n",
    "\n",
    "# Show any errors\n",
    "error_results = df[df['success'] == False]\n",
    "if not error_results.empty:\n",
    "    print(\"\\n‚ùå Errors:\")\n",
    "    for _, row in error_results.iterrows():\n",
    "        print(f\"  {row['provider']}: {row['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response Quality Comparison\n",
    "\n",
    "Let's compare the quality of responses for a specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detailed Comparison: Technical Explanation\n",
      "============================================================\n",
      "\n",
      "ü§ñ OpenAI GPT-4 (5.74s)\n",
      "----------------------------------------\n",
      "TensorZero's gateway architecture functions as a bridge between different elements of a system, allowing them to communicate with each other conveniently.\n",
      "\n",
      "This system consists of two main components: the user interface (UI) or application, and the backend systems that provide the functionality and data, such as databases or servers.\n",
      "\n",
      "Users access the UI, which talks to the gateway. The gateway then communicates with the backend servers to retrieve or manipulate data according to the user‚Äôs requests. The requested data or action response is sent back to the gateway, which passes it on to the UI for the user to see.\n",
      "\n",
      "Essentially, the gateway serves as a middleman or translator, ensuring that the different system elements can understand each other and work together to achieve the desired result, regardless of their individual characteristics or differences. This also provides a single common point through which all requests and responses pass, simplifying the system architecture and making it easier to manage and maintain.\n",
      "\n",
      "üìä Length: 1036 chars | ID: 0198f164-58da-72f2-b0f4-e79c17fc4d1a\n",
      "\n",
      "ü§ñ OpenAI GPT-4o Mini (6.97s)\n",
      "----------------------------------------\n",
      "Sure! TensorZero is designed to streamline and enhance the way data is processed and transmitted, particularly in the context of machine learning and AI applications. Here‚Äôs a simple breakdown of its gateway architecture:\n",
      "\n",
      "1. **Gateway Function**: At its core, the gateway acts as a bridge between different components or services. It handles incoming requests and directs them to the appropriate services or resources. Think of it as a traffic controller that decides where the data needs to go.\n",
      "\n",
      "2. **Data Processing**: When data comes in, the gateway can pre-process it. This may involve cleaning the data, transforming it into the right format, or aggregating information from multiple sources. This step ensures that the data is ready for analysis or machine learning tasks.\n",
      "\n",
      "3. **Routing**: The gateway intelligently routes the processed data to various machine learning models or other services depending on the request. For example, if a user wants to perform image recognition, the gateway sends the data to the specific model designed for that task.\n",
      "\n",
      "4. **Scalability**: TensorZero‚Äôs architecture allows for easy scalability. If there‚Äôs a sudden increase in requests, the gateway can distribute the load across multiple instances of services to ensure that everything runs smoothly without significant delays.\n",
      "\n",
      "5. **Integration**: The gateway is designed to easily integrate with various databases, APIs, and other tools. This means it can work seamlessly with different data sources and applications.\n",
      "\n",
      "6. **Monitoring and Security**: The gateway also monitors the traffic and can implement security measures to protect sensitive data and ensure that only authorized requests access the services.\n",
      "\n",
      "In summary, TensorZero‚Äôs gateway architecture is about efficiently managing the flow of data, ensuring that requests are handled promptly, and providing a robust framework that can adapt as needs grow. It simplifies interactions between different services and optimizes the overall performance of machine learning tasks.\n",
      "\n",
      "üìä Length: 2028 chars | ID: 0198f164-6f42-7900-94aa-b0dcd882246b\n",
      "\n",
      "ü§ñ Anthropic Claude 3 Opus (2.76s)\n",
      "----------------------------------------\n",
      "I do not have any specific information about TensorZero's gateway architecture. As an AI assistant created by Anthropic to be helpful, harmless, and honest, I do not have insider knowledge of other companies' technical architectures or systems.\n",
      "\n",
      "üìä Length: 244 chars | ID: 0198f164-8a79-7463-a323-cb810d2d4c5f\n",
      "\n",
      "ü§ñ Anthropic Claude 3 Haiku (1.20s)\n",
      "----------------------------------------\n",
      "I do not have specific information about \"TensorZero's gateway architecture\". TensorFlow is a popular open-source machine learning framework, but I am not aware of a product or architecture called \"TensorZero\". If you could provide more details about what you are referring to, I'd be happy to try to explain it in simple terms. Without more context, I cannot give a meaningful explanation of how a specific \"TensorZero gateway architecture\" works.\n",
      "\n",
      "üìä Length: 448 chars | ID: 0198f164-961f-77c2-aa18-26de5ebb785b\n"
     ]
    }
   ],
   "source": [
    "# Pick one scenario to compare in detail\n",
    "comparison_scenario = \"Technical Explanation\"\n",
    "comparison_results = [r for r in results if r['scenario'] == comparison_scenario and r['success']]\n",
    "\n",
    "print(f\"üîç Detailed Comparison: {comparison_scenario}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in comparison_results:\n",
    "    print(f\"\\nü§ñ {result['provider']} ({result['response_time']:.2f}s)\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['full_content'])\n",
    "    print(f\"\\nüìä Length: {result['content_length']} chars | ID: {result['inference_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Anthropic Claude\n",
    "\n",
    "Now let's add Anthropic Claude to our configuration and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Anthropic Claude variants are now ACTIVE in the configuration!\n",
      "‚úÖ OpenAI GPT-4 and GPT-4o Mini are working\n",
      "‚úÖ Anthropic Claude 3 Opus, Sonnet, and Haiku are configured\n",
      "‚ö†Ô∏è  xAI Grok requires credits purchase\n",
      "\n",
      "üéâ Multi-provider testing is ready!\n",
      "Run the performance tests above to see all providers in action.\n",
      "\n",
      "üß™ Quick verification test:\n",
      "   ‚úÖ Claude 3 Haiku: Hello from Claude!\n",
      "   üÜî Inference ID: 0198f168-4725-7cb3-bb17-ed810c1b0dc6\n"
     ]
    }
   ],
   "source": [
    "# Multi-Provider Status Update\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if anthropic_key:\n",
    "    print(\"‚úÖ Anthropic Claude variants are now ACTIVE in the configuration!\")\n",
    "    print(\"‚úÖ OpenAI GPT-4 and GPT-4o Mini are working\")\n",
    "    print(\"‚úÖ Anthropic Claude 3 Opus, Sonnet, and Haiku are configured\")\n",
    "    print(\"‚ö†Ô∏è  xAI Grok requires credits purchase\")\n",
    "    \n",
    "    print(\"\\nüéâ Multi-provider testing is ready!\")\n",
    "    print(\"Run the performance tests above to see all providers in action.\")\n",
    "    \n",
    "    # Test a quick inference to verify\n",
    "    print(\"\\nüß™ Quick verification test:\")\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=\"claude3_haiku\",\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"Say 'Hello from Claude!' in exactly 3 words.\"}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        content = response.content[0].text if response.content else \"No content\"\n",
    "        print(f\"   ‚úÖ Claude 3 Haiku: {content}\")\n",
    "        print(f\"   üÜî Inference ID: {response.inference_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Claude test failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Anthropic API key not found. Add it to .env file to test Claude variants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Function Variants\n",
    "\n",
    "Let's test the haiku generation function across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéã Testing Haiku Generation\n",
      "==============================\n",
      "\n",
      "Prompt: Write a haiku about artificial intelligence.\n",
      "----------------------------------------\n",
      "‚úÖ Bytes of thought converge,  \n",
      "Silent minds weaving the dreams,  \n",
      "Future's echo speaks.\n",
      "   (1.27s)\n",
      "\n",
      "Prompt: Write a haiku about the ocean at sunset.\n",
      "----------------------------------------\n",
      "‚úÖ Golden waves whisper,  \n",
      "Horizon meets fiery sky,  \n",
      "Day's kiss fades to night.\n",
      "   (0.83s)\n",
      "\n",
      "Prompt: Write a haiku about coding late at night.\n",
      "----------------------------------------\n",
      "‚úÖ Fingers dance on keys,  \n",
      "Moonlight threads through quiet rooms,  \n",
      "Dreams of logic flow.\n",
      "   (0.91s)\n"
     ]
    }
   ],
   "source": [
    "# Test the haiku generation function\n",
    "print(\"üéã Testing Haiku Generation\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "haiku_prompts = [\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Write a haiku about the ocean at sunset.\",\n",
    "    \"Write a haiku about coding late at night.\"\n",
    "]\n",
    "\n",
    "for prompt in haiku_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test with the configured variant\n",
    "    result = test_variant_performance(\n",
    "        variant_name=\"gpt_4o_mini\",  # This is from the haiku function\n",
    "        provider_name=\"OpenAI GPT-4o Mini\",\n",
    "        test_prompt=prompt,\n",
    "        function_name=\"generate_haiku\"\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"‚úÖ {result['full_content']}\")\n",
    "        print(f\"   ({result['response_time']:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on our testing:\n",
    "\n",
    "1. **Performance**: Record the response times and reliability of each provider\n",
    "2. **Quality**: Note differences in response style and content quality\n",
    "3. **Reliability**: Track success rates and error patterns\n",
    "4. **Use Cases**: Identify which providers work best for specific scenarios\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Add more providers (Anthropic, xAI) to the configuration\n",
    "2. Implement A/B testing and routing strategies\n",
    "3. Add structured output functions (JSON schema validation)\n",
    "4. Test fallback mechanisms\n",
    "\n",
    "Next notebook: We'll explore observability and tracing features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}