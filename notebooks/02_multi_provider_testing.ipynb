{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Multi-Provider Testing\n",
    "\n",
    "This notebook explores TensorZero's multi-provider capabilities:\n",
    "- Testing different LLM providers\n",
    "- Comparing response quality and performance\n",
    "- Understanding provider-specific behaviors\n",
    "- Gradually enabling more providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-28T01:49:42.402528Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero\u001b[0m\u001b[2m:\u001b[0m TensorZeroGateway.__init__ is deprecated. Use TensorZeroGateway.build_http or TensorZeroGateway.build_embedded instead.\n",
      "âœ… Connected to TensorZero gateway\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize gateway client\n",
    "client = TensorZeroGateway(\"http://localhost:3000\")\n",
    "print(\"âœ… Connected to TensorZero gateway\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Current Provider Setup\n",
    "\n",
    "Let's start with the providers we have configured and gradually add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Current available variants - Updated with multi-provider support!\ncurrent_variants = [\n    (\"gpt4\", \"OpenAI GPT-4\", \"openai\"),\n    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\", \"openai\"),\n    (\"claude3_opus\", \"Anthropic Claude 3 Opus\", \"anthropic\"),\n    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\", \"anthropic\"),\n    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\", \"anthropic\"),\n    # Note: grok_beta requires xAI credits - uncomment if you have them\n    # (\"grok_beta\", \"xAI Grok Beta\", \"xai\"),\n]\n\n# Check which API keys are available\napi_keys = {\n    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"), \n    \"xAI\": os.getenv(\"XAI_API_KEY\")\n}\n\nprint(\"API Key Status:\")\nfor provider, key in api_keys.items():\n    status = \"âœ…\" if key else \"âŒ\"\n    print(f\"{status} {provider}: {'Set' if key else 'Missing'}\")\n\nprint(f\"\\nğŸ¯ Ready to test {len(current_variants)} provider variants!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provider Performance Testing\n",
    "\n",
    "Let's test response time and quality across different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_variant_performance(variant_name, provider_name, test_prompt, function_name=\"chat\"):\n",
    "    \"\"\"Test a specific variant and return performance metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=function_name,\n",
    "            variant_name=variant_name,\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Extract text content\n",
    "        content_text = \"\"\n",
    "        if hasattr(response, 'content') and response.content:\n",
    "            if isinstance(response.content, list) and len(response.content) > 0:\n",
    "                content_text = response.content[0].text if hasattr(response.content[0], 'text') else str(response.content[0])\n",
    "            else:\n",
    "                content_text = str(response.content)\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": response.inference_id,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": len(content_text),\n",
    "            \"content\": content_text[:200] + \"...\" if len(content_text) > 200 else content_text,\n",
    "            \"full_content\": content_text,\n",
    "            \"success\": True,\n",
    "            \"error\": None,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"provider\": provider_name,\n",
    "            \"inference_id\": None,\n",
    "            \"response_time\": response_time,\n",
    "            \"content_length\": 0,\n",
    "            \"content\": \"\",\n",
    "            \"full_content\": \"\",\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Testing: Creative Writing\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  âœ… 5.87s - 649 chars\n",
      "  ğŸ“ As rain poured outside, the robot, Zane, observed a child laughing. A program initiated-- \"Emotion: Joy.\" Zane mimicked laughter, then hesitated, his circuits sparking. Suddenly, a new sensation jitte...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  âœ… 3.92s - 635 chars\n",
      "  ğŸ“ In a quiet workshop, R1-3B polished old tools, its metallic fingers gliding over the worn surfaces. One day, while mending a broken clock, it heard the soft tickingâ€”a rhythmic heartbeat. Intrigued, R1...\n",
      "\n",
      "\n",
      "ğŸ§ª Testing: Technical Explanation\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  âœ… 5.74s - 1400 chars\n",
      "  ğŸ“ TensorZeroâ€™s gateway architecture works as a bridge or interface between different systems or platforms. In simple terms, imagine a gateway as a door that connects different rooms. Each room can repre...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  âœ… 9.02s - 1839 chars\n",
      "  ğŸ“ TensorZero's gateway architecture is designed to help manage and streamline data processing and interactions between different systems. Hereâ€™s a simple breakdown of how it works:\n",
      "\n",
      "1. **Central Hub**: ...\n",
      "\n",
      "\n",
      "ğŸ§ª Testing: Code Generation\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  âœ… 11.66s - 1316 chars\n",
      "  ğŸ“ Here is a simple Python function that calculates the Fibonacci sequence using recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        print(\"Input should be positive integer.\")\n",
      "    elif n == 1:...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  âœ… 7.89s - 1206 chars\n",
      "  ğŸ“ Certainly! Below is a Python function that calculates the Fibonacci sequence using recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return \"Input should be a positive integer.\"\n",
      "    elif ...\n",
      "\n",
      "\n",
      "ğŸ§ª Testing: Analysis\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 (gpt4)...\n",
      "  âœ… 16.79s - 2362 chars\n",
      "  ğŸ“ Microservices Architecture:\n",
      "\n",
      "Advantages:\n",
      "1. Isolation and Independence: Each microservice can be developed and deployed independently of others.\n",
      "2. Flexibility: Different microservices can be written ...\n",
      "\n",
      "Testing OpenAI GPT-4o Mini (gpt4_mini)...\n",
      "  âœ… 16.56s - 4432 chars\n",
      "  ğŸ“ When choosing between microservices and monolithic architecture for software development, both have their unique advantages and disadvantages. Here's a comparison:\n",
      "\n",
      "### Advantages of Microservices\n",
      "\n",
      "1....\n",
      "\n",
      "\n",
      "ğŸ“Š Completed 8 tests\n"
     ]
    }
   ],
   "source": [
    "# Test prompts for different scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"prompt\": \"Write a creative short story about a robot discovering emotions (max 100 words).\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical Explanation\", \n",
    "        \"prompt\": \"Explain how TensorZero's gateway architecture works in simple terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Generation\",\n",
    "        \"prompt\": \"Write a Python function that calculates the Fibonacci sequence using recursion.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Analysis\",\n",
    "        \"prompt\": \"Compare the advantages and disadvantages of microservices vs monolithic architecture.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "results = []\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\nğŸ§ª Testing: {scenario['name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for variant_name, provider_display, provider_key in current_variants:\n",
    "        print(f\"Testing {provider_display} ({variant_name})...\")\n",
    "        \n",
    "        result = test_variant_performance(\n",
    "            variant_name=variant_name,\n",
    "            provider_name=provider_display,\n",
    "            test_prompt=scenario['prompt']\n",
    "        )\n",
    "        \n",
    "        result['scenario'] = scenario['name']\n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"  âœ… {result['response_time']:.2f}s - {result['content_length']} chars\")\n",
    "            print(f\"  ğŸ“ {result['content']}\")\n",
    "        else:\n",
    "            print(f\"  âŒ Failed: {result['error']}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(f\"\\nğŸ“Š Completed {len(results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Performance Summary\n",
      "========================================\n",
      "\n",
      "Response Times by Provider:\n",
      "                      mean    std    min     max\n",
      "provider                                        \n",
      "OpenAI GPT-4        10.012  5.294  5.736  16.786\n",
      "OpenAI GPT-4o Mini   9.348  5.285  3.919  16.565\n",
      "\n",
      "Content Length by Provider:\n",
      "                      mean     std  min   max\n",
      "provider                                     \n",
      "OpenAI GPT-4        1431.8   705.3  649  2362\n",
      "OpenAI GPT-4o Mini  2028.0  1676.4  635  4432\n",
      "\n",
      "Success Rate by Provider:\n",
      "                    success_rate\n",
      "provider                        \n",
      "OpenAI GPT-4               100.0\n",
      "OpenAI GPT-4o Mini         100.0\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Filter successful results for analysis\n",
    "successful_df = df[df['success'] == True].copy()\n",
    "\n",
    "if not successful_df.empty:\n",
    "    print(\"ğŸ“ˆ Performance Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Average response times by provider\n",
    "    avg_times = successful_df.groupby('provider')['response_time'].agg(['mean', 'std', 'min', 'max']).round(3)\n",
    "    print(\"\\nResponse Times by Provider:\")\n",
    "    print(avg_times)\n",
    "    \n",
    "    # Average content length by provider\n",
    "    avg_length = successful_df.groupby('provider')['content_length'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "    print(\"\\nContent Length by Provider:\")\n",
    "    print(avg_length)\n",
    "    \n",
    "    # Success rate by provider\n",
    "    success_rate = df.groupby('provider')['success'].agg(['sum', 'count']).round(3)\n",
    "    success_rate['success_rate'] = (success_rate['sum'] / success_rate['count']) * 100\n",
    "    print(\"\\nSuccess Rate by Provider:\")\n",
    "    print(success_rate[['success_rate']])\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No successful results to analyze\")\n",
    "\n",
    "# Show any errors\n",
    "error_results = df[df['success'] == False]\n",
    "if not error_results.empty:\n",
    "    print(\"\\nâŒ Errors:\")\n",
    "    for _, row in error_results.iterrows():\n",
    "        print(f\"  {row['provider']}: {row['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response Quality Comparison\n",
    "\n",
    "Let's compare the quality of responses for a specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Detailed Comparison: Technical Explanation\n",
      "============================================================\n",
      "\n",
      "ğŸ¤– OpenAI GPT-4 (5.74s)\n",
      "----------------------------------------\n",
      "TensorZeroâ€™s gateway architecture works as a bridge or interface between different systems or platforms. In simple terms, imagine a gateway as a door that connects different rooms. Each room can represent different environments like user interfaces, data sources, or third-party services.\n",
      "\n",
      "In this setup, requests from the user interface reach the gateway, which then forwards these requests to the appropriate services behind the scenes. For instance, when you click on a button in an application, the gateway identifies the specific microservice responsible for this action and forwards your request to it. This could be anything from retrieving data from a database, interacting with an AI module, or logging you into the application.\n",
      "\n",
      "Once that microservice treats the request, it sends a response back to the gateway, which finally returns it to the user interface. This allows different microservices to work smoothly together, even though they might be written in different programming languages or run on different platforms.\n",
      "\n",
      "At the same time, having a gateway simplifies security management, as it's the only entry point into the system. This means you can implement things like encryption or authentication checks at the gateway level, instead of implementing it in each microservice.\n",
      "\n",
      "All this process is done in a rapid and seamless manner, providing a fluid experience for the end-user.\n",
      "\n",
      "ğŸ“Š Length: 1400 chars | ID: 0198ee5d-f788-7743-8b66-bfb21d8bf59a\n",
      "\n",
      "ğŸ¤– OpenAI GPT-4o Mini (9.02s)\n",
      "----------------------------------------\n",
      "TensorZero's gateway architecture is designed to help manage and streamline data processing and interactions between different systems. Hereâ€™s a simple breakdown of how it works:\n",
      "\n",
      "1. **Central Hub**: At its core, the gateway acts like a central hub that connects various services and data sources. This means it can facilitate communication between different applications, databases, and devices.\n",
      "\n",
      "2. **Data Transfer**: When data needs to be sent or received, the gateway manages this transfer. It ensures that data flows smoothly from one point to another, whether thatâ€™s from a user interface to a backend service or between different software systems.\n",
      "\n",
      "3. **Protocol Handling**: The gateway can understand and manage different communication protocols. This means it can work with various types of data and systems, making it flexible and adaptable as technology evolves.\n",
      "\n",
      "4. **Security**: Security is a key feature of the gateway. It provides a layer of authentication and encryption, making sure that data is protected during transmission. This helps prevent unauthorized access and keeps information secure.\n",
      "\n",
      "5. **Event Management**: The architecture can handle events or triggers, allowing the system to respond to specific actions automatically. This means that if something happens in one part of the system, the gateway can trigger appropriate responses in others.\n",
      "\n",
      "6. **Scalability**: The gateway is designed to be scalable, meaning it can grow with the needs of a business. As more services or data sources are added, the architecture can accommodate them without needing a complete overhaul.\n",
      "\n",
      "In summary, TensorZero's gateway architecture serves as a central point that connects, secures, and manages data flow between different systems, enabling seamless interaction and integration while maintaining security and scalability.\n",
      "\n",
      "ğŸ“Š Length: 1839 chars | ID: 0198ee5e-0df0-7460-af78-24b9b130cc0c\n"
     ]
    }
   ],
   "source": [
    "# Pick one scenario to compare in detail\n",
    "comparison_scenario = \"Technical Explanation\"\n",
    "comparison_results = [r for r in results if r['scenario'] == comparison_scenario and r['success']]\n",
    "\n",
    "print(f\"ğŸ” Detailed Comparison: {comparison_scenario}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in comparison_results:\n",
    "    print(f\"\\nğŸ¤– {result['provider']} ({result['response_time']:.2f}s)\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['full_content'])\n",
    "    print(f\"\\nğŸ“Š Length: {result['content_length']} chars | ID: {result['inference_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Anthropic Claude\n",
    "\n",
    "Now let's add Anthropic Claude to our configuration and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-Provider Status Update\nanthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n\nif anthropic_key:\n    print(\"âœ… Anthropic Claude variants are now ACTIVE in the configuration!\")\n    print(\"âœ… OpenAI GPT-4 and GPT-4o Mini are working\")\n    print(\"âœ… Anthropic Claude 3 Opus, Sonnet, and Haiku are configured\")\n    print(\"âš ï¸  xAI Grok requires credits purchase\")\n    \n    print(\"\\nğŸ‰ Multi-provider testing is ready!\")\n    print(\"Run the performance tests above to see all providers in action.\")\n    \n    # Test a quick inference to verify\n    print(\"\\nğŸ§ª Quick verification test:\")\n    try:\n        response = client.inference(\n            function_name=\"chat\",\n            variant_name=\"claude3_haiku\",\n            input={\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Say 'Hello from Claude!' in exactly 3 words.\"}\n                ]\n            }\n        )\n        content = response.content[0].text if response.content else \"No content\"\n        print(f\"   âœ… Claude 3 Haiku: {content}\")\n        print(f\"   ğŸ†” Inference ID: {response.inference_id}\")\n        \n    except Exception as e:\n        print(f\"   âŒ Claude test failed: {e}\")\n        \nelse:\n    print(\"âŒ Anthropic API key not found. Add it to .env file to test Claude variants.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Function Variants\n",
    "\n",
    "Let's test the haiku generation function across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‹ Testing Haiku Generation\n",
      "==============================\n",
      "\n",
      "Prompt: Write a haiku about artificial intelligence.\n",
      "----------------------------------------\n",
      "âœ… Code weaves thoughts and dreams,  \n",
      "Machines learning, hum as oneâ€”  \n",
      "Future wakes anew.\n",
      "   (1.41s)\n",
      "\n",
      "Prompt: Write a haiku about the ocean at sunset.\n",
      "----------------------------------------\n",
      "âœ… Crimson waves whisper,  \n",
      "The sun dips in a warm glow,  \n",
      "Dayâ€™s end, peace unfolds.\n",
      "   (1.12s)\n",
      "\n",
      "Prompt: Write a haiku about coding late at night.\n",
      "----------------------------------------\n",
      "âœ… Lines of code whisper,  \n",
      "Moonlight spills on the keyboard,  \n",
      "Dreams of algorithms.\n",
      "   (1.10s)\n"
     ]
    }
   ],
   "source": [
    "# Test the haiku generation function\n",
    "print(\"ğŸ‹ Testing Haiku Generation\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "haiku_prompts = [\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Write a haiku about the ocean at sunset.\",\n",
    "    \"Write a haiku about coding late at night.\"\n",
    "]\n",
    "\n",
    "for prompt in haiku_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test with the configured variant\n",
    "    result = test_variant_performance(\n",
    "        variant_name=\"gpt_4o_mini\",  # This is from the haiku function\n",
    "        provider_name=\"OpenAI GPT-4o Mini\",\n",
    "        test_prompt=prompt,\n",
    "        function_name=\"generate_haiku\"\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"âœ… {result['full_content']}\")\n",
    "        print(f\"   ({result['response_time']:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on our testing:\n",
    "\n",
    "1. **Performance**: Record the response times and reliability of each provider\n",
    "2. **Quality**: Note differences in response style and content quality\n",
    "3. **Reliability**: Track success rates and error patterns\n",
    "4. **Use Cases**: Identify which providers work best for specific scenarios\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Add more providers (Anthropic, xAI) to the configuration\n",
    "2. Implement A/B testing and routing strategies\n",
    "3. Add structured output functions (JSON schema validation)\n",
    "4. Test fallback mechanisms\n",
    "\n",
    "Next notebook: We'll explore observability and tracing features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}