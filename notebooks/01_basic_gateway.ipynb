{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Basic TensorZero Gateway\n",
    "\n",
    "This notebook demonstrates basic TensorZero gateway functionality including:\n",
    "- Setting up the client\n",
    "- Making inference calls\n",
    "- Using different providers\n",
    "- Understanding the response structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key is set\n",
      "âœ… Anthropic API key is set\n",
      "âœ… xAI API key is set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "from tensorzero import TensorZeroGateway\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are set\n",
    "api_keys = {\n",
    "    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"xAI\": os.getenv(\"XAI_API_KEY\")\n",
    "}\n",
    "\n",
    "for provider, key in api_keys.items():\n",
    "    if key:\n",
    "        print(f\"âœ… {provider} API key is set\")\n",
    "    else:\n",
    "        print(f\"âœ— {provider} API key is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize TensorZero Client\n",
    "\n",
    "TensorZero can be used in two modes:\n",
    "1. **Standalone Gateway**: Connect to a running gateway service\n",
    "2. **Embedded Gateway**: Run gateway within your Python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to TensorZero gateway\n",
      "ðŸŒ Gateway API: http://localhost:3000\n",
      "ðŸŽ¨ TensorZero UI: http://localhost:4000\n",
      "ðŸ“Š ClickHouse: http://localhost:8123\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Connect to standalone gateway (requires docker compose up)\n",
    "# Note: The old constructor is deprecated, use build_http instead\n",
    "gateway_client = TensorZeroGateway.build_http(gateway_url=\"http://localhost:3000\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    # Make a simple health check request\n",
    "    print(\"âœ… Connected to TensorZero gateway\")\n",
    "    print(\"ðŸŒ Gateway API: http://localhost:3000\")\n",
    "    print(\"ðŸŽ¨ TensorZero UI: http://localhost:4000\")\n",
    "    print(\"ðŸ“Š ClickHouse: http://localhost:8123\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to connect: {e}\")\n",
    "    print(\"Make sure to run 'poe up' or 'docker compose up' first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-29T02:59:00.722048Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.grok3_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n",
      "\u001b[2m2025-08-29T02:59:00.722063Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.claude_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Embedded gateway (runs in-process)\n",
    "embedded_client = TensorZeroGateway.build_embedded(\n",
    "    clickhouse_url=\"http://chuser:chpassword@localhost:8123/tensorzero\",\n",
    "    config_file=\"../config/tensorzero.toml\",\n",
    ")\n",
    "\n",
    "# For this notebook, we'll use the standalone gateway\n",
    "client = gateway_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test structured sentiment analysis (JSON function)\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "# Test the analyze_sentiment JSON function (with system input for template)\n",
    "print(\"Testing sentiment analysis with structured JSON output:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"analyze_sentiment\",\n",
    "            variant_name=\"claude_json\",  # Using Anthropic for JSON function\n",
    "            input={\n",
    "                \"system\": {\"text\": text},  # Provide system input for the template\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # For JSON functions, use output.parsed to access structured data\n",
    "        if hasattr(response.output, 'parsed') and response.output.parsed:\n",
    "            result = response.output.parsed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Sentiment: {result.get('sentiment', 'unknown')} (confidence: {result.get('confidence', 0):.2f})\")\n",
    "            print(f\"Explanation: {result.get('explanation', 'No explanation provided')}\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        else:\n",
    "            # Fallback to raw output if parsing failed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            if hasattr(response.output, 'raw'):\n",
    "                print(f\"Raw response: {response.output.raw[:100]}...\")\n",
    "            else:\n",
    "                print(f\"Response: {str(response.output)[:100]}...\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Failed for '{text[:30]}...': {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Note: JSON functions with system templates require both 'system' and 'messages' inputs\")\n",
    "print(\"ðŸ’¡ Structured data is available in response.output.parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "OpenAI GPT-4 (gpt4):\n",
      "\n",
      "Algorithms awake,\n",
      "In data patterns they dive,\n",
      "Knowledge from chaos thrives.\n",
      "\n",
      "==================================================\n",
      "OpenAI GPT-4o Mini (gpt4_mini):\n",
      "\n",
      "Patterns in the code,  \n",
      "Machines learn as data flows,  \n",
      "Thoughts in circuits bloom.  \n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Opus (claude3_opus):\n",
      "\n",
      "Patterns in data\n",
      "Algorithms learn and grow\n",
      "Intelligence blooms\n",
      "\u001b[2m2025-08-29T03:05:14.865077Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Sonnet (claude3_sonnet): âŒ Failed - TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All mo...\n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Haiku (claude3_haiku):\n",
      "\n",
      "Here is a haiku about machine learning:\n",
      "\n",
      "Algorithms learn,\n",
      "Patterns emerge from data,\n",
      "Machines grow smarter.\n",
      "\n",
      "==================================================\n",
      "xAI Grok-3 Mini (grok3_mini):\n",
      "\n",
      "Neural nets awaken,  \n",
      "From vast data oceans deep,  \n",
      "Insights bloom anew.\n",
      "\n",
      "==================================================\n",
      "xAI Grok Code Fast (grok_code_fast):\n",
      "\n",
      "Data streams like rivers,  \n",
      "Algorithms weave patterns,  \n",
      "Wisdom from the code.\n",
      "\n",
      "==================================================\n",
      "xAI Grok-4 (grok4):\n",
      "\n",
      "Data whispers soft,  \n",
      "Algorithms weave patterns deep,  \n",
      "Insights bloom like spring.\n"
     ]
    }
   ],
   "source": [
    "# Test different providers - now with 8 variants!\n",
    "variants_to_test = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\"),\n",
    "    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\"),\n",
    "    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\"),\n",
    "    (\"grok3_mini\", \"xAI Grok-3 Mini\"),\n",
    "    (\"grok_code_fast\", \"xAI Grok Code Fast\"),\n",
    "    (\"grok4\", \"xAI Grok-4\"),\n",
    "]\n",
    "\n",
    "test_prompt = \"Write a haiku about machine learning\"\n",
    "\n",
    "for variant_name, display_name in variants_to_test:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=variant_name,  # Specify which variant to use\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}):\\n\")\n",
    "        print(response.content[0].text if response.content else \"No content\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}): âŒ Failed - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Specific Variants\n",
    "\n",
    "We can request specific model variants for our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sentiment analysis with structured JSON output:\n",
      "============================================================\n",
      "\n",
      "Text: I absolutely love using TensorZero! It makes LLM d...\n",
      "Sentiment: positive (confidence: 0.95)\n",
      "Explanation: The text expresses a very positive sentiment towards TensorZero. Key phrases like 'absolutely love' and 'so much easier' indicate strong positive feelings and enthusiasm for the product, with no negative qualifiers. The high degree of positive language gives a high confidence in the positive sentiment classification.\n",
      "Inference ID: 0198f3df-6346-7552-9bb5-6aa22fe2893e\n",
      "\n",
      "Text: The service is down again. This is really frustrat...\n",
      "Raw response: {\"sentiment\": \"negative\",\n",
      "  \"confidenceScore\": 0.85,\n",
      "  \"explanation\": \"The text expresses frustratio...\n",
      "Inference ID: 0198f3df-731f-7f31-85af-827c34dedfbe\n",
      "\n",
      "Text: The documentation is okay, but could use more exam...\n",
      "Raw response: {\"sentiment\": \"mixed\",\n",
      "    \"confidenceScore\": 0.65,\n",
      "    \"explanation\": \"The text expresses a mixed s...\n",
      "Inference ID: 0198f3df-852c-7371-ad29-8c4df76e1c98\n",
      "\n",
      "Text: Mixed feelings - great features but the setup was ...\n",
      "Sentiment: mixed (confidence: 0.85)\n",
      "Explanation: The text expresses both positive and negative sentiment. The phrase 'great features' indicates a positive opinion about the capabilities and functionality. However, this is counterbalanced by the negative phrase 'setup was complicated', implying the process of getting started was not easy or straightforward. The mixed language results in an overall mixed sentiment with positive and negative aspects.\n",
      "Inference ID: 0198f3df-9b40-70a3-a34c-9a6e6f503ee2\n",
      "\n",
      "ðŸ’¡ Note: JSON functions with system templates require both 'system' and 'messages' inputs\n",
      "ðŸ’¡ Structured data is available in response.output.parsed\n"
     ]
    }
   ],
   "source": [
    "# Test structured sentiment analysis (JSON function)\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "# Test the analyze_sentiment JSON function (with system input for template)\n",
    "print(\"Testing sentiment analysis with structured JSON output:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"analyze_sentiment\",\n",
    "            variant_name=\"claude_json\",  # Using Anthropic for JSON function\n",
    "            input={\n",
    "                \"system\": {\"text\": text},  # Provide system input for the template\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # For JSON functions, use output.parsed to access structured data\n",
    "        if hasattr(response.output, 'parsed') and response.output.parsed:\n",
    "            result = response.output.parsed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Sentiment: {result.get('sentiment', 'unknown')} (confidence: {result.get('confidence', 0):.2f})\")\n",
    "            print(f\"Explanation: {result.get('explanation', 'No explanation provided')}\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        else:\n",
    "            # Fallback to raw output if parsing failed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            if hasattr(response.output, 'raw'):\n",
    "                print(f\"Raw response: {response.output.raw[:100]}...\")\n",
    "            else:\n",
    "                print(f\"Response: {str(response.output)[:100]}...\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Failed for '{text[:30]}...': {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Note: JSON functions with system templates require both 'system' and 'messages' inputs\")\n",
    "print(\"ðŸ’¡ Structured data is available in response.output.parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversations\n",
    "\n",
    "TensorZero supports multi-turn conversations with message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I apologize, but as of my last training data in October 2021, there isn't enough information or any relevant references to TensorZero in context of legal education infrastructure or any other fields. ...\n",
      "\n",
      "Follow-up response: In general, observability refers to how well internal states of a system can be inferred from knowledge of its external outputs. In the field of computing and IT infrastructure, observability means yo...\n"
     ]
    }
   ],
   "source": [
    "# Build a conversation (using user/assistant roles only)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are a helpful AI assistant specializing in LLM infrastructure. What are the key components of TensorZero?\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response1.content[0].text[:200] + \"...\\n\")\n",
    "\n",
    "# Add response to conversation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response1.content[0].text})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Tell me more about the observability features.\"})\n",
    "\n",
    "# Second turn\n",
    "response2 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Follow-up response:\", response2.content[0].text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Metadata and Observability\n",
    "\n",
    "TensorZero provides rich metadata with each response for observability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: 8 variants configured across OpenAI, Anthropic, and xAI\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs (all Grok models support this!)\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "6. **Client API**: Use `TensorZeroGateway.build_http()` (constructor is deprecated)\n",
    "\n",
    "## Advanced Capabilities (NEW):\n",
    "- **xAI Grok Models**: All support structured output, reasoning, and function calling\n",
    "- **grok-4-0790**: Supports image input + text output\n",
    "- **JSON Functions**: Configured with schema files in `config/functions/`\n",
    "- **Services**: Gateway (3000), UI (4000), ClickHouse (8123)\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a conversation (using user/assistant roles only)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are a helpful AI assistant specializing in LLM infrastructure. What are the key components of TensorZero?\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response1.content[0].text[:200] + \"...\\n\")\n",
    "\n",
    "# Add response to conversation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response1.content[0].text})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Tell me more about the observability features.\"})\n",
    "\n",
    "# Second turn\n",
    "response2 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Follow-up response:\", response2.content[0].text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-29T03:33:47.971779Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (404 Not Found) for url (http://localhost:3000/inference)\n",
      "Expected error for invalid variant: TensorZeroError (status code 404): {\"error\":\"Unknown variant: non_existent_variant\"}\n",
      "\u001b[2m2025-08-29T03:33:48.127652Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (404 Not Found) for url (http://localhost:3000/inference)\n",
      "\n",
      "Expected error for invalid function: TensorZeroError (status code 404): {\"error\":\"Unknown function: non_existent_function\"}\n"
     ]
    }
   ],
   "source": [
    "# Test with invalid variant\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"chat\",\n",
    "        variant_name=\"non_existent_variant\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Expected error for invalid variant: {e}\")\n",
    "\n",
    "# Test with invalid function\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"non_existent_function\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nExpected error for invalid function: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Collecting Feedback\n",
    "\n",
    "TensorZero allows collecting feedback on inferences for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-29T03:33:55.575925Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (404 Not Found) for url (http://localhost:3000/inference)\n"
     ]
    },
    {
     "ename": "TensorZeroError",
     "evalue": "TensorZeroError (status code 404): {\"error\":\"Unknown function: creative_write\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTensorZeroError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Make an inference\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcreative_write\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWrite a creative tagline for TensorZero\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTagline: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInference ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.inference_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTensorZeroError\u001b[39m: TensorZeroError (status code 404): {\"error\":\"Unknown function: creative_write\"}"
     ]
    }
   ],
   "source": [
    "# Make an inference\n",
    "response = client.inference(\n",
    "    function_name=\"creative_write\",\n",
    "    input={\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Write a creative tagline for TensorZero\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Tagline: {response.content}\")\n",
    "print(f\"\\nInference ID: {response.inference_id}\")\n",
    "\n",
    "# Collect feedback\n",
    "try:\n",
    "    client.feedback(\n",
    "        inference_id=response.inference_id,\n",
    "        feedback={\n",
    "            \"score\": 0.9,\n",
    "            \"helpful\": True,\n",
    "            \"creative\": True,\n",
    "            \"comment\": \"Great tagline!\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\nâœ“ Feedback submitted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Failed to submit feedback: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: Easy to switch between providers using variants\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
