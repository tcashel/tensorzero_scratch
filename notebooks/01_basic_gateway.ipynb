{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Basic TensorZero Gateway\n",
    "\n",
    "This notebook demonstrates basic TensorZero gateway functionality including:\n",
    "- Setting up the client\n",
    "- Making inference calls\n",
    "- Using different providers\n",
    "- Understanding the response structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key is set\n",
      "‚úÖ Anthropic API key is set\n",
      "‚úÖ xAI API key is set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are set\n",
    "api_keys = {\n",
    "    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"xAI\": os.getenv(\"XAI_API_KEY\")\n",
    "}\n",
    "\n",
    "for provider, key in api_keys.items():\n",
    "    if key:\n",
    "        print(f\"‚úÖ {provider} API key is set\")\n",
    "    else:\n",
    "        print(f\"‚úó {provider} API key is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize TensorZero Client\n",
    "\n",
    "TensorZero can be used in two modes:\n",
    "1. **Standalone Gateway**: Connect to a running gateway service\n",
    "2. **Embedded Gateway**: Run gateway within your Python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option 1: Connect to standalone gateway (requires docker compose up)\n# Note: The old constructor is deprecated, use build_http instead\ngateway_client = TensorZeroGateway.build_http(gateway_url=\"http://localhost:3000\")\n\n# Test connection\ntry:\n    # Make a simple health check request\n    print(\"‚úÖ Connected to TensorZero gateway\")\n    print(\"üåê Gateway API: http://localhost:3000\")\n    print(\"üé® TensorZero UI: http://localhost:4000\")\n    print(\"üìä ClickHouse: http://localhost:8123\")\nexcept Exception as e:\n    print(f\"‚úó Failed to connect: {e}\")\n    print(\"Make sure to run 'poe up' or 'docker compose up' first!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option 2: Embedded gateway (runs in-process)\nembedded_client = TensorZeroGateway.build_embedded(\n    clickhouse_url=\"http://chuser:chpassword@localhost:8123/tensorzero\",\n    config_file=\"../config/tensorzero.toml\",\n)\n\n# For this notebook, we'll use the standalone gateway\nclient = gateway_client"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Inference\n",
    "\n",
    "Let's start with a simple chat completion using our configured functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different providers - now with 8 variants!\n",
    "variants_to_test = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\"),\n",
    "    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\"),\n",
    "    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\"),\n",
    "    (\"grok3_mini\", \"xAI Grok-3 Mini\"),\n",
    "    (\"grok_code_fast\", \"xAI Grok Code Fast\"),\n",
    "    (\"grok4\", \"xAI Grok-4\"),\n",
    "]\n",
    "\n",
    "test_prompt = \"Write a haiku about machine learning\"\n",
    "\n",
    "for variant_name, display_name in variants_to_test:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=variant_name,  # Specify which variant to use\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}):\\n\")\n",
    "        print(response.content[0].text if response.content else \"No content\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}): ‚ùå Failed - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Specific Variants\n",
    "\n",
    "We can request specific model variants for our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different providers\n",
    "variants_to_test = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\"),\n",
    "    (\"gpt35\", \"OpenAI GPT-3.5\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\"),\n",
    "    (\"grok\", \"xAI Grok\")\n",
    "]\n",
    "\n",
    "test_prompt = \"Write a haiku about machine learning\"\n",
    "\n",
    "for variant_name, display_name in variants_to_test:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=variant_name,  # Specify which variant to use\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}):\\n\")\n",
    "        print(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}): ‚ùå Failed - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentiment analysis with structured output - NEW!\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "# Test with different providers that support structured output\n",
    "structured_variants = [\n",
    "    (\"gpt4_json\", \"OpenAI GPT-4\"),\n",
    "    (\"claude_json\", \"Anthropic Claude\"),  \n",
    "    (\"grok3_json\", \"xAI Grok-3 (with structured output!)\")\n",
    "]\n",
    "\n",
    "for variant_name, provider_name in structured_variants:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {provider_name} - Structured Output\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for text in test_texts[:2]:  # Test first 2 texts\n",
    "        try:\n",
    "            response = client.inference(\n",
    "                function_name=\"analyze_sentiment\",\n",
    "                variant_name=variant_name,\n",
    "                input={\n",
    "                    \"system\": {\"text\": text},  # Note: system input may be required\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": text}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            result = json.loads(response.content[0].text)\n",
    "            \n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "            print(f\"Explanation: {result['explanation']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Failed for '{text[:30]}...': {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Output with JSON Schema\n",
    "\n",
    "TensorZero supports structured outputs using JSON schema validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentiment analysis with structured output\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    response = client.inference(\n",
    "        function_name=\"analyze_sentiment\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    result = json.loads(response.content)\n",
    "    \n",
    "    print(f\"\\nText: {text[:50]}...\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "    print(f\"Explanation: {result['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversations\n",
    "\n",
    "TensorZero supports multi-turn conversations with message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in LLM infrastructure.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the key components of TensorZero?\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response1.content[:200] + \"...\\n\")\n",
    "\n",
    "# Add response to conversation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response1.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Tell me more about the observability features.\"})\n",
    "\n",
    "# Second turn\n",
    "response2 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Follow-up response:\", response2.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Metadata and Observability\n",
    "\n",
    "TensorZero provides rich metadata with each response for observability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: 8 variants configured across OpenAI, Anthropic, and xAI\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs (all Grok models support this!)\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "6. **Client API**: Use `TensorZeroGateway.build_http()` (constructor is deprecated)\n",
    "\n",
    "## Advanced Capabilities (NEW):\n",
    "- **xAI Grok Models**: All support structured output, reasoning, and function calling\n",
    "- **grok-4-0790**: Supports image input + text output\n",
    "- **JSON Functions**: Configured with schema files in `config/functions/`\n",
    "- **Services**: Gateway (3000), UI (4000), ClickHouse (8123)\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Fallbacks\n",
    "\n",
    "Let's test how TensorZero handles errors and provider failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with invalid variant\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"chat\",\n",
    "        variant_name=\"non_existent_variant\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Expected error for invalid variant: {e}\")\n",
    "\n",
    "# Test with invalid function\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"non_existent_function\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nExpected error for invalid function: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Collecting Feedback\n",
    "\n",
    "TensorZero allows collecting feedback on inferences for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an inference\n",
    "response = client.inference(\n",
    "    function_name=\"creative_write\",\n",
    "    input={\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Write a creative tagline for TensorZero\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Tagline: {response.content}\")\n",
    "print(f\"\\nInference ID: {response.inference_id}\")\n",
    "\n",
    "# Collect feedback\n",
    "try:\n",
    "    client.feedback(\n",
    "        inference_id=response.inference_id,\n",
    "        feedback={\n",
    "            \"score\": 0.9,\n",
    "            \"helpful\": True,\n",
    "            \"creative\": True,\n",
    "            \"comment\": \"Great tagline!\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\n‚úì Feedback submitted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Failed to submit feedback: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: Easy to switch between providers using variants\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}