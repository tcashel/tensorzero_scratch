{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Basic TensorZero Gateway\n",
    "\n",
    "This notebook demonstrates basic TensorZero gateway functionality including:\n",
    "- Setting up the client\n",
    "- Making inference calls\n",
    "- Using different providers\n",
    "- Understanding the response structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key is set\n",
      "✅ Anthropic API key is set\n",
      "✅ xAI API key is set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are set\n",
    "api_keys = {\n",
    "    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"xAI\": os.getenv(\"XAI_API_KEY\")\n",
    "}\n",
    "\n",
    "for provider, key in api_keys.items():\n",
    "    if key:\n",
    "        print(f\"✅ {provider} API key is set\")\n",
    "    else:\n",
    "        print(f\"✗ {provider} API key is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize TensorZero Client\n",
    "\n",
    "TensorZero can be used in two modes:\n",
    "1. **Standalone Gateway**: Connect to a running gateway service\n",
    "2. **Embedded Gateway**: Run gateway within your Python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to TensorZero gateway\n",
      "🌐 Gateway API: http://localhost:3000\n",
      "🎨 TensorZero UI: http://localhost:4000\n",
      "📊 ClickHouse: http://localhost:8123\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Connect to standalone gateway (requires docker compose up)\n",
    "# Note: The old constructor is deprecated, use build_http instead\n",
    "gateway_client = TensorZeroGateway.build_http(gateway_url=\"http://localhost:3000\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    # Make a simple health check request\n",
    "    print(\"✅ Connected to TensorZero gateway\")\n",
    "    print(\"🌐 Gateway API: http://localhost:3000\")\n",
    "    print(\"🎨 TensorZero UI: http://localhost:4000\")\n",
    "    print(\"📊 ClickHouse: http://localhost:8123\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to connect: {e}\")\n",
    "    print(\"Make sure to run 'poe up' or 'docker compose up' first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-29T01:10:31.385198Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.grok3_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n",
      "\u001b[2m2025-08-29T01:10:31.385206Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.claude_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Option 2: Embedded gateway (runs in-process)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m embedded_client = \u001b[43mTensorZeroGateway\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_embedded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclickhouse_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://chuser:chpassword@localhost:8123/tensorzero\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../config/tensorzero.toml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# For this notebook, we'll use the standalone gateway\u001b[39;00m\n\u001b[32m      8\u001b[39m client = gateway_client\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Option 2: Embedded gateway (runs in-process)\n",
    "embedded_client = TensorZeroGateway.build_embedded(\n",
    "    clickhouse_url=\"http://chuser:chpassword@localhost:8123/tensorzero\",\n",
    "    config_file=\"../config/tensorzero.toml\",\n",
    ")\n",
    "\n",
    "# For this notebook, we'll use the standalone gateway\n",
    "client = gateway_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Inference\n",
    "\n",
    "Let's start with a simple chat completion using our configured functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variant_name, display_name \u001b[38;5;129;01min\u001b[39;00m variants_to_test:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify which variant to use\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test different providers - now with 8 variants!\n",
    "variants_to_test = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\"),\n",
    "    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\"),\n",
    "    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\"),\n",
    "    (\"grok3_mini\", \"xAI Grok-3 Mini\"),\n",
    "    (\"grok_code_fast\", \"xAI Grok Code Fast\"),\n",
    "    (\"grok4\", \"xAI Grok-4\"),\n",
    "]\n",
    "\n",
    "test_prompt = \"Write a haiku about machine learning\"\n",
    "\n",
    "for variant_name, display_name in variants_to_test:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=variant_name,  # Specify which variant to use\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}):\\n\")\n",
    "        print(response.content[0].text if response.content else \"No content\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}): ❌ Failed - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Specific Variants\n",
    "\n",
    "We can request specific model variants for our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sentiment analysis with structured JSON output:\n",
      "============================================================\n",
      "\u001b[2m2025-08-29T01:10:12.898576Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "❌ Failed for 'I absolutely love using Tensor...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\u001b[2m2025-08-29T01:10:13.038817Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "❌ Failed for 'The service is down again. Thi...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\u001b[2m2025-08-29T01:10:13.172178Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "❌ Failed for 'The documentation is okay, but...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\u001b[2m2025-08-29T01:10:13.303565Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "❌ Failed for 'Mixed feelings - great feature...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\n",
      "💡 Note: JSON functions return structured data in response.output.parsed\n"
     ]
    }
   ],
   "source": [
    "# Test structured sentiment analysis (JSON function)\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "# Test the analyze_sentiment JSON function (no system input required)\n",
    "print(\"Testing sentiment analysis with structured JSON output:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"analyze_sentiment\",\n",
    "            variant_name=\"claude_json\",  # Using Anthropic for JSON function\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # For JSON functions, use output.parsed to access structured data\n",
    "        if response.output.parsed:\n",
    "            result = response.output.parsed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "            print(f\"Explanation: {result['explanation']}\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        else:\n",
    "            # Fallback to raw output if parsing failed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Raw response: {response.output.raw[:100]}...\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Failed for '{text[:30]}...': {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\n💡 Note: JSON functions return structured data in response.output.parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 - Structured Output\n",
      "==================================================\n",
      "\u001b[2m2025-08-29T01:10:13.636848Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "❌ Failed for 'I absolutely love using Tensor...': TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: gpt4_json: All model p\n",
      "\u001b[2m2025-08-29T01:10:14.026275Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "❌ Failed for 'The service is down again. Thi...': TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: gpt4_json: All model p\n",
      "\n",
      "==================================================\n",
      "Testing Anthropic Claude - Structured Output\n",
      "==================================================\n",
      "\n",
      "❌ Failed for 'I absolutely love using Tensor...': 'JsonInferenceResponse' object has no attribute 'content'\n",
      "\n",
      "❌ Failed for 'The service is down again. Thi...': 'JsonInferenceResponse' object has no attribute 'content'\n",
      "\n",
      "==================================================\n",
      "Testing xAI Grok-3 (with structured output!) - Structured Output\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_texts[:\u001b[32m2\u001b[39m]:  \u001b[38;5;66;03m# Test first 2 texts\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyze_sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Note: system input may be required\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m         \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[32m     35\u001b[39m         result = json.loads(response.content[\u001b[32m0\u001b[39m].text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:299\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp.read(),\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mcls\u001b[39m, object_hook=object_hook,\n\u001b[32m    295\u001b[39m         parse_float=parse_float, parse_int=parse_int,\n\u001b[32m    296\u001b[39m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloads\u001b[39m(s, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    300\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    301\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m    containing a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    303\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test sentiment analysis with structured output - NEW!\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "# Test with different providers that support structured output\n",
    "structured_variants = [\n",
    "    (\"gpt4_json\", \"OpenAI GPT-4\"),\n",
    "    (\"claude_json\", \"Anthropic Claude\"),  \n",
    "    (\"grok3_json\", \"xAI Grok-3 (with structured output!)\")\n",
    "]\n",
    "\n",
    "for variant_name, provider_name in structured_variants:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {provider_name} - Structured Output\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for text in test_texts[:2]:  # Test first 2 texts\n",
    "        try:\n",
    "            response = client.inference(\n",
    "                function_name=\"analyze_sentiment\",\n",
    "                variant_name=variant_name,\n",
    "                input={\n",
    "                    \"system\": {\"text\": text},  # Note: system input may be required\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": text}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            result = json.loads(response.content[0].text)\n",
    "            \n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "            print(f\"Explanation: {result['explanation']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Failed for '{text[:30]}...': {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Output with JSON Schema\n",
    "\n",
    "TensorZero supports structured outputs using JSON schema validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-28T18:56:55.595864Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n"
     ]
    },
    {
     "ename": "TensorZeroError",
     "evalue": "TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is present.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTensorZeroError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m test_texts = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI absolutely love using TensorZero! It makes LLM development so much easier.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe service is down again. This is really frustrating and impacting our work.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe documentation is okay, but could use more examples.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMixed feelings - great features but the setup was complicated.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyze_sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[32m     20\u001b[39m     result = json.loads(response.content)\n",
      "\u001b[31mTensorZeroError\u001b[39m: TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is present.\"}"
     ]
    }
   ],
   "source": [
    "# Test sentiment analysis with structured output\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    response = client.inference(\n",
    "        function_name=\"analyze_sentiment\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    result = json.loads(response.content)\n",
    "    \n",
    "    print(f\"\\nText: {text[:50]}...\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "    print(f\"Explanation: {result['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversations\n",
    "\n",
    "TensorZero supports multi-turn conversations with message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in LLM infrastructure.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the key components of TensorZero?\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response1.content[:200] + \"...\\n\")\n",
    "\n",
    "# Add response to conversation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response1.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Tell me more about the observability features.\"})\n",
    "\n",
    "# Second turn\n",
    "response2 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Follow-up response:\", response2.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Metadata and Observability\n",
    "\n",
    "TensorZero provides rich metadata with each response for observability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: 8 variants configured across OpenAI, Anthropic, and xAI\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs (all Grok models support this!)\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "6. **Client API**: Use `TensorZeroGateway.build_http()` (constructor is deprecated)\n",
    "\n",
    "## Advanced Capabilities (NEW):\n",
    "- **xAI Grok Models**: All support structured output, reasoning, and function calling\n",
    "- **grok-4-0790**: Supports image input + text output\n",
    "- **JSON Functions**: Configured with schema files in `config/functions/`\n",
    "- **Services**: Gateway (3000), UI (4000), ClickHouse (8123)\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Fallbacks\n",
    "\n",
    "Let's test how TensorZero handles errors and provider failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with invalid variant\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"chat\",\n",
    "        variant_name=\"non_existent_variant\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Expected error for invalid variant: {e}\")\n",
    "\n",
    "# Test with invalid function\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"non_existent_function\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nExpected error for invalid function: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Collecting Feedback\n",
    "\n",
    "TensorZero allows collecting feedback on inferences for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an inference\n",
    "response = client.inference(\n",
    "    function_name=\"creative_write\",\n",
    "    input={\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Write a creative tagline for TensorZero\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Tagline: {response.content}\")\n",
    "print(f\"\\nInference ID: {response.inference_id}\")\n",
    "\n",
    "# Collect feedback\n",
    "try:\n",
    "    client.feedback(\n",
    "        inference_id=response.inference_id,\n",
    "        feedback={\n",
    "            \"score\": 0.9,\n",
    "            \"helpful\": True,\n",
    "            \"creative\": True,\n",
    "            \"comment\": \"Great tagline!\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\n✓ Feedback submitted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Failed to submit feedback: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: Easy to switch between providers using variants\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
