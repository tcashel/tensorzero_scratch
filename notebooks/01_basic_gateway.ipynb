{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Basic TensorZero Gateway\n",
    "\n",
    "This notebook demonstrates basic TensorZero gateway functionality including:\n",
    "- Setting up the client\n",
    "- Making inference calls\n",
    "- Using different providers\n",
    "- Understanding the response structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom tensorzero import TensorZeroGateway\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Verify API keys are set\napi_keys = {\n    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n    \"xAI\": os.getenv(\"XAI_API_KEY\")\n}\n\nfor provider, key in api_keys.items():\n    if key:\n        print(f\"âœ… {provider} API key is set\")\n    else:\n        print(f\"âœ— {provider} API key is missing\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize TensorZero Client\n",
    "\n",
    "TensorZero can be used in two modes:\n",
    "1. **Standalone Gateway**: Connect to a running gateway service\n",
    "2. **Embedded Gateway**: Run gateway within your Python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option 1: Connect to standalone gateway (requires docker compose up)\n# Note: The old constructor is deprecated, use build_http instead\ngateway_client = TensorZeroGateway.build_http(\"http://localhost:3000\")\n\n# Test connection\ntry:\n    # Make a simple health check request\n    print(\"âœ… Connected to TensorZero gateway\")\n    print(\"ðŸŒ Gateway API: http://localhost:3000\")\n    print(\"ðŸŽ¨ TensorZero UI: http://localhost:4000\")\n    print(\"ðŸ“Š ClickHouse: http://localhost:8123\")\nexcept Exception as e:\n    print(f\"âœ— Failed to connect: {e}\")\n    print(\"Make sure to run 'poe up' or 'docker compose up' first!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-28T18:22:06.173881Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.grok3_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n",
      "\u001b[2m2025-08-28T18:22:06.173895Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.claude_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Embedded gateway (runs in-process)\n",
    "embedded_client = TensorZeroGateway.build_embedded(\n",
    "    clickhouse_url=\"http://chuser:chpassword@localhost:8123/tensorzero\",\n",
    "    config_file=\"../config/tensorzero.toml\",\n",
    ")\n",
    "\n",
    "# For this notebook, we'll use the standalone gateway\n",
    "client = gateway_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Inference\n",
    "\n",
    "Let's start with a simple chat completion using our configured functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test different providers - now with 8 variants!\nvariants_to_test = [\n    (\"gpt4\", \"OpenAI GPT-4\"),\n    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\"),\n    (\"claude3_opus\", \"Anthropic Claude 3 Opus\"),\n    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\"),\n    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\"),\n    (\"grok3_mini\", \"xAI Grok-3 Mini\"),\n    (\"grok_code_fast\", \"xAI Grok Code Fast\"),\n    (\"grok4\", \"xAI Grok-4\"),\n]\n\ntest_prompt = \"Write a haiku about machine learning\"\n\nfor variant_name, display_name in variants_to_test:\n    try:\n        response = client.inference(\n            function_name=\"chat\",\n            variant_name=variant_name,  # Specify which variant to use\n            input={\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": test_prompt}\n                ]\n            }\n        )\n        print(f\"\\n{'='*50}\")\n        print(f\"{display_name} ({variant_name}):\\n\")\n        print(response.content[0].text if response.content else \"No content\")\n    except Exception as e:\n        print(f\"\\n{'='*50}\")\n        print(f\"{display_name} ({variant_name}): âŒ Failed - {str(e)[:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Specific Variants\n",
    "\n",
    "We can request specific model variants for our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "OpenAI GPT-4 (gpt4):\n",
      "\n",
      "[Text(text=\"Algorithms take flight,\\nData spun to knowledge bright,\\nMachine learning's might.\", arguments=None, type='text')]\n",
      "\u001b[2m2025-08-28T18:22:18.182896Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (404 Not Found) for url (http://localhost:3000/inference)\n",
      "\n",
      "==================================================\n",
      "OpenAI GPT-3.5 (gpt35): âŒ Failed - TensorZeroError (status code 404): {\"error\":\"Unknown variant: gpt35\"}\n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Opus (claude3_opus):\n",
      "\n",
      "[Text(text=\"Algorithms learn\\nFrom data, patterns emerge\\nMachine's intellect\", arguments=None, type='text')]\n",
      "\u001b[2m2025-08-28T18:22:20.056825Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Sonnet (claude3_sonnet): âŒ Failed - TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All model providers failed to infer with errors: anthropic: Error from anthropic server: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"not_found_error\\\",\\\"message\\\":\\\"model: claude-3-sonnet-20240229\\\"},\\\"request_id\\\":\\\"req_011CSaaCEqESaA6pAibxCQiB\\\"}\"}\n",
      "\u001b[2m2025-08-28T18:22:20.201032Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (404 Not Found) for url (http://localhost:3000/inference)\n",
      "\n",
      "==================================================\n",
      "xAI Grok (grok): âŒ Failed - TensorZeroError (status code 404): {\"error\":\"Unknown variant: grok\"}\n"
     ]
    }
   ],
   "source": [
    "# Test different providers\n",
    "variants_to_test = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\"),\n",
    "    (\"gpt35\", \"OpenAI GPT-3.5\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\"),\n",
    "    (\"grok\", \"xAI Grok\")\n",
    "]\n",
    "\n",
    "test_prompt = \"Write a haiku about machine learning\"\n",
    "\n",
    "for variant_name, display_name in variants_to_test:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=variant_name,  # Specify which variant to use\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}):\\n\")\n",
    "        print(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}): âŒ Failed - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Test sentiment analysis with structured output - NEW!\ntest_texts = [\n    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n    \"The service is down again. This is really frustrating and impacting our work.\",\n    \"The documentation is okay, but could use more examples.\",\n    \"Mixed feelings - great features but the setup was complicated.\"\n]\n\n# Test with different providers that support structured output\nstructured_variants = [\n    (\"gpt4_json\", \"OpenAI GPT-4\"),\n    (\"claude_json\", \"Anthropic Claude\"),  \n    (\"grok3_json\", \"xAI Grok-3 (with structured output!)\")\n]\n\nfor variant_name, provider_name in structured_variants:\n    print(f\"\\n{'='*50}\")\n    print(f\"Testing {provider_name} - Structured Output\")\n    print(\"=\"*50)\n    \n    for text in test_texts[:2]:  # Test first 2 texts\n        try:\n            response = client.inference(\n                function_name=\"analyze_sentiment\",\n                variant_name=variant_name,\n                input={\n                    \"system\": {\"text\": text},  # Note: system input may be required\n                    \"messages\": [\n                        {\"role\": \"user\", \"content\": text}\n                    ]\n                }\n            )\n            \n            # Parse the JSON response\n            result = json.loads(response.content[0].text)\n            \n            print(f\"\\nText: {text[:50]}...\")\n            print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n            print(f\"Explanation: {result['explanation']}\")\n        except Exception as e:\n            print(f\"\\nâŒ Failed for '{text[:30]}...': {str(e)[:100]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Output with JSON Schema\n",
    "\n",
    "TensorZero supports structured outputs using JSON schema validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-28T18:22:20.335871Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n"
     ]
    },
    {
     "ename": "TensorZeroError",
     "evalue": "TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is present.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTensorZeroError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m test_texts = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI absolutely love using TensorZero! It makes LLM development so much easier.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe service is down again. This is really frustrating and impacting our work.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe documentation is okay, but could use more examples.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMixed feelings - great features but the setup was complicated.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyze_sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[32m     20\u001b[39m     result = json.loads(response.content)\n",
      "\u001b[31mTensorZeroError\u001b[39m: TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is present.\"}"
     ]
    }
   ],
   "source": [
    "# Test sentiment analysis with structured output\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    response = client.inference(\n",
    "        function_name=\"analyze_sentiment\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    result = json.loads(response.content)\n",
    "    \n",
    "    print(f\"\\nText: {text[:50]}...\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "    print(f\"Explanation: {result['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversations\n",
    "\n",
    "TensorZero supports multi-turn conversations with message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in LLM infrastructure.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the key components of TensorZero?\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response1.content[:200] + \"...\\n\")\n",
    "\n",
    "# Add response to conversation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response1.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Tell me more about the observability features.\"})\n",
    "\n",
    "# Second turn\n",
    "response2 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Follow-up response:\", response2.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Metadata and Observability\n",
    "\n",
    "TensorZero provides rich metadata with each response for observability."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Key Learnings\n\n1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n2. **Multi-Provider**: 8 variants configured across OpenAI, Anthropic, and xAI\n3. **Structured Output**: JSON schema validation for reliable outputs (all Grok models support this!)\n4. **Observability**: Each inference has a unique ID for tracking\n5. **Feedback Loop**: Built-in feedback collection for optimization\n6. **Client API**: Use `TensorZeroGateway.build_http()` (constructor is deprecated)\n\n## Advanced Capabilities (NEW):\n- **xAI Grok Models**: All support structured output, reasoning, and function calling\n- **grok-4-0790**: Supports image input + text output\n- **JSON Functions**: Configured with schema files in `config/functions/`\n- **Services**: Gateway (3000), UI (4000), ClickHouse (8123)\n\nNext notebook: We'll explore multi-provider testing and performance comparisons."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Fallbacks\n",
    "\n",
    "Let's test how TensorZero handles errors and provider failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with invalid variant\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"chat\",\n",
    "        variant_name=\"non_existent_variant\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Expected error for invalid variant: {e}\")\n",
    "\n",
    "# Test with invalid function\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"non_existent_function\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nExpected error for invalid function: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Collecting Feedback\n",
    "\n",
    "TensorZero allows collecting feedback on inferences for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an inference\n",
    "response = client.inference(\n",
    "    function_name=\"creative_write\",\n",
    "    input={\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Write a creative tagline for TensorZero\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Tagline: {response.content}\")\n",
    "print(f\"\\nInference ID: {response.inference_id}\")\n",
    "\n",
    "# Collect feedback\n",
    "try:\n",
    "    client.feedback(\n",
    "        inference_id=response.inference_id,\n",
    "        feedback={\n",
    "            \"score\": 0.9,\n",
    "            \"helpful\": True,\n",
    "            \"creative\": True,\n",
    "            \"comment\": \"Great tagline!\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\nâœ“ Feedback submitted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Failed to submit feedback: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: Easy to switch between providers using variants\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}