{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Basic TensorZero Gateway\n",
    "\n",
    "This notebook demonstrates basic TensorZero gateway functionality including:\n",
    "- Setting up the client\n",
    "- Making inference calls\n",
    "- Using different providers\n",
    "- Understanding the response structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key is set\n",
      "âœ… Anthropic API key is set\n",
      "âœ… xAI API key is set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tensorzero import TensorZeroGateway\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are set\n",
    "api_keys = {\n",
    "    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"xAI\": os.getenv(\"XAI_API_KEY\")\n",
    "}\n",
    "\n",
    "for provider, key in api_keys.items():\n",
    "    if key:\n",
    "        print(f\"âœ… {provider} API key is set\")\n",
    "    else:\n",
    "        print(f\"âœ— {provider} API key is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize TensorZero Client\n",
    "\n",
    "TensorZero can be used in two modes:\n",
    "1. **Standalone Gateway**: Connect to a running gateway service\n",
    "2. **Embedded Gateway**: Run gateway within your Python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to TensorZero gateway\n",
      "ðŸŒ Gateway API: http://localhost:3000\n",
      "ðŸŽ¨ TensorZero UI: http://localhost:4000\n",
      "ðŸ“Š ClickHouse: http://localhost:8123\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Connect to standalone gateway (requires docker compose up)\n",
    "# Note: The old constructor is deprecated, use build_http instead\n",
    "gateway_client = TensorZeroGateway.build_http(gateway_url=\"http://localhost:3000\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    # Make a simple health check request\n",
    "    print(\"âœ… Connected to TensorZero gateway\")\n",
    "    print(\"ðŸŒ Gateway API: http://localhost:3000\")\n",
    "    print(\"ðŸŽ¨ TensorZero UI: http://localhost:4000\")\n",
    "    print(\"ðŸ“Š ClickHouse: http://localhost:8123\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to connect: {e}\")\n",
    "    print(\"Make sure to run 'poe up' or 'docker compose up' first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-28T18:56:14.643483Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.grok3_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n",
      "\u001b[2m2025-08-28T18:56:14.643497Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::config\u001b[0m\u001b[2m:\u001b[0m Deprecation Warning: `json_mode` is not specified for `[functions.analyze_sentiment.variants.claude_json]` (parent function `analyze_sentiment` is a JSON function), defaulting to `strict`. This field will become required in a future release - see https://github.com/tensorzero/tensorzero/issues/1043 on GitHub for details.\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Embedded gateway (runs in-process)\n",
    "embedded_client = TensorZeroGateway.build_embedded(\n",
    "    clickhouse_url=\"http://chuser:chpassword@localhost:8123/tensorzero\",\n",
    "    config_file=\"../config/tensorzero.toml\",\n",
    ")\n",
    "\n",
    "# For this notebook, we'll use the standalone gateway\n",
    "client = gateway_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Inference\n",
    "\n",
    "Let's start with a simple chat completion using our configured functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "OpenAI GPT-4 (gpt4):\n",
      "\n",
      "Data weaves its dance,\n",
      "Learning from the soft machine,\n",
      "Wisdom in silence.\n",
      "\n",
      "==================================================\n",
      "OpenAI GPT-4o Mini (gpt4_mini):\n",
      "\n",
      "Data whispers truth,  \n",
      "Patterns dance in silent code,  \n",
      "Machines learn, then dream.\n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Opus (claude3_opus):\n",
      "\n",
      "Here is a haiku about machine learning:\n",
      "\n",
      "Data trains models\n",
      "Algorithms learn patterns\n",
      "Insights from machines\n",
      "\u001b[2m2025-08-28T18:56:19.180716Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Sonnet (claude3_sonnet): âŒ Failed - TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: claude3_sonnet: All mo...\n",
      "\n",
      "==================================================\n",
      "Anthropic Claude 3 Haiku (claude3_haiku):\n",
      "\n",
      "Here is a haiku about machine learning:\n",
      "\n",
      "Data flows and churns,\n",
      "Algorithms learn and adapt,\n",
      "Insights manifest.\n",
      "\n",
      "==================================================\n",
      "xAI Grok-3 Mini (grok3_mini):\n",
      "\n",
      "Algorithms whirl,  \n",
      "Teaching machines to predict,  \n",
      "Patterns emerge.\n",
      "\u001b[2m2025-08-28T18:56:26.859746Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "==================================================\n",
      "xAI Grok Code Fast (grok_code_fast): âŒ Failed - TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: grok_code_fast: All mo...\n",
      "\n",
      "==================================================\n",
      "xAI Grok-4 (grok4):\n",
      "\n",
      "Data whispers secrets,  \n",
      "Algorithms learn and weave,  \n",
      "Insights bloom in code.\n"
     ]
    }
   ],
   "source": [
    "# Test different providers - now with 8 variants!\n",
    "variants_to_test = [\n",
    "    (\"gpt4\", \"OpenAI GPT-4\"),\n",
    "    (\"gpt4_mini\", \"OpenAI GPT-4o Mini\"),\n",
    "    (\"claude3_opus\", \"Anthropic Claude 3 Opus\"),\n",
    "    (\"claude3_sonnet\", \"Anthropic Claude 3 Sonnet\"),\n",
    "    (\"claude3_haiku\", \"Anthropic Claude 3 Haiku\"),\n",
    "    (\"grok3_mini\", \"xAI Grok-3 Mini\"),\n",
    "    (\"grok_code_fast\", \"xAI Grok Code Fast\"),\n",
    "    (\"grok4\", \"xAI Grok-4\"),\n",
    "]\n",
    "\n",
    "test_prompt = \"Write a haiku about machine learning\"\n",
    "\n",
    "for variant_name, display_name in variants_to_test:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"chat\",\n",
    "            variant_name=variant_name,  # Specify which variant to use\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": test_prompt}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}):\\n\")\n",
    "        print(response.content[0].text if response.content else \"No content\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{display_name} ({variant_name}): âŒ Failed - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Specific Variants\n",
    "\n",
    "We can request specific model variants for our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sentiment analysis with structured JSON output:\n",
      "============================================================\n",
      "\u001b[2m2025-08-28T18:56:37.130956Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "âŒ Failed for 'I absolutely love using Tensor...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\u001b[2m2025-08-28T18:56:37.260957Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "âŒ Failed for 'The service is down again. Thi...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\u001b[2m2025-08-28T18:56:37.387641Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "âŒ Failed for 'The documentation is okay, but...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\u001b[2m2025-08-28T18:56:37.517700Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n",
      "\n",
      "âŒ Failed for 'Mixed feelings - great feature...': TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is presen\n",
      "\n",
      "ðŸ’¡ Note: JSON functions return structured data in response.output.parsed\n"
     ]
    }
   ],
   "source": [
    "# Test structured sentiment analysis (JSON function)\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "# Test the analyze_sentiment JSON function (no system input required)\n",
    "print(\"Testing sentiment analysis with structured JSON output:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    try:\n",
    "        response = client.inference(\n",
    "            function_name=\"analyze_sentiment\",\n",
    "            variant_name=\"claude_json\",  # Using Anthropic for JSON function\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # For JSON functions, use output.parsed to access structured data\n",
    "        if response.output.parsed:\n",
    "            result = response.output.parsed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "            print(f\"Explanation: {result['explanation']}\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        else:\n",
    "            # Fallback to raw output if parsing failed\n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Raw response: {response.output.raw[:100]}...\")\n",
    "            print(f\"Inference ID: {response.inference_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Failed for '{text[:30]}...': {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Note: JSON functions return structured data in response.output.parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing OpenAI GPT-4 - Structured Output\n",
      "==================================================\n",
      "\u001b[2m2025-08-28T18:56:37.888162Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "âŒ Failed for 'I absolutely love using Tensor...': TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: gpt4_json: All model p\n",
      "\u001b[2m2025-08-28T18:56:38.514456Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "\n",
      "âŒ Failed for 'The service is down again. Thi...': TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: gpt4_json: All model p\n",
      "\n",
      "==================================================\n",
      "Testing Anthropic Claude - Structured Output\n",
      "==================================================\n",
      "\n",
      "âŒ Failed for 'I absolutely love using Tensor...': 'JsonInferenceResponse' object has no attribute 'content'\n",
      "\n",
      "âŒ Failed for 'The service is down again. Thi...': 'JsonInferenceResponse' object has no attribute 'content'\n",
      "\n",
      "==================================================\n",
      "Testing xAI Grok-3 (with structured output!) - Structured Output\n",
      "==================================================\n",
      "\n",
      "âŒ Failed for 'I absolutely love using Tensor...': 'JsonInferenceResponse' object has no attribute 'content'\n",
      "\n",
      "âŒ Failed for 'The service is down again. Thi...': 'JsonInferenceResponse' object has no attribute 'content'\n"
     ]
    }
   ],
   "source": [
    "# Test sentiment analysis with structured output - NEW!\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "# Test with different providers that support structured output\n",
    "structured_variants = [\n",
    "    (\"gpt4_json\", \"OpenAI GPT-4\"),\n",
    "    (\"claude_json\", \"Anthropic Claude\"),  \n",
    "    (\"grok3_json\", \"xAI Grok-3 (with structured output!)\")\n",
    "]\n",
    "\n",
    "for variant_name, provider_name in structured_variants:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {provider_name} - Structured Output\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for text in test_texts[:2]:  # Test first 2 texts\n",
    "        try:\n",
    "            response = client.inference(\n",
    "                function_name=\"analyze_sentiment\",\n",
    "                variant_name=variant_name,\n",
    "                input={\n",
    "                    \"system\": {\"text\": text},  # Note: system input may be required\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": text}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            result = json.loads(response.content[0].text)\n",
    "            \n",
    "            print(f\"\\nText: {text[:50]}...\")\n",
    "            print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "            print(f\"Explanation: {result['explanation']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Failed for '{text[:30]}...': {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Output with JSON Schema\n",
    "\n",
    "TensorZero supports structured outputs using JSON schema validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-28T18:56:55.595864Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_core::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status client error (400 Bad Request) for url (http://localhost:3000/inference)\n"
     ]
    },
    {
     "ename": "TensorZeroError",
     "evalue": "TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is present.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTensorZeroError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m test_texts = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI absolutely love using TensorZero! It makes LLM development so much easier.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe service is down again. This is really frustrating and impacting our work.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe documentation is okay, but could use more examples.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMixed feelings - great features but the setup was complicated.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyze_sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[32m     20\u001b[39m     result = json.loads(response.content)\n",
      "\u001b[31mTensorZeroError\u001b[39m: TensorZeroError (status code 400): {\"error\":\"`input.system` is empty but a system template is present.\"}"
     ]
    }
   ],
   "source": [
    "# Test sentiment analysis with structured output\n",
    "test_texts = [\n",
    "    \"I absolutely love using TensorZero! It makes LLM development so much easier.\",\n",
    "    \"The service is down again. This is really frustrating and impacting our work.\",\n",
    "    \"The documentation is okay, but could use more examples.\",\n",
    "    \"Mixed feelings - great features but the setup was complicated.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    response = client.inference(\n",
    "        function_name=\"analyze_sentiment\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    result = json.loads(response.content)\n",
    "    \n",
    "    print(f\"\\nText: {text[:50]}...\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "    print(f\"Explanation: {result['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversations\n",
    "\n",
    "TensorZero supports multi-turn conversations with message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in LLM infrastructure.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the key components of TensorZero?\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response1.content[:200] + \"...\\n\")\n",
    "\n",
    "# Add response to conversation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response1.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Tell me more about the observability features.\"})\n",
    "\n",
    "# Second turn\n",
    "response2 = client.inference(\n",
    "    function_name=\"chat\",\n",
    "    variant_name=\"gpt4\",\n",
    "    input={\"messages\": messages}\n",
    ")\n",
    "\n",
    "print(\"Follow-up response:\", response2.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Metadata and Observability\n",
    "\n",
    "TensorZero provides rich metadata with each response for observability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: 8 variants configured across OpenAI, Anthropic, and xAI\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs (all Grok models support this!)\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "6. **Client API**: Use `TensorZeroGateway.build_http()` (constructor is deprecated)\n",
    "\n",
    "## Advanced Capabilities (NEW):\n",
    "- **xAI Grok Models**: All support structured output, reasoning, and function calling\n",
    "- **grok-4-0790**: Supports image input + text output\n",
    "- **JSON Functions**: Configured with schema files in `config/functions/`\n",
    "- **Services**: Gateway (3000), UI (4000), ClickHouse (8123)\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Fallbacks\n",
    "\n",
    "Let's test how TensorZero handles errors and provider failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with invalid variant\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"chat\",\n",
    "        variant_name=\"non_existent_variant\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Expected error for invalid variant: {e}\")\n",
    "\n",
    "# Test with invalid function\n",
    "try:\n",
    "    response = client.inference(\n",
    "        function_name=\"non_existent_function\",\n",
    "        input={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nExpected error for invalid function: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Collecting Feedback\n",
    "\n",
    "TensorZero allows collecting feedback on inferences for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an inference\n",
    "response = client.inference(\n",
    "    function_name=\"creative_write\",\n",
    "    input={\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Write a creative tagline for TensorZero\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Tagline: {response.content}\")\n",
    "print(f\"\\nInference ID: {response.inference_id}\")\n",
    "\n",
    "# Collect feedback\n",
    "try:\n",
    "    client.feedback(\n",
    "        inference_id=response.inference_id,\n",
    "        feedback={\n",
    "            \"score\": 0.9,\n",
    "            \"helpful\": True,\n",
    "            \"creative\": True,\n",
    "            \"comment\": \"Great tagline!\"\n",
    "        }\n",
    "    )\n",
    "    print(\"\\nâœ“ Feedback submitted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Failed to submit feedback: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Gateway Modes**: TensorZero supports both standalone and embedded gateway modes\n",
    "2. **Multi-Provider**: Easy to switch between providers using variants\n",
    "3. **Structured Output**: JSON schema validation for reliable outputs\n",
    "4. **Observability**: Each inference has a unique ID for tracking\n",
    "5. **Feedback Loop**: Built-in feedback collection for optimization\n",
    "\n",
    "Next notebook: We'll explore multi-provider testing and performance comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
