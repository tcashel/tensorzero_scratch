#!/usr/bin/env python3
"""
LangGraph Agent with TensorZero Integration

This module demonstrates how to use TensorZero as a proxy for LLM provider calls
within a LangGraph agent using the create_react_agent() function from LangChain.

The agent uses xAI's Grok model through TensorZero and can call various tools
defined in Python. It outputs a predefined conversation to demonstrate tool calling
with proper formatting and pretty printing.
"""

import asyncio
from typing import Any, Optional

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.utils.utils import from_env
from pydantic import Field, model_validator
from typing_extensions import Self

from langgraph.graph import END, StateGraph
from typing_extensions import TypedDict

from tensorzero import TensorZeroGateway, Message, Text, ToolCall, ToolResult, ChatInferenceResponse
from rich.console import Console
from rich.panel import Panel
from .tensorzero_chat_model import TensorZeroChatModel


# Define Python-based tools
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate a response using TensorZero."""

        # Convert messages and make inference call
        tensorzero_messages = self._convert_messages_to_tensorzero(messages)
        response = self.gateway.inference(
            function_name=self.function_name,
            variant_name=self.variant_name,
            input={"messages": tensorzero_messages},
            episode_id=self.episode_id
        )

        # Store episode ID for future calls
        if hasattr(response, 'episode_id'):
            self.episode_id = response.episode_id

        # Extract content and create response
        content, tool_calls = self._extract_response_content(response)
        ai_message = self._create_ai_message(content, tool_calls)

        generation = ChatGeneration(message=ai_message)
        return ChatResult(generations=[generation])

    def _convert_messages_to_tensorzero(self, messages: list[BaseMessage]) -> list[Message]:
        """Convert LangChain messages to TensorZero format."""
        tensorzero_messages = []

        for msg in messages:
            if isinstance(msg, HumanMessage):
                tensorzero_messages.append(self._convert_human_message(msg))
            elif isinstance(msg, AIMessage):
                tensorzero_messages.append(self._convert_ai_message(msg))
            elif isinstance(msg, ToolMessage):
                tensorzero_messages.append(self._convert_tool_message(msg))

        return tensorzero_messages

    def _convert_human_message(self, msg: HumanMessage) -> Message:
        """Convert HumanMessage to TensorZero format."""
        content = msg.content
        if isinstance(content, str):
            return Message(role="user", content=content)
        else:
            return Message(role="user", content=str(content))

    def _convert_ai_message(self, msg: AIMessage) -> Message:
        """Convert AIMessage to TensorZero format."""
        content = msg.content
        if isinstance(content, str):
            return Message(role="assistant", content=content)
        else:
            return Message(role="assistant", content=str(content))

    def _convert_tool_message(self, msg: ToolMessage) -> Message:
        """Convert ToolMessage to TensorZero format."""
        # Create a proper tool message format that matches OpenAI's expectations
        tool_result = ToolResult(
            name=msg.name or "unknown_tool",
            result=msg.content,
            id=msg.tool_call_id or "unknown_id"
        )

        # For TensorZero, we need to send this as a user message containing the tool result
        # This matches the pattern used in the TensorZero examples
        return Message(
            role="user",
            content=[tool_result]
        )

    def _extract_response_content(self, response) -> tuple[str, list[dict]]:
        """Extract content and tool calls from TensorZero response."""
        content = ""
        tool_calls = []

        if not (hasattr(response, 'content') and response.content):
            return content, tool_calls

        for content_block in response.content:
            if isinstance(content_block, Text):
                content += content_block.text
            elif isinstance(content_block, ToolCall):
                tool_call = self._create_tool_call_dict(content_block, len(tool_calls))
                tool_calls.append(tool_call)

        return content, tool_calls

    def _create_tool_call_dict(self, content_block: ToolCall, call_index: int) -> dict:
        """Create a tool call dictionary from a ToolCall object."""
        return {
            "name": content_block.name,
            "args": content_block.arguments if hasattr(content_block, 'arguments') else {},
            "id": content_block.id if hasattr(content_block, 'id') else f"call_{call_index}"
        }

    def _create_ai_message(self, content: str, tool_calls: list[dict]) -> AIMessage:
        """Create AIMessage with optional tool calls."""
        if tool_calls:
            return AIMessage(
                content=content,
                tool_calls=[
                    {
                        "name": tc["name"],
                        "args": tc["args"],
                        "id": tc["id"]
                    }
                    for tc in tool_calls
                ]
            )
        else:
            return AIMessage(content=content)

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate a response using TensorZero."""

        # Convert messages and make inference call
        tensorzero_messages = self._convert_messages_to_tensorzero(messages)
        response = self.gateway.inference(
            function_name=self.function_name,
            variant_name=self.variant_name,
            input={"messages": tensorzero_messages},
            episode_id=self.episode_id
        )

        # Store episode ID for future calls
        if hasattr(response, 'episode_id'):
            self.episode_id = response.episode_id

        # Extract content and create response
        content, tool_calls = self._extract_response_content(response)
        ai_message = self._create_ai_message(content, tool_calls)

        generation = ChatGeneration(message=ai_message)
        return ChatResult(generations=[generation])

    @property
    def _llm_type(self) -> str:
        """Return the type of LLM."""
        return "tensorzero"

    def bind_tools(self, tools: list, **kwargs: Any):
        """Bind tools to the model for tool calling."""
        # For TensorZero, tools are configured at the function level
        # in the tensorzero.toml file, not dynamically bound
        # So we just return self with tool information stored
        self._bound_tools = tools
        return self

    @property
    def _identifying_params(self) -> dict[str, Any]:
        """Get identifying parameters."""
        return {
            "function_name": self.function_name,
            "variant_name": self.variant_name,
            "gateway_url": self.gateway_url,
            "episode_id": self.episode_id
        }


# Define Python-based tools
@tool
def calculator(expression: str) -> str:
    """
    Safely evaluate a mathematical expression for calculations.

    Args:
        expression: A mathematical expression like '2 + 2' or '10 * 5'

    Returns:
        The result of the mathematical expression as a string
    """
    try:
        # Use eval with restricted globals for safety
        result = eval(expression, {"__builtins__": {}}, {})
        return f"The result of {expression} is {result}"
    except Exception as e:
        return f"Error evaluating expression '{expression}': {str(e)}"


@tool
def get_weather(location: str) -> str:
    """
    Get weather information for a location.

    Args:
        location: The city or location to get weather for

    Returns:
        Weather information for the specified location
    """
    # Mock weather data for demonstration
    weather_data = {
        "New York": "Sunny, 72Â°F",
        "London": "Cloudy, 15Â°C",
        "Tokyo": "Rainy, 22Â°C",
        "San Francisco": "Foggy, 18Â°C",
        "Paris": "Partly cloudy, 20Â°C"
    }

    location = location.strip().title()
    if location in weather_data:
        return f"The current weather in {location} is {weather_data[location]}."
    else:
        return f"Sorry, I don't have weather data for {location}. Available locations: {', '.join(weather_data.keys())}"


@tool
def search_tensorzero_docs(query: str) -> str:
    """
    Search TensorZero documentation for information.

    Args:
        query: The search query to look for in TensorZero docs

    Returns:
        Relevant information from TensorZero documentation
    """
    # Mock documentation search for demonstration
    docs = {
        "gateway": "The TensorZero Gateway is the core component that routes requests to different LLM providers based on your configuration.",
        "functions": "Functions in TensorZero define how the gateway should process requests, including which models to use and what tools are available.",
        "tools": "Tools allow your LLM to perform actions and gather information. TensorZero supports both built-in and custom tools.",
        "variants": "Variants let you configure different model providers and settings for the same function, enabling A/B testing and fallbacks.",
        "observability": "TensorZero provides comprehensive observability features including request tracing, metrics collection, and feedback analysis."
    }

    query_lower = query.lower()
    results = []

    for key, content in docs.items():
        if key in query_lower or any(word in query_lower for word in key.split()):
            results.append(f"**{key.title()}:** {content}")

    if results:
        return "\n\n".join(results)
    else:
        return f"No documentation found for '{query}'. Try searching for: gateway, functions, tools, variants, or observability."


class AgentState(TypedDict):
    """State for the LangGraph agent."""
    messages: list[BaseMessage]


class CalculatorTool:
    """Tool for safely evaluating mathematical expressions."""

    def __call__(self, state):
        """Execute calculator tool."""
        tool_call_id = None

        # Find the calculator tool call
        for content_block in state["messages"][-1]["content"]:
            if (
                isinstance(content_block, ToolCall)
                and content_block.name == "calculator"
            ):
                tool_call_id = content_block.id
                # Get the expression from arguments
                if hasattr(content_block, 'arguments') and 'expression' in content_block.arguments:
                    expression = content_block.arguments['expression']
                    result = calculator(expression)
                else:
                    result = "Error: No expression provided"
                break

        if tool_call_id is None:
            raise ValueError("CalculatorTool didn't find a tool call for `calculator`")

        # Create tool result message
        message = Message(
            role="user",
            content=[ToolResult(name="calculator", result=result, id=tool_call_id)],
        )

        return {"messages": state["messages"] + [message]}


class WeatherTool:
    """Tool for getting weather information."""

    def __call__(self, state):
        """Execute weather tool."""
        tool_call_id = None

        # Find the weather tool call
        for content_block in state["messages"][-1]["content"]:
            if (
                isinstance(content_block, ToolCall)
                and content_block.name == "get_weather"
            ):
                tool_call_id = content_block.id
                # Get the location from arguments
                if hasattr(content_block, 'arguments') and 'location' in content_block.arguments:
                    location = content_block.arguments['location']
                    result = get_weather(location)
                else:
                    result = "Error: No location provided"
                break

        if tool_call_id is None:
            raise ValueError("WeatherTool didn't find a tool call for `get_weather`")

        # Create tool result message
        message = Message(
            role="user",
            content=[ToolResult(name="get_weather", result=result, id=tool_call_id)],
        )

        return {"messages": state["messages"] + [message]}


class DocsTool:
    """Tool for searching TensorZero documentation."""

    def __call__(self, state):
        """Execute documentation search tool."""
        tool_call_id = None

        # Find the docs tool call
        for content_block in state["messages"][-1]["content"]:
            if (
                isinstance(content_block, ToolCall)
                and content_block.name == "search_tensorzero_docs"
            ):
                tool_call_id = content_block.id
                # Get the query from arguments
                if hasattr(content_block, 'arguments') and 'query' in content_block.arguments:
                    query = content_block.arguments['query']
                    result = search_tensorzero_docs(query)
                else:
                    result = "Error: No query provided"
                break

        if tool_call_id is None:
            raise ValueError("DocsTool didn't find a tool call for `search_tensorzero_docs`")

        # Create tool result message
        message = Message(
            role="user",
            content=[ToolResult(name="search_tensorzero_docs", result=result, id=tool_call_id)],
        )

        return {"messages": state["messages"] + [message]}


class TensorZeroLangGraphAgent:
    """
    LangGraph Agent that uses TensorZero as the LLM provider.

    This agent follows the TensorZero LangGraph integration pattern:
    - Uses TensorZero for LLM inference (with tools defined in config)
    - Implements tool execution in Python within the LangGraph graph
    - Uses StateGraph instead of create_react_agent for custom tool handling
    """

    def __init__(self, gateway_url: str = "http://localhost:3000"):
        """Initialize the agent."""
        self.console = Console()
        self.gateway = TensorZeroGateway.build_http(gateway_url=gateway_url)

        # Build the custom graph
        self.graph = self._build_graph()

    def _route_agent_response(self, state):
        """Route execution based on the agent's message content."""
        for content_block in state["messages"][-1]["content"]:
            if isinstance(content_block, ToolCall):
                if content_block.name == "calculator":
                    return "calculator"
                elif content_block.name == "get_weather":
                    return "get_weather"
                elif content_block.name == "search_tensorzero_docs":
                    return "search_tensorzero_docs"
                else:
                    raise ValueError(f"Unknown tool call: {content_block.name}")

        return END

    def _chatbot(self, state):
        """Call the agent_chat function in TensorZero."""
        # Convert LangChain messages to TensorZero format
        tensorzero_messages = []
        for msg in state["messages"]:
            if hasattr(msg, 'content'):
                if isinstance(msg.content, list):
                    # Handle complex content with tool calls/results
                    tensorzero_messages.append(Message(role=msg.role, content=msg.content))
                else:
                    # Handle simple string content
                    tensorzero_messages.append(Message(role=msg.role, content=msg.content))
            else:
                # Fallback for other message types
                tensorzero_messages.append(Message(role="user", content=str(msg)))

        response = self.gateway.inference(
            function_name="agent_chat",
            variant_name="gpt4_mini",
            input={"messages": tensorzero_messages},
            episode_id=state.get("episode_id")
        )

        if not isinstance(response, ChatInferenceResponse):
            raise ValueError(f"Unexpected Response Type: {type(response)}")

        assistant_message = Message(role="assistant", content=response.content)

        return {
            "messages": state["messages"] + [assistant_message],
            "episode_id": response.episode_id,
        }

    def _build_graph(self):
        """Build the LangGraph graph for the agent."""
        graph_builder = StateGraph(AgentState)

        graph_builder.add_node("chatbot", self._chatbot)
        graph_builder.add_node("calculator", CalculatorTool())
        graph_builder.add_node("get_weather", WeatherTool())
        graph_builder.add_node("search_tensorzero_docs", DocsTool())

        graph_builder.set_entry_point("chatbot")
        graph_builder.add_conditional_edges("chatbot", self._route_agent_response)
        graph_builder.add_edge("calculator", "chatbot")
        graph_builder.add_edge("get_weather", "chatbot")
        graph_builder.add_edge("search_tensorzero_docs", "chatbot")

        graph = graph_builder.compile()
        return graph

    def _format_message(self, message: BaseMessage, message_type: str) -> Panel:
        """Format a message for display."""
        if isinstance(message, AIMessage):
            if message.tool_calls:
                content = f"{message.content}\n\n[bold cyan]Tool Calls:[/bold cyan]\n"
                for tool_call in message.tool_calls:
                    content += f"â€¢ {tool_call['name']}({tool_call['args']})\n"
                content = content.strip()
            else:
                content = message.content
            border_color = "blue"
        elif isinstance(message, HumanMessage):
            content = message.content
            border_color = "green"
        elif isinstance(message, ToolMessage):
            content = f"Tool Result: {message.content}"
            border_color = "yellow"
        else:
            content = str(message.content)
            border_color = "white"

        return Panel(
            content,
            title=f"[bold]{message_type}[/bold]",
            border_style=border_color,
            title_align="left"
        )

    def _display_message(self, message: BaseMessage, message_type: str):
        """Display a message with formatting."""
        panel = self._format_message(message, message_type)
        self.console.print(panel)
        self.console.print()  # Add spacing

    async def run_demo_conversation(self):
        """
        Run a predefined conversation demonstrating tool calling capabilities.

        This method executes a series of predefined interactions to showcase
        how the agent uses TensorZero and calls various tools.
        """
        self.console.print("\n[bold magenta]ðŸ¤– TensorZero LangGraph Agent Demo[/bold magenta]\n")
        self.console.print("[dim]This demo shows how TensorZero acts as a proxy for LLM calls in a LangGraph agent[/dim]\n")

        # Predefined conversation
        demo_messages = [
            "Hello! I'm exploring TensorZero. Can you help me understand what it is?",
            "What's the weather like in Tokyo?",
            "Can you calculate 15 * 23 + 7?",
            "Tell me more about TensorZero functions and tools",
            "What's 2 ** 8?",
            "How's the weather in Paris?"
        ]

        for user_message in demo_messages:
            self._display_message(HumanMessage(content=user_message), "User")

            # Show processing indicator
            with self.console.status("[bold green]Processing with TensorZero...", spinner="dots"):
                try:
                    # Convert user message to TensorZero format
                    user_msg = Message(role="user", content=user_message)
                    messages = [user_msg]

                    # Run the graph
                    for event in self.graph.stream({"messages": messages}):
                        if "chatbot" in event:
                            messages = event["chatbot"]["messages"]

                            # Display the assistant message
                            assistant_msg = messages[-1]
                            if isinstance(assistant_msg, dict) and assistant_msg.get("role") == "assistant":
                                # Convert TensorZero message back to LangChain format for display
                                content = ""
                                tool_calls = []

                                if "content" in assistant_msg and assistant_msg["content"]:
                                    for content_block in assistant_msg["content"]:
                                        if isinstance(content_block, Text):
                                            content += content_block.text
                                        elif isinstance(content_block, ToolCall):
                                            tool_calls.append({
                                                "name": content_block.name,
                                                "args": content_block.arguments if hasattr(content_block, 'arguments') else {},
                                                "id": content_block.id if hasattr(content_block, 'id') else f"call_{len(tool_calls)}"
                                            })

                                if tool_calls:
                                    ai_message = AIMessage(
                                        content=content,
                                        tool_calls=[
                                            {
                                                "name": tc["name"],
                                                "args": tc["args"],
                                                "id": tc["id"]
                                            }
                                            for tc in tool_calls
                                        ]
                                    )
                                else:
                                    ai_message = AIMessage(content=content)

                                self._display_message(ai_message, "Assistant")
                            elif isinstance(assistant_msg, Message) and assistant_msg.role == "assistant":
                                # Handle direct Message objects
                                content = ""
                                tool_calls = []

                                if hasattr(assistant_msg, 'content') and assistant_msg.content:
                                    for content_block in assistant_msg.content:
                                        if isinstance(content_block, Text):
                                            content += content_block.text
                                        elif isinstance(content_block, ToolCall):
                                            tool_calls.append({
                                                "name": content_block.name,
                                                "args": content_block.arguments if hasattr(content_block, 'arguments') else {},
                                                "id": content_block.id if hasattr(content_block, 'id') else f"call_{len(tool_calls)}"
                                            })

                                if tool_calls:
                                    ai_message = AIMessage(
                                        content=content,
                                        tool_calls=[
                                            {
                                                "name": tc["name"],
                                                "args": tc["args"],
                                                "id": tc["id"]
                                            }
                                            for tc in tool_calls
                                        ]
                                    )
                                else:
                                    ai_message = AIMessage(content=content)

                                self._display_message(ai_message, "Assistant")

                        elif event:  # Handle tool execution events
                            messages = list(event.values())[0]["messages"]

                except Exception as e:
                    self.console.print(f"[bold red]Error:[/bold red] {str(e)}")
                    import traceback
                    traceback.print_exc()
                    continue

    async def interactive_chat(self):
        """
        Start an interactive chat session with the agent.

        Users can type messages and the agent will respond using TensorZero
        and tool calling capabilities.
        """
        self.console.print("\n[bold magenta]ðŸ¤– Interactive TensorZero Chat[/bold magenta]\n")
        self.console.print("[dim]Type 'quit' to exit, 'demo' to run demo conversation[/dim]\n")

        while True:
            try:
                user_input = await asyncio.to_thread(input, "You: ")
                user_input = user_input.strip()

                if user_input.lower() == 'quit':
                    self.console.print("[bold blue]Goodbye! ðŸ‘‹[/bold blue]")
                    break
                elif user_input.lower() == 'demo':
                    await self.run_demo_conversation()
                    continue
                elif not user_input:
                    continue

                # Show processing indicator
                with self.console.status("[bold green]Processing with TensorZero...", spinner="dots"):
                    try:
                        # Convert user message to TensorZero format
                        user_msg = Message(role="user", content=user_input)
                        messages = [user_msg]

                        # Run the graph
                        for event in self.graph.stream({"messages": messages}):
                            if "chatbot" in event:
                                messages = event["chatbot"]["messages"]

                                # Display the assistant message
                                assistant_msg = messages[-1]
                                if isinstance(assistant_msg, dict) and assistant_msg.get("role") == "assistant":
                                    # Convert TensorZero message back to LangChain format for display
                                    content = ""
                                    tool_calls = []

                                    if "content" in assistant_msg and assistant_msg["content"]:
                                        for content_block in assistant_msg["content"]:
                                            if isinstance(content_block, Text):
                                                content += content_block.text
                                            elif isinstance(content_block, ToolCall):
                                                tool_calls.append({
                                                    "name": content_block.name,
                                                    "args": content_block.arguments if hasattr(content_block, 'arguments') else {},
                                                    "id": content_block.id if hasattr(content_block, 'id') else f"call_{len(tool_calls)}"
                                                })

                                    if tool_calls:
                                        ai_message = AIMessage(
                                            content=content,
                                            tool_calls=[
                                                {
                                                    "name": tc["name"],
                                                    "args": tc["args"],
                                                    "id": tc["id"]
                                                }
                                                for tc in tool_calls
                                            ]
                                        )
                                    else:
                                        ai_message = AIMessage(content=content)

                                    self._display_message(ai_message, "Assistant")
                                elif isinstance(assistant_msg, Message) and assistant_msg.role == "assistant":
                                    # Handle direct Message objects
                                    content = ""
                                    tool_calls = []

                                    if hasattr(assistant_msg, 'content') and assistant_msg.content:
                                        for content_block in assistant_msg.content:
                                            if isinstance(content_block, Text):
                                                content += content_block.text
                                            elif isinstance(content_block, ToolCall):
                                                tool_calls.append({
                                                    "name": content_block.name,
                                                    "args": content_block.arguments if hasattr(content_block, 'arguments') else {},
                                                    "id": content_block.id if hasattr(content_block, 'id') else f"call_{len(tool_calls)}"
                                                })

                                    if tool_calls:
                                        ai_message = AIMessage(
                                            content=content,
                                            tool_calls=[
                                                {
                                                    "name": tc["name"],
                                                    "args": tc["args"],
                                                    "id": tc["id"]
                                                }
                                                for tc in tool_calls
                                            ]
                                        )
                                    else:
                                        ai_message = AIMessage(content=content)

                                    self._display_message(ai_message, "Assistant")

                            elif event:  # Handle tool execution events
                                messages = list(event.values())[0]["messages"]

                    except Exception as e:
                        self.console.print(f"[bold red]Error:[/bold red] {str(e)}")
                        continue

            except KeyboardInterrupt:
                self.console.print("\n[bold blue]Goodbye! ðŸ‘‹[/bold blue]")
                break
            except Exception as e:
                self.console.print(f"[bold red]Error:[/bold red] {str(e)}")
                continue


async def main():
    """
    Main function to demonstrate the TensorZero LangGraph agent.

    This function creates an agent and runs either a demo conversation
    or starts an interactive chat session.
    """
    try:
        agent = TensorZeroLangGraphAgent()

        # Check if we should run demo or interactive mode
        import sys
        if len(sys.argv) > 1 and sys.argv[1] == "--demo":
            await agent.run_demo_conversation()
        else:
            await agent.interactive_chat()

    except Exception as e:
        console = Console()
        console.print(f"[bold red]Error initializing agent:[/bold red] {str(e)}")
        console.print("[dim]Make sure TensorZero gateway is running on http://localhost:3000[/dim]")


if __name__ == "__main__":
    asyncio.run(main())
